{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter notebook for classification models (Logistic Regression and Random Forest) - Phase 2\n",
    "# CSC691 Final Project\n",
    "# PaceMakers: Predicting Average Heart Rate for Bike Rides\n",
    "# Patrick, Esteban, Sarah\n",
    "# 12/2/19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference for building Logistic Regression model: https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please update the path to the activities.csv file from the repository\n",
    "df = pd.read_csv('activities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Moving Time</th>\n",
       "      <th>Distance (km)</th>\n",
       "      <th>Elevation Gain (m)</th>\n",
       "      <th>Avg Moving Speed (kph)</th>\n",
       "      <th>Avg Pace (/km)</th>\n",
       "      <th>Calories</th>\n",
       "      <th>Best 20min Speed (kph)</th>\n",
       "      <th>...</th>\n",
       "      <th>HRSS / h</th>\n",
       "      <th>Best 20min HR (bpm)</th>\n",
       "      <th>Cadence Avg Moving (rpm or spm)</th>\n",
       "      <th>Avg Watts (w)</th>\n",
       "      <th>Avg Watts / Kilograms (w/kg)</th>\n",
       "      <th>Best 20min Power (w)</th>\n",
       "      <th>Power Stress Score</th>\n",
       "      <th>Power Stress Score / h</th>\n",
       "      <th>Athlete Settings</th>\n",
       "      <th>Delete</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2019-09-22T16:27:01-04:00</td>\n",
       "      <td>Último día de verano</td>\n",
       "      <td>Ride</td>\n",
       "      <td>01:58:06</td>\n",
       "      <td>62.5</td>\n",
       "      <td>589.0</td>\n",
       "      <td>31.7</td>\n",
       "      <td>01:53</td>\n",
       "      <td>1772</td>\n",
       "      <td>34</td>\n",
       "      <td>...</td>\n",
       "      <td>73</td>\n",
       "      <td>163</td>\n",
       "      <td>95</td>\n",
       "      <td>179</td>\n",
       "      <td>2.56</td>\n",
       "      <td>201</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>MaxHr 190bpm. RestHr 65bpm. Weight 70kg.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2019-09-21T11:57:23-04:00</td>\n",
       "      <td>Dos loops</td>\n",
       "      <td>Ride</td>\n",
       "      <td>02:38:51</td>\n",
       "      <td>80.1</td>\n",
       "      <td>890.0</td>\n",
       "      <td>28.7</td>\n",
       "      <td>02:05</td>\n",
       "      <td>2432</td>\n",
       "      <td>36.7</td>\n",
       "      <td>...</td>\n",
       "      <td>77</td>\n",
       "      <td>173</td>\n",
       "      <td>89</td>\n",
       "      <td>174</td>\n",
       "      <td>2.49</td>\n",
       "      <td>225</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>MaxHr 190bpm. RestHr 65bpm. Weight 70kg.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2019-09-20T17:55:47-04:00</td>\n",
       "      <td>Con poco tiempo</td>\n",
       "      <td>Ride</td>\n",
       "      <td>01:07:52</td>\n",
       "      <td>35.2</td>\n",
       "      <td>314.0</td>\n",
       "      <td>30.9</td>\n",
       "      <td>01:56</td>\n",
       "      <td>1029</td>\n",
       "      <td>34.5</td>\n",
       "      <td>...</td>\n",
       "      <td>76</td>\n",
       "      <td>163</td>\n",
       "      <td>91</td>\n",
       "      <td>169</td>\n",
       "      <td>2.42</td>\n",
       "      <td>188</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>MaxHr 190bpm. RestHr 65bpm. Weight 70kg.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2019-09-19T23:45:00-04:00</td>\n",
       "      <td>Complimentary calisthenics</td>\n",
       "      <td>Workout</td>\n",
       "      <td>01:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>Weight 70kg.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2019-09-18T17:41:48-04:00</td>\n",
       "      <td>Afternoon Ride</td>\n",
       "      <td>Ride</td>\n",
       "      <td>01:26:05</td>\n",
       "      <td>45.6</td>\n",
       "      <td>447.0</td>\n",
       "      <td>31.4</td>\n",
       "      <td>01:54</td>\n",
       "      <td>1326</td>\n",
       "      <td>36</td>\n",
       "      <td>...</td>\n",
       "      <td>81</td>\n",
       "      <td>173</td>\n",
       "      <td>97</td>\n",
       "      <td>181</td>\n",
       "      <td>2.59</td>\n",
       "      <td>221</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>MaxHr 190bpm. RestHr 65bpm. Weight 70kg.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Date                        Name     Type Moving Time  \\\n",
       "0  2019-09-22T16:27:01-04:00        Último día de verano     Ride    01:58:06   \n",
       "1  2019-09-21T11:57:23-04:00                   Dos loops     Ride    02:38:51   \n",
       "2  2019-09-20T17:55:47-04:00             Con poco tiempo     Ride    01:07:52   \n",
       "3  2019-09-19T23:45:00-04:00  Complimentary calisthenics  Workout    01:00:00   \n",
       "4  2019-09-18T17:41:48-04:00              Afternoon Ride     Ride    01:26:05   \n",
       "\n",
       "   Distance (km)  Elevation Gain (m) Avg Moving Speed (kph) Avg Pace (/km)  \\\n",
       "0           62.5               589.0                   31.7          01:53   \n",
       "1           80.1               890.0                   28.7          02:05   \n",
       "2           35.2               314.0                   30.9          01:56   \n",
       "3            0.0                 0.0                      -              -   \n",
       "4           45.6               447.0                   31.4          01:54   \n",
       "\n",
       "  Calories Best 20min Speed (kph)  ... HRSS / h Best 20min HR (bpm)  \\\n",
       "0     1772                     34  ...       73                 163   \n",
       "1     2432                   36.7  ...       77                 173   \n",
       "2     1029                   34.5  ...       76                 163   \n",
       "3        -                      -  ...        -                   -   \n",
       "4     1326                     36  ...       81                 173   \n",
       "\n",
       "  Cadence Avg Moving (rpm or spm) Avg Watts (w) Avg Watts / Kilograms (w/kg)  \\\n",
       "0                              95           179                         2.56   \n",
       "1                              89           174                         2.49   \n",
       "2                              91           169                         2.42   \n",
       "3                               -             -                            -   \n",
       "4                              97           181                         2.59   \n",
       "\n",
       "  Best 20min Power (w) Power Stress Score Power Stress Score / h  \\\n",
       "0                  201                  -                      0   \n",
       "1                  225                  -                      0   \n",
       "2                  188                  -                      0   \n",
       "3                    -                  -                      -   \n",
       "4                  221                  -                      0   \n",
       "\n",
       "                           Athlete Settings Delete  \n",
       "0  MaxHr 190bpm. RestHr 65bpm. Weight 70kg.    NaN  \n",
       "1  MaxHr 190bpm. RestHr 65bpm. Weight 70kg.    NaN  \n",
       "2  MaxHr 190bpm. RestHr 65bpm. Weight 70kg.    NaN  \n",
       "3                              Weight 70kg.    NaN  \n",
       "4  MaxHr 190bpm. RestHr 65bpm. Weight 70kg.    NaN  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Distance (km)</th>\n",
       "      <th>Elevation Gain (m)</th>\n",
       "      <th>Delete</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>573.000000</td>\n",
       "      <td>573.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>35.888656</td>\n",
       "      <td>620.718325</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>29.855291</td>\n",
       "      <td>601.756726</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>9.300000</td>\n",
       "      <td>189.400000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>32.900000</td>\n",
       "      <td>526.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>51.900000</td>\n",
       "      <td>884.600000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>170.600000</td>\n",
       "      <td>3617.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Distance (km)  Elevation Gain (m)  Delete\n",
       "count     573.000000          573.000000     0.0\n",
       "mean       35.888656          620.718325     NaN\n",
       "std        29.855291          601.756726     NaN\n",
       "min         0.000000            0.000000     NaN\n",
       "25%         9.300000          189.400000     NaN\n",
       "50%        32.900000          526.000000     NaN\n",
       "75%        51.900000          884.600000     NaN\n",
       "max       170.600000         3617.000000     NaN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before feature pre-processing\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AvgHR</th>\n",
       "      <th>Date</th>\n",
       "      <th>Type</th>\n",
       "      <th>Distance (km)</th>\n",
       "      <th>Avg Pace (/km)</th>\n",
       "      <th>Calories</th>\n",
       "      <th>HRSS</th>\n",
       "      <th>Elevation Gain (m)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>158</td>\n",
       "      <td>2019-09-22T16:27:01-04:00</td>\n",
       "      <td>Ride</td>\n",
       "      <td>62.5</td>\n",
       "      <td>01:53</td>\n",
       "      <td>1772</td>\n",
       "      <td>144</td>\n",
       "      <td>589.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>2019-09-21T11:57:23-04:00</td>\n",
       "      <td>Ride</td>\n",
       "      <td>80.1</td>\n",
       "      <td>02:05</td>\n",
       "      <td>2432</td>\n",
       "      <td>217</td>\n",
       "      <td>890.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>159</td>\n",
       "      <td>2019-09-20T17:55:47-04:00</td>\n",
       "      <td>Ride</td>\n",
       "      <td>35.2</td>\n",
       "      <td>01:56</td>\n",
       "      <td>1029</td>\n",
       "      <td>87</td>\n",
       "      <td>314.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-</td>\n",
       "      <td>2019-09-19T23:45:00-04:00</td>\n",
       "      <td>Workout</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>161</td>\n",
       "      <td>2019-09-18T17:41:48-04:00</td>\n",
       "      <td>Ride</td>\n",
       "      <td>45.6</td>\n",
       "      <td>01:54</td>\n",
       "      <td>1326</td>\n",
       "      <td>119</td>\n",
       "      <td>447.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  AvgHR                       Date     Type  Distance (km) Avg Pace (/km)  \\\n",
       "0   158  2019-09-22T16:27:01-04:00     Ride           62.5          01:53   \n",
       "1   158  2019-09-21T11:57:23-04:00     Ride           80.1          02:05   \n",
       "2   159  2019-09-20T17:55:47-04:00     Ride           35.2          01:56   \n",
       "3     -  2019-09-19T23:45:00-04:00  Workout            0.0              -   \n",
       "4   161  2019-09-18T17:41:48-04:00     Ride           45.6          01:54   \n",
       "\n",
       "  Calories HRSS  Elevation Gain (m)  \n",
       "0     1772  144               589.0  \n",
       "1     2432  217               890.0  \n",
       "2     1029   87               314.0  \n",
       "3        -    -                 0.0  \n",
       "4     1326  119               447.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select features and rename columns\n",
    "df = df[['Avg HR (bpm)','Date','Type','Distance (km)','Avg Pace (/km)','Calories','HRSS','Elevation Gain (m)']]\n",
    "df = df.rename(columns={\"Avg HR (bpm)\": \"AvgHR\"})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AvgHR</th>\n",
       "      <th>Distance (km)</th>\n",
       "      <th>Avg Pace (/km)</th>\n",
       "      <th>Calories</th>\n",
       "      <th>HRSS</th>\n",
       "      <th>Elevation Gain (m)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>309.000000</td>\n",
       "      <td>309.000000</td>\n",
       "      <td>309.000000</td>\n",
       "      <td>309.000000</td>\n",
       "      <td>309.000000</td>\n",
       "      <td>309.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>155.032362</td>\n",
       "      <td>52.237540</td>\n",
       "      <td>150.624595</td>\n",
       "      <td>1573.423948</td>\n",
       "      <td>152.022654</td>\n",
       "      <td>888.765372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>12.505282</td>\n",
       "      <td>27.117491</td>\n",
       "      <td>42.469105</td>\n",
       "      <td>848.025054</td>\n",
       "      <td>82.021593</td>\n",
       "      <td>650.796847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>149.000000</td>\n",
       "      <td>32.900000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>979.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>492.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>50.100000</td>\n",
       "      <td>151.000000</td>\n",
       "      <td>1354.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>715.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>60.400000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>1827.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>1128.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>191.000000</td>\n",
       "      <td>170.600000</td>\n",
       "      <td>477.000000</td>\n",
       "      <td>5415.000000</td>\n",
       "      <td>471.000000</td>\n",
       "      <td>3617.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            AvgHR  Distance (km)  Avg Pace (/km)     Calories        HRSS  \\\n",
       "count  309.000000     309.000000      309.000000   309.000000  309.000000   \n",
       "mean   155.032362      52.237540      150.624595  1573.423948  152.022654   \n",
       "std     12.505282      27.117491       42.469105   848.025054   82.021593   \n",
       "min     84.000000       1.400000       86.000000    48.000000    3.000000   \n",
       "25%    149.000000      32.900000      119.000000   979.000000   98.000000   \n",
       "50%    155.000000      50.100000      151.000000  1354.000000  128.000000   \n",
       "75%    161.000000      60.400000      168.000000  1827.000000  187.000000   \n",
       "max    191.000000     170.600000      477.000000  5415.000000  471.000000   \n",
       "\n",
       "       Elevation Gain (m)  \n",
       "count          309.000000  \n",
       "mean           888.765372  \n",
       "std            650.796847  \n",
       "min              0.000000  \n",
       "25%            492.000000  \n",
       "50%            715.000000  \n",
       "75%           1128.900000  \n",
       "max           3617.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define features and pre-process data for final dataset\n",
    "from dateutil.parser import parse\n",
    "# Filter rows to include only those with AvgHR\n",
    "df = df[df.AvgHR != '-']\n",
    "# Only include Ride and VirtualRide types\n",
    "types = ['Ride', 'VirtualRide']\n",
    "df = df[df.Type.isin(types)]\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "# Convert features to numbers\n",
    "df[\"AvgHR\"] = pd.to_numeric(df[\"AvgHR\"])\n",
    "df[\"Calories\"] = pd.to_numeric(df[\"Calories\"])\n",
    "df[\"HRSS\"] = pd.to_numeric(df[\"HRSS\"])\n",
    "df.head()\n",
    "# Convert Avg Pace to seconds, parse Date and Time as separate columns\n",
    "for i in range(df.shape[0]):\n",
    "    #print('done')\n",
    "    #print(df.loc[i,'Avg Pace (/km)'])\n",
    "    (m, s) = str(df.loc[i,'Avg Pace (/km)']).split(':')\n",
    "    df.loc[i,'Avg Pace (/km)']= (int(m) * 60) + int(s)\n",
    "    dt = parse(df.loc[i,'Date'])\n",
    "    df.loc[i,'Date'] = dt.date()\n",
    "    df.loc[i,'Time'] = dt.time()\n",
    "# Convert Avg Pace to number\n",
    "df['Avg Pace (/km)'] = pd.to_numeric(df['Avg Pace (/km)'])\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AvgHR</th>\n",
       "      <th>Distance (km)</th>\n",
       "      <th>Avg Pace (/km)</th>\n",
       "      <th>Calories</th>\n",
       "      <th>HRSS</th>\n",
       "      <th>Elevation Gain (m)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Ride</td>\n",
       "      <td>154.535593</td>\n",
       "      <td>53.271525</td>\n",
       "      <td>152.298305</td>\n",
       "      <td>1615.294915</td>\n",
       "      <td>155.122034</td>\n",
       "      <td>912.209831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>VirtualRide</td>\n",
       "      <td>165.500000</td>\n",
       "      <td>30.450000</td>\n",
       "      <td>115.357143</td>\n",
       "      <td>691.142857</td>\n",
       "      <td>86.714286</td>\n",
       "      <td>394.757143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  AvgHR  Distance (km)  Avg Pace (/km)     Calories  \\\n",
       "Type                                                                  \n",
       "Ride         154.535593      53.271525      152.298305  1615.294915   \n",
       "VirtualRide  165.500000      30.450000      115.357143   691.142857   \n",
       "\n",
       "                   HRSS  Elevation Gain (m)  \n",
       "Type                                         \n",
       "Ride         155.122034          912.209831  \n",
       "VirtualRide   86.714286          394.757143  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['Type']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">AvgHR</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Distance (km)</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"2\" halign=\"left\">HRSS</th>\n",
       "      <th colspan=\"8\" halign=\"left\">Elevation Gain (m)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>...</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Ride</td>\n",
       "      <td>295.0</td>\n",
       "      <td>154.535593</td>\n",
       "      <td>12.103490</td>\n",
       "      <td>84.0</td>\n",
       "      <td>149.00</td>\n",
       "      <td>155.0</td>\n",
       "      <td>160.50</td>\n",
       "      <td>191.0</td>\n",
       "      <td>295.0</td>\n",
       "      <td>53.271525</td>\n",
       "      <td>...</td>\n",
       "      <td>189.5</td>\n",
       "      <td>471.0</td>\n",
       "      <td>295.0</td>\n",
       "      <td>912.209831</td>\n",
       "      <td>656.472151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>743.0</td>\n",
       "      <td>1131.85</td>\n",
       "      <td>3617.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>VirtualRide</td>\n",
       "      <td>14.0</td>\n",
       "      <td>165.500000</td>\n",
       "      <td>16.383622</td>\n",
       "      <td>135.0</td>\n",
       "      <td>156.75</td>\n",
       "      <td>163.0</td>\n",
       "      <td>175.75</td>\n",
       "      <td>191.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>30.450000</td>\n",
       "      <td>...</td>\n",
       "      <td>94.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>394.757143</td>\n",
       "      <td>114.192663</td>\n",
       "      <td>230.6</td>\n",
       "      <td>301.0</td>\n",
       "      <td>368.5</td>\n",
       "      <td>500.00</td>\n",
       "      <td>558.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             AvgHR                                                       \\\n",
       "             count        mean        std    min     25%    50%     75%   \n",
       "Type                                                                      \n",
       "Ride         295.0  154.535593  12.103490   84.0  149.00  155.0  160.50   \n",
       "VirtualRide   14.0  165.500000  16.383622  135.0  156.75  163.0  175.75   \n",
       "\n",
       "                   Distance (km)             ...   HRSS         \\\n",
       "               max         count       mean  ...    75%    max   \n",
       "Type                                         ...                 \n",
       "Ride         191.0         295.0  53.271525  ...  189.5  471.0   \n",
       "VirtualRide  191.0          14.0  30.450000  ...   94.0  127.0   \n",
       "\n",
       "            Elevation Gain (m)                                               \\\n",
       "                         count        mean         std    min    25%    50%   \n",
       "Type                                                                          \n",
       "Ride                     295.0  912.209831  656.472151    0.0  512.0  743.0   \n",
       "VirtualRide               14.0  394.757143  114.192663  230.6  301.0  368.5   \n",
       "\n",
       "                              \n",
       "                 75%     max  \n",
       "Type                          \n",
       "Ride         1131.85  3617.0  \n",
       "VirtualRide   500.00   558.0  \n",
       "\n",
       "[2 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['Type']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary labels for High 'AvgHR' (1) and Low 'AvgHR' (0) based on threshold of 154 bpm\n",
    "for j in range(df.shape[0]):\n",
    "    if int(df.loc[j,'AvgHR']) > 154:\n",
    "        #print(df.loc[j,'AvgHR'])\n",
    "        df.loc[j,'AvgHR_bin'] = 1\n",
    "    else: \n",
    "        df.loc[j,'AvgHR_bin'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AvgHR</th>\n",
       "      <th>Date</th>\n",
       "      <th>Type</th>\n",
       "      <th>Distance (km)</th>\n",
       "      <th>Avg Pace (/km)</th>\n",
       "      <th>Calories</th>\n",
       "      <th>HRSS</th>\n",
       "      <th>Elevation Gain (m)</th>\n",
       "      <th>Time</th>\n",
       "      <th>AvgHR_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>158</td>\n",
       "      <td>2019-09-22</td>\n",
       "      <td>Ride</td>\n",
       "      <td>62.5</td>\n",
       "      <td>113</td>\n",
       "      <td>1772</td>\n",
       "      <td>144</td>\n",
       "      <td>589.0</td>\n",
       "      <td>16:27:01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>2019-09-21</td>\n",
       "      <td>Ride</td>\n",
       "      <td>80.1</td>\n",
       "      <td>125</td>\n",
       "      <td>2432</td>\n",
       "      <td>217</td>\n",
       "      <td>890.0</td>\n",
       "      <td>11:57:23</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>159</td>\n",
       "      <td>2019-09-20</td>\n",
       "      <td>Ride</td>\n",
       "      <td>35.2</td>\n",
       "      <td>116</td>\n",
       "      <td>1029</td>\n",
       "      <td>87</td>\n",
       "      <td>314.0</td>\n",
       "      <td>17:55:47</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>161</td>\n",
       "      <td>2019-09-18</td>\n",
       "      <td>Ride</td>\n",
       "      <td>45.6</td>\n",
       "      <td>114</td>\n",
       "      <td>1326</td>\n",
       "      <td>119</td>\n",
       "      <td>447.0</td>\n",
       "      <td>17:41:48</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>155</td>\n",
       "      <td>2019-09-17</td>\n",
       "      <td>Ride</td>\n",
       "      <td>41.1</td>\n",
       "      <td>120</td>\n",
       "      <td>1156</td>\n",
       "      <td>96</td>\n",
       "      <td>454.0</td>\n",
       "      <td>16:47:31</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>149</td>\n",
       "      <td>2017-08-18</td>\n",
       "      <td>Ride</td>\n",
       "      <td>70.9</td>\n",
       "      <td>146</td>\n",
       "      <td>1817</td>\n",
       "      <td>175</td>\n",
       "      <td>1202.7</td>\n",
       "      <td>10:09:39</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>148</td>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>Ride</td>\n",
       "      <td>70.1</td>\n",
       "      <td>159</td>\n",
       "      <td>1827</td>\n",
       "      <td>191</td>\n",
       "      <td>1185.3</td>\n",
       "      <td>11:44:59</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>306</td>\n",
       "      <td>150</td>\n",
       "      <td>2017-08-14</td>\n",
       "      <td>Ride</td>\n",
       "      <td>53.7</td>\n",
       "      <td>163</td>\n",
       "      <td>1432</td>\n",
       "      <td>161</td>\n",
       "      <td>1015.8</td>\n",
       "      <td>13:41:10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>307</td>\n",
       "      <td>131</td>\n",
       "      <td>2017-08-13</td>\n",
       "      <td>Ride</td>\n",
       "      <td>40.5</td>\n",
       "      <td>161</td>\n",
       "      <td>941</td>\n",
       "      <td>70</td>\n",
       "      <td>350.4</td>\n",
       "      <td>13:14:20</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>308</td>\n",
       "      <td>151</td>\n",
       "      <td>2017-08-05</td>\n",
       "      <td>Ride</td>\n",
       "      <td>52.0</td>\n",
       "      <td>138</td>\n",
       "      <td>1332</td>\n",
       "      <td>126</td>\n",
       "      <td>811.4</td>\n",
       "      <td>09:32:39</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>309 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     AvgHR        Date  Type  Distance (km)  Avg Pace (/km)  Calories  HRSS  \\\n",
       "0      158  2019-09-22  Ride           62.5             113      1772   144   \n",
       "1      158  2019-09-21  Ride           80.1             125      2432   217   \n",
       "2      159  2019-09-20  Ride           35.2             116      1029    87   \n",
       "3      161  2019-09-18  Ride           45.6             114      1326   119   \n",
       "4      155  2019-09-17  Ride           41.1             120      1156    96   \n",
       "..     ...         ...   ...            ...             ...       ...   ...   \n",
       "304    149  2017-08-18  Ride           70.9             146      1817   175   \n",
       "305    148  2017-08-16  Ride           70.1             159      1827   191   \n",
       "306    150  2017-08-14  Ride           53.7             163      1432   161   \n",
       "307    131  2017-08-13  Ride           40.5             161       941    70   \n",
       "308    151  2017-08-05  Ride           52.0             138      1332   126   \n",
       "\n",
       "     Elevation Gain (m)      Time  AvgHR_bin  \n",
       "0                 589.0  16:27:01        1.0  \n",
       "1                 890.0  11:57:23        1.0  \n",
       "2                 314.0  17:55:47        1.0  \n",
       "3                 447.0  17:41:48        1.0  \n",
       "4                 454.0  16:47:31        1.0  \n",
       "..                  ...       ...        ...  \n",
       "304              1202.7  10:09:39        0.0  \n",
       "305              1185.3  11:44:59        0.0  \n",
       "306              1015.8  13:41:10        0.0  \n",
       "307               350.4  13:14:20        0.0  \n",
       "308               811.4  09:32:39        0.0  \n",
       "\n",
       "[309 rows x 10 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AvgHR</th>\n",
       "      <th>Distance (km)</th>\n",
       "      <th>Avg Pace (/km)</th>\n",
       "      <th>Calories</th>\n",
       "      <th>HRSS</th>\n",
       "      <th>Elevation Gain (m)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AvgHR_bin</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.0</td>\n",
       "      <td>145.794521</td>\n",
       "      <td>55.214384</td>\n",
       "      <td>167.794521</td>\n",
       "      <td>1644.650685</td>\n",
       "      <td>145.445205</td>\n",
       "      <td>1048.490411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1.0</td>\n",
       "      <td>163.306748</td>\n",
       "      <td>49.571166</td>\n",
       "      <td>135.245399</td>\n",
       "      <td>1509.625767</td>\n",
       "      <td>157.914110</td>\n",
       "      <td>745.698773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                AvgHR  Distance (km)  Avg Pace (/km)     Calories        HRSS  \\\n",
       "AvgHR_bin                                                                       \n",
       "0.0        145.794521      55.214384      167.794521  1644.650685  145.445205   \n",
       "1.0        163.306748      49.571166      135.245399  1509.625767  157.914110   \n",
       "\n",
       "           Elevation Gain (m)  \n",
       "AvgHR_bin                      \n",
       "0.0               1048.490411  \n",
       "1.0                745.698773  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('AvgHR_bin').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots for Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/pandas/plotting/_matplotlib/converter.py:103: FutureWarning: Using an implicitly registered datetime converter for a matplotlib plotting method. The converter was registered by pandas on import. Future versions of pandas will require you to explicitly register matplotlib converters.\n",
      "\n",
      "To register the converters:\n",
      "\t>>> from pandas.plotting import register_matplotlib_converters\n",
      "\t>>> register_matplotlib_converters()\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot AvgHR over time for all virtual and outdoor bike rides\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot_date(df['Date'],df['AvgHR'])\n",
    "plt.title('AvgHR Over Time for Rides')\n",
    "plt.xlabel('Date')\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('AvgHR')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of High AvgHR is 52.75\n",
      "percentage of Low AvgHR is 47.25\n"
     ]
    }
   ],
   "source": [
    "# Calculate sample size of each class\n",
    "count_no_sub = len(df[df['AvgHR_bin']==1])\n",
    "count_sub = len(df[df['AvgHR_bin']==0])\n",
    "pct_of_no_sub = count_no_sub/(count_no_sub+count_sub)\n",
    "print(\"percentage of High AvgHR is\", '%.2f' %(pct_of_no_sub*100))\n",
    "pct_of_sub = count_sub/(count_no_sub+count_sub)\n",
    "print(\"percentage of Low AvgHR is\", '%.2f' %(pct_of_sub*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Frequency of Average HR')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot distribution of AvgHR for Rides and VirtualRides\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.crosstab(df.Type,df.AvgHR_bin).plot(kind='bar')\n",
    "plt.title('Average HR for Type')\n",
    "plt.xlabel('Type')\n",
    "plt.ylabel('Frequency of Average HR')\n",
    "#plt.savefig('purchase_avghr_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Distance (km)  Avg Pace (/km)  HRSS  Elevation Gain (m)\n",
      "0             62.5             113   144               589.0\n",
      "1             80.1             125   217               890.0\n",
      "2             35.2             116    87               314.0\n",
      "3             45.6             114   119               447.0\n",
      "4             41.1             120    96               454.0\n",
      "..             ...             ...   ...                 ...\n",
      "289           40.6             233    86               716.4\n",
      "290          124.3             159   391              2602.0\n",
      "291           53.4             151   187              1012.3\n",
      "292           53.4             152   169              1006.7\n",
      "293           40.3             170   115               794.6\n",
      "\n",
      "[294 rows x 4 columns]      AvgHR_bin\n",
      "0          1.0\n",
      "1          1.0\n",
      "2          1.0\n",
      "3          1.0\n",
      "4          1.0\n",
      "..         ...\n",
      "289        0.0\n",
      "290        0.0\n",
      "291        1.0\n",
      "292        1.0\n",
      "293        0.0\n",
      "\n",
      "[294 rows x 1 columns]\n",
      "[ True  True  True  True]\n",
      "[1 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Define train and test set, perform RFE\n",
    "window_size = 294\n",
    "# removed Calories as feature since p-value was 0.07 > 0.05 (from below Logit function), as recommended\n",
    "df_vars = ['Distance (km)', 'Avg Pace (/km)', 'HRSS', 'Elevation Gain (m)','AvgHR_bin']\n",
    "df_final = df[df_vars]\n",
    "df_final_vars=df_final.columns.values.tolist()\n",
    "y=df_final.AvgHR_bin\n",
    "X=[i for i in df_final_vars if i not in y]\n",
    "\n",
    "X = df_final.loc[:, df_final.columns != 'AvgHR_bin']\n",
    "y = df_final.loc[:, df_final.columns == 'AvgHR_bin']\n",
    "\n",
    "# Configure train and test sets\n",
    "X_train = X.iloc[window_size:]\n",
    "y_train = y.iloc[window_size:]\n",
    "\n",
    "X_test = X.iloc[:window_size]\n",
    "y_test = y.iloc[:window_size]\n",
    "print(X_test, y_test)\n",
    "\n",
    "# Perform RFE (recursive feature elimination) to determine ranking of features\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "rfe = RFE(logreg, 20)\n",
    "rfe = rfe.fit(X_train, y_train.values.ravel())\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.307462\n",
      "         Iterations 8\n",
      "                          Results: Logit\n",
      "==================================================================\n",
      "Model:               Logit            Pseudo R-squared: 0.555     \n",
      "Dependent Variable:  AvgHR_bin        AIC:              198.0117  \n",
      "Date:                2019-12-04 22:21 BIC:              212.9451  \n",
      "No. Observations:    309              Log-Likelihood:   -95.006   \n",
      "Df Model:            3                LL-Null:          -213.71   \n",
      "Df Residuals:        305              LLR p-value:      3.4432e-51\n",
      "Converged:           1.0000           Scale:            1.0000    \n",
      "No. Iterations:      8.0000                                       \n",
      "------------------------------------------------------------------\n",
      "                    Coef.  Std.Err.    z    P>|z|   [0.025  0.975]\n",
      "------------------------------------------------------------------\n",
      "Distance (km)      -0.1857   0.0271 -6.8416 0.0000 -0.2389 -0.1325\n",
      "Avg Pace (/km)     -0.0120   0.0029 -4.1900 0.0000 -0.0175 -0.0064\n",
      "HRSS                0.1395   0.0176  7.9317 0.0000  0.1050  0.1740\n",
      "Elevation Gain (m) -0.0107   0.0015 -7.2999 0.0000 -0.0136 -0.0078\n",
      "==================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Implement Logit model to determine p-values and coefficients for each feature\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "logit_model=sm.Logit(y,X)\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate Over All Window Sizes for LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Predictions:  1\n",
      "accuracy for window size 308: 1.0\n",
      "# Predictions:  2\n",
      "accuracy for window size 307: 1.0\n",
      "# Predictions:  3\n",
      "accuracy for window size 306: 1.0\n",
      "# Predictions:  4\n",
      "accuracy for window size 305: 1.0\n",
      "# Predictions:  5\n",
      "accuracy for window size 304: 0.8\n",
      "# Predictions:  6\n",
      "accuracy for window size 303: 0.6666666666666666\n",
      "# Predictions:  7\n",
      "accuracy for window size 302: 0.7142857142857143\n",
      "# Predictions:  8\n",
      "accuracy for window size 301: 0.75\n",
      "# Predictions:  9\n",
      "accuracy for window size 300: 0.7777777777777778\n",
      "# Predictions:  10\n",
      "accuracy for window size 299: 0.8\n",
      "# Predictions:  11\n",
      "accuracy for window size 298: 0.8181818181818182\n",
      "# Predictions:  12\n",
      "accuracy for window size 297: 0.8333333333333334\n",
      "# Predictions:  13\n",
      "accuracy for window size 296: 0.8461538461538461\n",
      "# Predictions:  14\n",
      "accuracy for window size 295: 0.7142857142857143\n",
      "# Predictions:  15\n",
      "accuracy for window size 294: 0.7333333333333333\n",
      "# Predictions:  16\n",
      "accuracy for window size 293: 0.75\n",
      "# Predictions:  17\n",
      "accuracy for window size 292: 0.7647058823529411\n",
      "# Predictions:  18\n",
      "accuracy for window size 291: 0.7777777777777778\n",
      "# Predictions:  19\n",
      "accuracy for window size 290: 0.7894736842105263\n",
      "# Predictions:  20\n",
      "accuracy for window size 289: 0.75\n",
      "# Predictions:  21\n",
      "accuracy for window size 288: 0.8095238095238095\n",
      "# Predictions:  22\n",
      "accuracy for window size 287: 0.8181818181818182\n",
      "# Predictions:  23\n",
      "accuracy for window size 286: 0.782608695652174\n",
      "# Predictions:  24\n",
      "accuracy for window size 285: 0.7916666666666666\n",
      "# Predictions:  25\n",
      "accuracy for window size 284: 0.8\n",
      "# Predictions:  26\n",
      "accuracy for window size 283: 0.8076923076923077\n",
      "# Predictions:  27\n",
      "accuracy for window size 282: 0.8148148148148148\n",
      "# Predictions:  28\n",
      "accuracy for window size 281: 0.8214285714285714\n",
      "# Predictions:  29\n",
      "accuracy for window size 280: 0.8620689655172413\n",
      "# Predictions:  30\n",
      "accuracy for window size 279: 0.8666666666666667\n",
      "# Predictions:  31\n",
      "accuracy for window size 278: 0.8387096774193549\n",
      "# Predictions:  32\n",
      "accuracy for window size 277: 0.84375\n",
      "# Predictions:  33\n",
      "accuracy for window size 276: 0.8181818181818182\n",
      "# Predictions:  34\n",
      "accuracy for window size 275: 0.8235294117647058\n",
      "# Predictions:  35\n",
      "accuracy for window size 274: 0.8285714285714286\n",
      "# Predictions:  36\n",
      "accuracy for window size 273: 0.8611111111111112\n",
      "# Predictions:  37\n",
      "accuracy for window size 272: 0.8648648648648649\n",
      "# Predictions:  38\n",
      "accuracy for window size 271: 0.868421052631579\n",
      "# Predictions:  39\n",
      "accuracy for window size 270: 0.8717948717948718\n",
      "# Predictions:  40\n",
      "accuracy for window size 269: 0.875\n",
      "# Predictions:  41\n",
      "accuracy for window size 268: 0.8780487804878049\n",
      "# Predictions:  42\n",
      "accuracy for window size 267: 0.8809523809523809\n",
      "# Predictions:  43\n",
      "accuracy for window size 266: 0.8837209302325582\n",
      "# Predictions:  44\n",
      "accuracy for window size 265: 0.8863636363636364\n",
      "# Predictions:  45\n",
      "accuracy for window size 264: 0.8888888888888888\n",
      "# Predictions:  46\n",
      "accuracy for window size 263: 0.8913043478260869\n",
      "# Predictions:  47\n",
      "accuracy for window size 262: 0.8723404255319149\n",
      "# Predictions:  48\n",
      "accuracy for window size 261: 0.875\n",
      "# Predictions:  49\n",
      "accuracy for window size 260: 0.8979591836734694\n",
      "# Predictions:  50\n",
      "accuracy for window size 259: 0.92\n",
      "# Predictions:  51\n",
      "accuracy for window size 258: 0.9215686274509803\n",
      "# Predictions:  52\n",
      "accuracy for window size 257: 0.9038461538461539\n",
      "# Predictions:  53\n",
      "accuracy for window size 256: 0.9245283018867925\n",
      "# Predictions:  54\n",
      "accuracy for window size 255: 0.9074074074074074\n",
      "# Predictions:  55\n",
      "accuracy for window size 254: 0.8909090909090909\n",
      "# Predictions:  56\n",
      "accuracy for window size 253: 0.8928571428571429\n",
      "# Predictions:  57\n",
      "accuracy for window size 252: 0.8771929824561403\n",
      "# Predictions:  58\n",
      "accuracy for window size 251: 0.896551724137931\n",
      "# Predictions:  59\n",
      "accuracy for window size 250: 0.8813559322033898\n",
      "# Predictions:  60\n",
      "accuracy for window size 249: 0.8833333333333333\n",
      "# Predictions:  61\n",
      "accuracy for window size 248: 0.8852459016393442\n",
      "# Predictions:  62\n",
      "accuracy for window size 247: 0.8870967741935484\n",
      "# Predictions:  63\n",
      "accuracy for window size 246: 0.8888888888888888\n",
      "# Predictions:  64\n",
      "accuracy for window size 245: 0.890625\n",
      "# Predictions:  65\n",
      "accuracy for window size 244: 0.8923076923076924\n",
      "# Predictions:  66\n",
      "accuracy for window size 243: 0.9090909090909091\n",
      "# Predictions:  67\n",
      "accuracy for window size 242: 0.8955223880597015\n",
      "# Predictions:  68\n",
      "accuracy for window size 241: 0.8970588235294118\n",
      "# Predictions:  69\n",
      "accuracy for window size 240: 0.8985507246376812\n",
      "# Predictions:  70\n",
      "accuracy for window size 239: 0.9142857142857143\n",
      "# Predictions:  71\n",
      "accuracy for window size 238: 0.9154929577464789\n",
      "# Predictions:  72\n",
      "accuracy for window size 237: 0.9166666666666666\n",
      "# Predictions:  73\n",
      "accuracy for window size 236: 0.9041095890410958\n",
      "# Predictions:  74\n",
      "accuracy for window size 235: 0.9054054054054054\n",
      "# Predictions:  75\n",
      "accuracy for window size 234: 0.9066666666666666\n",
      "# Predictions:  76\n",
      "accuracy for window size 233: 0.9078947368421053\n",
      "# Predictions:  77\n",
      "accuracy for window size 232: 0.9090909090909091\n",
      "# Predictions:  78\n",
      "accuracy for window size 231: 0.9102564102564102\n",
      "# Predictions:  79\n",
      "accuracy for window size 230: 0.9113924050632911\n",
      "# Predictions:  80\n",
      "accuracy for window size 229: 0.9125\n",
      "# Predictions:  81\n",
      "accuracy for window size 228: 0.9135802469135802\n",
      "# Predictions:  82\n",
      "accuracy for window size 227: 0.9146341463414634\n",
      "# Predictions:  83\n",
      "accuracy for window size 226: 0.9156626506024096\n",
      "# Predictions:  84\n",
      "accuracy for window size 225: 0.9047619047619048\n",
      "# Predictions:  85\n",
      "accuracy for window size 224: 0.8941176470588236\n",
      "# Predictions:  86\n",
      "accuracy for window size 223: 0.8953488372093024\n",
      "# Predictions:  87\n",
      "accuracy for window size 222: 0.896551724137931\n",
      "# Predictions:  88\n",
      "accuracy for window size 221: 0.8977272727272727\n",
      "# Predictions:  89\n",
      "accuracy for window size 220: 0.898876404494382\n",
      "# Predictions:  90\n",
      "accuracy for window size 219: 0.9\n",
      "# Predictions:  91\n",
      "accuracy for window size 218: 0.9010989010989011\n",
      "# Predictions:  92\n",
      "accuracy for window size 217: 0.8913043478260869\n",
      "# Predictions:  93\n",
      "accuracy for window size 216: 0.8924731182795699\n",
      "# Predictions:  94\n",
      "accuracy for window size 215: 0.8936170212765957\n",
      "# Predictions:  95\n",
      "accuracy for window size 214: 0.8947368421052632\n",
      "# Predictions:  96\n",
      "accuracy for window size 213: 0.90625\n",
      "# Predictions:  97\n",
      "accuracy for window size 212: 0.8969072164948454\n",
      "# Predictions:  98\n",
      "accuracy for window size 211: 0.8979591836734694\n",
      "# Predictions:  99\n",
      "accuracy for window size 210: 0.9090909090909091\n",
      "# Predictions:  100\n",
      "accuracy for window size 209: 0.91\n",
      "# Predictions:  101\n",
      "accuracy for window size 208: 0.9108910891089109\n",
      "# Predictions:  102\n",
      "accuracy for window size 207: 0.9117647058823529\n",
      "# Predictions:  103\n",
      "accuracy for window size 206: 0.912621359223301\n",
      "# Predictions:  104\n",
      "accuracy for window size 205: 0.9134615384615384\n",
      "# Predictions:  105\n",
      "accuracy for window size 204: 0.9142857142857143\n",
      "# Predictions:  106\n",
      "accuracy for window size 203: 0.9150943396226415\n",
      "# Predictions:  107\n",
      "accuracy for window size 202: 0.9345794392523364\n",
      "# Predictions:  108\n",
      "accuracy for window size 201: 0.9351851851851852\n",
      "# Predictions:  109\n",
      "accuracy for window size 200: 0.9174311926605505\n",
      "# Predictions:  110\n",
      "accuracy for window size 199: 0.9363636363636364\n",
      "# Predictions:  111\n",
      "accuracy for window size 198: 0.9369369369369369\n",
      "# Predictions:  112\n",
      "accuracy for window size 197: 0.9375\n",
      "# Predictions:  113\n",
      "accuracy for window size 196: 0.9203539823008849\n",
      "# Predictions:  114\n",
      "accuracy for window size 195: 0.9298245614035088\n",
      "# Predictions:  115\n",
      "accuracy for window size 194: 0.9304347826086956\n",
      "# Predictions:  116\n",
      "accuracy for window size 193: 0.9310344827586207\n",
      "# Predictions:  117\n",
      "accuracy for window size 192: 0.9316239316239316\n",
      "# Predictions:  118\n",
      "accuracy for window size 191: 0.9322033898305084\n",
      "# Predictions:  119\n",
      "accuracy for window size 190: 0.9327731092436975\n",
      "# Predictions:  120\n",
      "accuracy for window size 189: 0.9333333333333333\n",
      "# Predictions:  121\n",
      "accuracy for window size 188: 0.9338842975206612\n",
      "# Predictions:  122\n",
      "accuracy for window size 187: 0.9344262295081968\n",
      "# Predictions:  123\n",
      "accuracy for window size 186: 0.926829268292683\n",
      "# Predictions:  124\n",
      "accuracy for window size 185: 0.9274193548387096\n",
      "# Predictions:  125\n",
      "accuracy for window size 184: 0.928\n",
      "# Predictions:  126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for window size 183: 0.9365079365079365\n",
      "# Predictions:  127\n",
      "accuracy for window size 182: 0.937007874015748\n",
      "# Predictions:  128\n",
      "accuracy for window size 181: 0.9375\n",
      "# Predictions:  129\n",
      "accuracy for window size 180: 0.9302325581395349\n",
      "# Predictions:  130\n",
      "accuracy for window size 179: 0.9307692307692308\n",
      "# Predictions:  131\n",
      "accuracy for window size 178: 0.9312977099236641\n",
      "# Predictions:  132\n",
      "accuracy for window size 177: 0.9318181818181818\n",
      "# Predictions:  133\n",
      "accuracy for window size 176: 0.9323308270676691\n",
      "# Predictions:  134\n",
      "accuracy for window size 175: 0.9328358208955224\n",
      "# Predictions:  135\n",
      "accuracy for window size 174: 0.9333333333333333\n",
      "# Predictions:  136\n",
      "accuracy for window size 173: 0.9264705882352942\n",
      "# Predictions:  137\n",
      "accuracy for window size 172: 0.927007299270073\n",
      "# Predictions:  138\n",
      "accuracy for window size 171: 0.927536231884058\n",
      "# Predictions:  139\n",
      "accuracy for window size 170: 0.9280575539568345\n",
      "# Predictions:  140\n",
      "accuracy for window size 169: 0.9285714285714286\n",
      "# Predictions:  141\n",
      "accuracy for window size 168: 0.9290780141843972\n",
      "# Predictions:  142\n",
      "accuracy for window size 167: 0.9154929577464789\n",
      "# Predictions:  143\n",
      "accuracy for window size 166: 0.916083916083916\n",
      "# Predictions:  144\n",
      "accuracy for window size 165: 0.9166666666666666\n",
      "# Predictions:  145\n",
      "accuracy for window size 164: 0.9172413793103448\n",
      "# Predictions:  146\n",
      "accuracy for window size 163: 0.9246575342465754\n",
      "# Predictions:  147\n",
      "accuracy for window size 162: 0.9251700680272109\n",
      "# Predictions:  148\n",
      "accuracy for window size 161: 0.9256756756756757\n",
      "# Predictions:  149\n",
      "accuracy for window size 160: 0.9261744966442953\n",
      "# Predictions:  150\n",
      "accuracy for window size 159: 0.9266666666666666\n",
      "# Predictions:  151\n",
      "accuracy for window size 158: 0.9271523178807947\n",
      "# Predictions:  152\n",
      "accuracy for window size 157: 0.9210526315789473\n",
      "# Predictions:  153\n",
      "accuracy for window size 156: 0.9215686274509803\n",
      "# Predictions:  154\n",
      "accuracy for window size 155: 0.922077922077922\n",
      "# Predictions:  155\n",
      "accuracy for window size 154: 0.9225806451612903\n",
      "# Predictions:  156\n",
      "accuracy for window size 153: 0.9166666666666666\n",
      "# Predictions:  157\n",
      "accuracy for window size 152: 0.910828025477707\n",
      "# Predictions:  158\n",
      "accuracy for window size 151: 0.9113924050632911\n",
      "# Predictions:  159\n",
      "accuracy for window size 150: 0.9119496855345912\n",
      "# Predictions:  160\n",
      "accuracy for window size 149: 0.9125\n",
      "# Predictions:  161\n",
      "accuracy for window size 148: 0.9130434782608695\n",
      "# Predictions:  162\n",
      "accuracy for window size 147: 0.9135802469135802\n",
      "# Predictions:  163\n",
      "accuracy for window size 146: 0.9141104294478528\n",
      "# Predictions:  164\n",
      "accuracy for window size 145: 0.9207317073170732\n",
      "# Predictions:  165\n",
      "accuracy for window size 144: 0.9212121212121213\n",
      "# Predictions:  166\n",
      "accuracy for window size 143: 0.9216867469879518\n",
      "# Predictions:  167\n",
      "accuracy for window size 142: 0.9221556886227545\n",
      "# Predictions:  168\n",
      "accuracy for window size 141: 0.9166666666666666\n",
      "# Predictions:  169\n",
      "accuracy for window size 140: 0.9112426035502958\n",
      "# Predictions:  170\n",
      "accuracy for window size 139: 0.9058823529411765\n",
      "# Predictions:  171\n",
      "accuracy for window size 138: 0.9064327485380117\n",
      "# Predictions:  172\n",
      "accuracy for window size 137: 0.9069767441860465\n",
      "# Predictions:  173\n",
      "accuracy for window size 136: 0.9075144508670521\n",
      "# Predictions:  174\n",
      "accuracy for window size 135: 0.9022988505747126\n",
      "# Predictions:  175\n",
      "accuracy for window size 134: 0.9028571428571428\n",
      "# Predictions:  176\n",
      "accuracy for window size 133: 0.9034090909090909\n",
      "# Predictions:  177\n",
      "accuracy for window size 132: 0.903954802259887\n",
      "# Predictions:  178\n",
      "accuracy for window size 131: 0.9044943820224719\n",
      "# Predictions:  179\n",
      "accuracy for window size 130: 0.9050279329608939\n",
      "# Predictions:  180\n",
      "accuracy for window size 129: 0.9055555555555556\n",
      "# Predictions:  181\n",
      "accuracy for window size 128: 0.9005524861878453\n",
      "# Predictions:  182\n",
      "accuracy for window size 127: 0.9010989010989011\n",
      "# Predictions:  183\n",
      "accuracy for window size 126: 0.907103825136612\n",
      "# Predictions:  184\n",
      "accuracy for window size 125: 0.8967391304347826\n",
      "# Predictions:  185\n",
      "accuracy for window size 124: 0.8972972972972973\n",
      "# Predictions:  186\n",
      "accuracy for window size 123: 0.9032258064516129\n",
      "# Predictions:  187\n",
      "accuracy for window size 122: 0.8983957219251337\n",
      "# Predictions:  188\n",
      "accuracy for window size 121: 0.898936170212766\n",
      "# Predictions:  189\n",
      "accuracy for window size 120: 0.8994708994708994\n",
      "# Predictions:  190\n",
      "accuracy for window size 119: 0.9\n",
      "# Predictions:  191\n",
      "accuracy for window size 118: 0.8952879581151832\n",
      "# Predictions:  192\n",
      "accuracy for window size 117: 0.8958333333333334\n",
      "# Predictions:  193\n",
      "accuracy for window size 116: 0.8963730569948186\n",
      "# Predictions:  194\n",
      "accuracy for window size 115: 0.8969072164948454\n",
      "# Predictions:  195\n",
      "accuracy for window size 114: 0.8923076923076924\n",
      "# Predictions:  196\n",
      "accuracy for window size 113: 0.8928571428571429\n",
      "# Predictions:  197\n",
      "accuracy for window size 112: 0.8934010152284264\n",
      "# Predictions:  198\n",
      "accuracy for window size 111: 0.898989898989899\n",
      "# Predictions:  199\n",
      "accuracy for window size 110: 0.9045226130653267\n",
      "# Predictions:  200\n",
      "accuracy for window size 109: 0.895\n",
      "# Predictions:  201\n",
      "accuracy for window size 108: 0.8905472636815921\n",
      "# Predictions:  202\n",
      "accuracy for window size 107: 0.8910891089108911\n",
      "# Predictions:  203\n",
      "accuracy for window size 106: 0.8916256157635468\n",
      "# Predictions:  204\n",
      "accuracy for window size 105: 0.8921568627450981\n",
      "# Predictions:  205\n",
      "accuracy for window size 104: 0.8926829268292683\n",
      "# Predictions:  206\n",
      "accuracy for window size 103: 0.8883495145631068\n",
      "# Predictions:  207\n",
      "accuracy for window size 102: 0.8888888888888888\n",
      "# Predictions:  208\n",
      "accuracy for window size 101: 0.8798076923076923\n",
      "# Predictions:  209\n",
      "accuracy for window size 100: 0.8803827751196173\n",
      "# Predictions:  210\n",
      "accuracy for window size 99: 0.8761904761904762\n",
      "# Predictions:  211\n",
      "accuracy for window size 98: 0.8767772511848341\n",
      "# Predictions:  212\n",
      "accuracy for window size 97: 0.8773584905660378\n",
      "# Predictions:  213\n",
      "accuracy for window size 96: 0.8826291079812206\n",
      "# Predictions:  214\n",
      "accuracy for window size 95: 0.883177570093458\n",
      "# Predictions:  215\n",
      "accuracy for window size 94: 0.8883720930232558\n",
      "# Predictions:  216\n",
      "accuracy for window size 93: 0.8796296296296297\n",
      "# Predictions:  217\n",
      "accuracy for window size 92: 0.8755760368663594\n",
      "# Predictions:  218\n",
      "accuracy for window size 91: 0.8715596330275229\n",
      "# Predictions:  219\n",
      "accuracy for window size 90: 0.867579908675799\n",
      "# Predictions:  220\n",
      "accuracy for window size 89: 0.8727272727272727\n",
      "# Predictions:  221\n",
      "accuracy for window size 88: 0.8733031674208145\n",
      "# Predictions:  222\n",
      "accuracy for window size 87: 0.8738738738738738\n",
      "# Predictions:  223\n",
      "accuracy for window size 86: 0.8654708520179372\n",
      "# Predictions:  224\n",
      "accuracy for window size 85: 0.8571428571428571\n",
      "# Predictions:  225\n",
      "accuracy for window size 84: 0.8577777777777778\n",
      "# Predictions:  226\n",
      "accuracy for window size 83: 0.8539823008849557\n",
      "# Predictions:  227\n",
      "accuracy for window size 82: 0.8502202643171806\n",
      "# Predictions:  228\n",
      "accuracy for window size 81: 0.8552631578947368\n",
      "# Predictions:  229\n",
      "accuracy for window size 80: 0.851528384279476\n",
      "# Predictions:  230\n",
      "accuracy for window size 79: 0.8565217391304348\n",
      "# Predictions:  231\n",
      "accuracy for window size 78: 0.8614718614718615\n",
      "# Predictions:  232\n",
      "accuracy for window size 77: 0.8577586206896551\n",
      "# Predictions:  233\n",
      "accuracy for window size 76: 0.8669527896995708\n",
      "# Predictions:  234\n",
      "accuracy for window size 75: 0.8675213675213675\n",
      "# Predictions:  235\n",
      "accuracy for window size 74: 0.8680851063829788\n",
      "# Predictions:  236\n",
      "accuracy for window size 73: 0.8686440677966102\n",
      "# Predictions:  237\n",
      "accuracy for window size 72: 0.8649789029535865\n",
      "# Predictions:  238\n",
      "accuracy for window size 71: 0.8613445378151261\n",
      "# Predictions:  239\n",
      "accuracy for window size 70: 0.8744769874476988\n",
      "# Predictions:  240\n",
      "accuracy for window size 69: 0.8708333333333333\n",
      "# Predictions:  241\n",
      "accuracy for window size 68: 0.8755186721991701\n",
      "# Predictions:  242\n",
      "accuracy for window size 67: 0.8801652892561983\n",
      "# Predictions:  243\n",
      "accuracy for window size 66: 0.8765432098765432\n",
      "# Predictions:  244\n",
      "accuracy for window size 65: 0.8770491803278688\n",
      "# Predictions:  245\n",
      "accuracy for window size 64: 0.8653061224489796\n",
      "# Predictions:  246\n",
      "accuracy for window size 63: 0.8536585365853658\n",
      "# Predictions:  247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for window size 62: 0.8502024291497976\n",
      "# Predictions:  248\n",
      "accuracy for window size 61: 0.8508064516129032\n",
      "# Predictions:  249\n",
      "accuracy for window size 60: 0.8514056224899599\n",
      "# Predictions:  250\n",
      "accuracy for window size 59: 0.844\n",
      "# Predictions:  251\n",
      "accuracy for window size 58: 0.8446215139442231\n",
      "# Predictions:  252\n",
      "accuracy for window size 57: 0.8373015873015873\n",
      "# Predictions:  253\n",
      "accuracy for window size 56: 0.841897233201581\n",
      "# Predictions:  254\n",
      "accuracy for window size 55: 0.8385826771653543\n",
      "# Predictions:  255\n",
      "accuracy for window size 54: 0.8431372549019608\n",
      "# Predictions:  256\n",
      "accuracy for window size 53: 0.83203125\n",
      "# Predictions:  257\n",
      "accuracy for window size 52: 0.8249027237354085\n",
      "# Predictions:  258\n",
      "accuracy for window size 51: 0.8333333333333334\n",
      "# Predictions:  259\n",
      "accuracy for window size 50: 0.833976833976834\n",
      "# Predictions:  260\n",
      "accuracy for window size 49: 0.8307692307692308\n",
      "# Predictions:  261\n",
      "accuracy for window size 48: 0.8275862068965517\n",
      "# Predictions:  262\n",
      "accuracy for window size 47: 0.8358778625954199\n",
      "# Predictions:  263\n",
      "accuracy for window size 46: 0.8326996197718631\n",
      "# Predictions:  264\n",
      "accuracy for window size 45: 0.8333333333333334\n",
      "# Predictions:  265\n",
      "accuracy for window size 44: 0.8264150943396227\n",
      "# Predictions:  266\n",
      "accuracy for window size 43: 0.8308270676691729\n",
      "# Predictions:  267\n",
      "accuracy for window size 42: 0.8277153558052435\n",
      "# Predictions:  268\n",
      "accuracy for window size 41: 0.8283582089552238\n",
      "# Predictions:  269\n",
      "accuracy for window size 40: 0.8252788104089219\n",
      "# Predictions:  270\n",
      "accuracy for window size 39: 0.8148148148148148\n",
      "# Predictions:  271\n",
      "accuracy for window size 38: 0.8081180811808119\n",
      "# Predictions:  272\n",
      "accuracy for window size 37: 0.8161764705882353\n",
      "# Predictions:  273\n",
      "accuracy for window size 36: 0.8278388278388278\n",
      "# Predictions:  274\n",
      "accuracy for window size 35: 0.8284671532846716\n",
      "# Predictions:  275\n",
      "accuracy for window size 34: 0.8218181818181818\n",
      "# Predictions:  276\n",
      "accuracy for window size 33: 0.8260869565217391\n",
      "Accuracies:  [1.         1.         1.         1.         0.8        0.66666667\n",
      " 0.71428571 0.75       0.77777778 0.8        0.81818182 0.83333333\n",
      " 0.84615385 0.71428571 0.73333333 0.75       0.76470588 0.77777778\n",
      " 0.78947368 0.75       0.80952381 0.81818182 0.7826087  0.79166667\n",
      " 0.8        0.80769231 0.81481481 0.82142857 0.86206897 0.86666667\n",
      " 0.83870968 0.84375    0.81818182 0.82352941 0.82857143 0.86111111\n",
      " 0.86486486 0.86842105 0.87179487 0.875      0.87804878 0.88095238\n",
      " 0.88372093 0.88636364 0.88888889 0.89130435 0.87234043 0.875\n",
      " 0.89795918 0.92       0.92156863 0.90384615 0.9245283  0.90740741\n",
      " 0.89090909 0.89285714 0.87719298 0.89655172 0.88135593 0.88333333\n",
      " 0.8852459  0.88709677 0.88888889 0.890625   0.89230769 0.90909091\n",
      " 0.89552239 0.89705882 0.89855072 0.91428571 0.91549296 0.91666667\n",
      " 0.90410959 0.90540541 0.90666667 0.90789474 0.90909091 0.91025641\n",
      " 0.91139241 0.9125     0.91358025 0.91463415 0.91566265 0.9047619\n",
      " 0.89411765 0.89534884 0.89655172 0.89772727 0.8988764  0.9\n",
      " 0.9010989  0.89130435 0.89247312 0.89361702 0.89473684 0.90625\n",
      " 0.89690722 0.89795918 0.90909091 0.91       0.91089109 0.91176471\n",
      " 0.91262136 0.91346154 0.91428571 0.91509434 0.93457944 0.93518519\n",
      " 0.91743119 0.93636364 0.93693694 0.9375     0.92035398 0.92982456\n",
      " 0.93043478 0.93103448 0.93162393 0.93220339 0.93277311 0.93333333\n",
      " 0.9338843  0.93442623 0.92682927 0.92741935 0.928      0.93650794\n",
      " 0.93700787 0.9375     0.93023256 0.93076923 0.93129771 0.93181818\n",
      " 0.93233083 0.93283582 0.93333333 0.92647059 0.9270073  0.92753623\n",
      " 0.92805755 0.92857143 0.92907801 0.91549296 0.91608392 0.91666667\n",
      " 0.91724138 0.92465753 0.92517007 0.92567568 0.9261745  0.92666667\n",
      " 0.92715232 0.92105263 0.92156863 0.92207792 0.92258065 0.91666667\n",
      " 0.91082803 0.91139241 0.91194969 0.9125     0.91304348 0.91358025\n",
      " 0.91411043 0.92073171 0.92121212 0.92168675 0.92215569 0.91666667\n",
      " 0.9112426  0.90588235 0.90643275 0.90697674 0.90751445 0.90229885\n",
      " 0.90285714 0.90340909 0.9039548  0.90449438 0.90502793 0.90555556\n",
      " 0.90055249 0.9010989  0.90710383 0.89673913 0.8972973  0.90322581\n",
      " 0.89839572 0.89893617 0.8994709  0.9        0.89528796 0.89583333\n",
      " 0.89637306 0.89690722 0.89230769 0.89285714 0.89340102 0.8989899\n",
      " 0.90452261 0.895      0.89054726 0.89108911 0.89162562 0.89215686\n",
      " 0.89268293 0.88834951 0.88888889 0.87980769 0.88038278 0.87619048\n",
      " 0.87677725 0.87735849 0.88262911 0.88317757 0.88837209 0.87962963\n",
      " 0.87557604 0.87155963 0.86757991 0.87272727 0.87330317 0.87387387\n",
      " 0.86547085 0.85714286 0.85777778 0.8539823  0.85022026 0.85526316\n",
      " 0.85152838 0.85652174 0.86147186 0.85775862 0.86695279 0.86752137\n",
      " 0.86808511 0.86864407 0.8649789  0.86134454 0.87447699 0.87083333\n",
      " 0.87551867 0.88016529 0.87654321 0.87704918 0.86530612 0.85365854\n",
      " 0.85020243 0.85080645 0.85140562 0.844      0.84462151 0.83730159\n",
      " 0.84189723 0.83858268 0.84313725 0.83203125 0.82490272 0.83333333\n",
      " 0.83397683 0.83076923 0.82758621 0.83587786 0.83269962 0.83333333\n",
      " 0.82641509 0.83082707 0.82771536 0.82835821 0.82527881 0.81481481\n",
      " 0.80811808 0.81617647 0.82783883 0.82846715 0.82181818 0.82608696]\n"
     ]
    }
   ],
   "source": [
    "# Iterate over all window sizes to determine optimal size\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Change window_size_optimization flag to perform iterations\n",
    "window_size_optimization = False\n",
    "\n",
    "if window_size_optimization == True:\n",
    "\n",
    "    logreg = LogisticRegression()\n",
    "    accuracies = np.zeros(276)\n",
    "    #298 accuracy indices\n",
    "\n",
    "    # Iterate over all window sizes and compute accuracy for each LR model\n",
    "    for window_size in range(1,277):\n",
    "    #window_size = 299\n",
    "        print('# Predictions: ', window_size)\n",
    "        X_train = X.iloc[window_size:]\n",
    "        y_train = y.iloc[window_size:]\n",
    "        # print('train labels: ', y_train)\n",
    "        X_test = X.iloc[:window_size]\n",
    "        y_test = y.iloc[:window_size]\n",
    "        # print('test labels: ', y_test)\n",
    "\n",
    "        actuals = pd.DataFrame(y_test)\n",
    "        actuals = actuals.rename(columns={'AvgHR_bin':'Actuals'})\n",
    "        # print('actuals: \\n', actuals)\n",
    "        preds = np.zeros(X_test.shape[0])\n",
    "        # print('X_test: \\n', X_test)\n",
    "        for i in range(0,y_test.shape[0]):\n",
    "            # print('X train shape: ', X_train.shape[0])\n",
    "            # print('X train: ', X_train.head())\n",
    "            logreg.fit(X_train, y_train.values.ravel())\n",
    "            # Predict test set\n",
    "            # print('X test: ', X_test.iloc[0])\n",
    "            y_pred = logreg.predict(np.array(X_test.iloc[-1]).reshape(1,-1))\n",
    "            # print('actual: ',y_test.loc[0, 'AvgHR_bin'], '\\n pred: ',y_pred, '\\n')\n",
    "            preds[i] = y_pred\n",
    "            #print('Accuracy of logistic regression classifier on test set {}: {:.2f}'.format(i, logreg.score(X_test, y_test)))\n",
    "            #print(\"X test -1: \", X_test.iloc[-1])\n",
    "            \n",
    "            X_test_inst = pd.DataFrame(data= [X_test.iloc[-1]],columns=[\"Distance (km)\",\"Avg Pace (/km)\", \"HRSS\", \"Elevation Gain (m)\"])\n",
    "            y_test_inst = pd.DataFrame(data = [y_test.iloc[-1]],columns=[\"AvgHR_bin\"])\n",
    "            #print(\"X test inst: \", X_test_inst)\n",
    "            X_train = X_train.drop(X_train.index[-1]).reset_index(drop=True)\n",
    "            X_train = pd.concat([X_test_inst,X_train]).reset_index(drop = True)\n",
    "            #print(\"X train: \\n\", X_train)\n",
    "\n",
    "            y_train = y_train.drop(y_train.index[-1])\n",
    "            y_train = pd.concat([y_test_inst,y_train])\n",
    "            y_train = y_train.reset_index(drop=True)\n",
    "            #print(\"y train \\n\", y_train)\n",
    "\n",
    "            X_test = X_test.drop(X_test.index[-1])\n",
    "            X_test = X_test.reset_index(drop=True)\n",
    "            #print(\"X test: \\n\", X_test)\n",
    "            y_test = y_test.drop(y_test.index[-1])\n",
    "            y_test = y_test.reset_index(drop=True)\n",
    "            \n",
    "        preds_act_df = pd.DataFrame(preds, columns=['Predictions'])\n",
    "        preds_act_df = preds_act_df.join(actuals.iloc[::-1].reset_index(drop = True))\n",
    "        # print('actuals and preds: \\n', preds_act_df)\n",
    "        \n",
    "        accuracy = metrics.accuracy_score(preds_act_df.Actuals.ravel(),preds_act_df.Predictions.ravel())\n",
    "        print('accuracy for window size {}: {}'.format(309-window_size, accuracy))\n",
    "        accuracies[window_size-1] = accuracy\n",
    "    \n",
    "    print('Accuracies: ', accuracies)\n",
    "    \n",
    "    # Output accuracies for each window size to Accuracies_for_Window_Size_Variations.csv file \n",
    "    with open('Accuracies_for_Window_Size_Variations.csv', 'w') as f:\n",
    "        for i in range(0,len(accuracies)):\n",
    "            f.write(str(308-i) + ': ' + str(accuracies[i]))\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Window             Accuracy\n",
      "0      308                  1.0\n",
      "1      307                  1.0\n",
      "2      306                  1.0\n",
      "3      305                  1.0\n",
      "111    197               0.9375\n",
      "127    181               0.9375\n",
      "126    182    0.937007874015748\n",
      "110    198   0.9369369369369369\n",
      "125    183   0.9365079365079365\n",
      "109    199   0.9363636363636364\n",
      "107    201   0.9351851851851852\n",
      "106    202   0.9345794392523364\n",
      "121    187   0.9344262295081968\n",
      "120    188   0.9338842975206612\n",
      "119    189   0.9333333333333333\n",
      "134    174   0.9333333333333333\n",
      "133    175   0.9328358208955224\n",
      "118    190   0.9327731092436975\n",
      "132    176   0.9323308270676691\n",
      "117    191   0.9322033898305084\n",
      "131    177   0.9318181818181818\n",
      "116    192   0.9316239316239316\n",
      "130    178   0.9312977099236641\n",
      "115    193   0.9310344827586207\n",
      "129    179   0.9307692307692308\n",
      "114    194   0.9304347826086956\n",
      "128    180   0.9302325581395349\n",
      "113    195   0.9298245614035088\n",
      "140    168   0.9290780141843972\n",
      "139    169   0.9285714285714286\n",
      "138    170   0.9280575539568345\n",
      "124    184                0.928\n",
      "137    171    0.927536231884058\n",
      "123    185   0.9274193548387096\n",
      "150    158   0.9271523178807947\n",
      "136    172    0.927007299270073\n",
      "122    186    0.926829268292683\n",
      "149    159   0.9266666666666666\n",
      "135    173   0.9264705882352942\n",
      "148    160   0.9261744966442953\n",
      "147    161   0.9256756756756757\n",
      "146    162   0.9251700680272109\n",
      "145    163   0.9246575342465754\n",
      "52     256   0.9245283018867925\n",
      "154    154   0.9225806451612903\n",
      "166    142   0.9221556886227545\n",
      "153    155    0.922077922077922\n",
      "165    143   0.9216867469879518\n",
      "50     258   0.9215686274509803\n",
      "152    156   0.9215686274509803\n",
      "164    144   0.9212121212121213\n",
      "151    157   0.9210526315789473\n",
      "163    145   0.9207317073170732\n",
      "112    196   0.9203539823008849\n",
      "49     259                 0.92\n",
      "108    200   0.9174311926605505\n",
      "144    164   0.9172413793103448\n",
      "143    165   0.9166666666666666\n",
      "167    141   0.9166666666666666\n",
      "155    153   0.9166666666666666\n",
      "71     237   0.9166666666666666\n",
      "142    166    0.916083916083916\n",
      "82     226   0.9156626506024096\n",
      "141    167   0.9154929577464789\n",
      "70     238   0.9154929577464789\n",
      "105    203   0.9150943396226415\n",
      "81     227   0.9146341463414634\n",
      "104    204   0.9142857142857143\n",
      "69     239   0.9142857142857143\n",
      "162    146   0.9141104294478528\n",
      "161    147   0.9135802469135802\n",
      "80     228   0.9135802469135802\n",
      "103    205   0.9134615384615384\n",
      "160    148   0.9130434782608695\n",
      "102    206    0.912621359223301\n",
      "79     229               0.9125\n",
      "159    149               0.9125\n",
      "158    150   0.9119496855345912\n",
      "101    207   0.9117647058823529\n",
      "157    151   0.9113924050632911\n",
      "78     230   0.9113924050632911\n",
      "168    140   0.9112426035502958\n",
      "100    208   0.9108910891089109\n",
      "156    152    0.910828025477707\n",
      "77     231   0.9102564102564102\n",
      "99     209                 0.91\n",
      "65     243   0.9090909090909091\n",
      "98     210   0.9090909090909091\n",
      "76     232   0.9090909090909091\n",
      "75     233   0.9078947368421053\n",
      "172    136   0.9075144508670521\n",
      "53     255   0.9074074074074074\n",
      "182    126    0.907103825136612\n",
      "171    137   0.9069767441860465\n",
      "74     234   0.9066666666666666\n",
      "170    138   0.9064327485380117\n",
      "95     213              0.90625\n",
      "169    139   0.9058823529411765\n",
      "179    129   0.9055555555555556\n",
      "73     235   0.9054054054054054\n",
      "178    130   0.9050279329608939\n",
      "83     225   0.9047619047619048\n",
      "198    110   0.9045226130653267\n",
      "177    131   0.9044943820224719\n",
      "72     236   0.9041095890410958\n",
      "176    132    0.903954802259887\n",
      "51     257   0.9038461538461539\n",
      "175    133   0.9034090909090909\n",
      "185    123   0.9032258064516129\n",
      "174    134   0.9028571428571428\n",
      "173    135   0.9022988505747126\n",
      "90     218   0.9010989010989011\n",
      "181    127   0.9010989010989011\n",
      "180    128   0.9005524861878453\n",
      "189    119                  0.9\n",
      "89     219                  0.9\n",
      "188    120   0.8994708994708994\n",
      "197    111    0.898989898989899\n",
      "187    121    0.898936170212766\n",
      "88     220    0.898876404494382\n",
      "68     240   0.8985507246376812\n",
      "186    122   0.8983957219251337\n",
      "48     260   0.8979591836734694\n",
      "97     211   0.8979591836734694\n",
      "87     221   0.8977272727272727\n",
      "184    124   0.8972972972972973\n",
      "67     241   0.8970588235294118\n",
      "96     212   0.8969072164948454\n",
      "193    115   0.8969072164948454\n",
      "183    125   0.8967391304347826\n",
      "57     251    0.896551724137931\n",
      "86     222    0.896551724137931\n",
      "192    116   0.8963730569948186\n",
      "191    117   0.8958333333333334\n",
      "66     242   0.8955223880597015\n",
      "85     223   0.8953488372093024\n",
      "190    118   0.8952879581151832\n",
      "199    109                0.895\n",
      "94     214   0.8947368421052632\n",
      "84     224   0.8941176470588236\n",
      "93     215   0.8936170212765957\n",
      "196    112   0.8934010152284264\n",
      "55     253   0.8928571428571429\n",
      "195    113   0.8928571428571429\n",
      "204    104   0.8926829268292683\n",
      "92     216   0.8924731182795699\n",
      "194    114   0.8923076923076924\n",
      "64     244   0.8923076923076924\n",
      "203    105   0.8921568627450981\n",
      "202    106   0.8916256157635468\n",
      "91     217   0.8913043478260869\n",
      "45     263   0.8913043478260869\n",
      "201    107   0.8910891089108911\n",
      "54     254   0.8909090909090909\n",
      "63     245             0.890625\n",
      "200    108   0.8905472636815921\n",
      "62     246   0.8888888888888888\n",
      "206    102   0.8888888888888888\n",
      "44     264   0.8888888888888888\n",
      "214     94   0.8883720930232558\n",
      "205    103   0.8883495145631068\n",
      "61     247   0.8870967741935484\n",
      "43     265   0.8863636363636364\n",
      "60     248   0.8852459016393442\n",
      "42     266   0.8837209302325582\n",
      "59     249   0.8833333333333333\n",
      "213     95    0.883177570093458\n",
      "212     96   0.8826291079812206\n",
      "58     250   0.8813559322033898\n",
      "41     267   0.8809523809523809\n",
      "208    100   0.8803827751196173\n",
      "241     67   0.8801652892561983\n",
      "207    101   0.8798076923076923\n",
      "215     93   0.8796296296296297\n",
      "40     268   0.8780487804878049\n",
      "211     97   0.8773584905660378\n",
      "56     252   0.8771929824561403\n",
      "243     65   0.8770491803278688\n",
      "210     98   0.8767772511848341\n",
      "242     66   0.8765432098765432\n",
      "209     99   0.8761904761904762\n",
      "216     92   0.8755760368663594\n",
      "240     68   0.8755186721991701\n",
      "47     261                0.875\n",
      "39     269                0.875\n",
      "238     70   0.8744769874476988\n",
      "221     87   0.8738738738738738\n",
      "220     88   0.8733031674208145\n",
      "219     89   0.8727272727272727\n",
      "46     262   0.8723404255319149\n",
      "38     270   0.8717948717948718\n",
      "217     91   0.8715596330275229\n",
      "239     69   0.8708333333333333\n",
      "235     73   0.8686440677966102\n",
      "37     271    0.868421052631579\n",
      "234     74   0.8680851063829788\n",
      "218     90    0.867579908675799\n",
      "233     75   0.8675213675213675\n",
      "232     76   0.8669527896995708\n",
      "29     279   0.8666666666666667\n",
      "222     86   0.8654708520179372\n",
      "244     64   0.8653061224489796\n",
      "236     72   0.8649789029535865\n",
      "36     272   0.8648648648648649\n",
      "28     280   0.8620689655172413\n",
      "230     78   0.8614718614718615\n",
      "237     71   0.8613445378151261\n",
      "35     273   0.8611111111111112\n",
      "224     84   0.8577777777777778\n",
      "231     77   0.8577586206896551\n",
      "223     85   0.8571428571428571\n",
      "229     79   0.8565217391304348\n",
      "227     81   0.8552631578947368\n",
      "225     83   0.8539823008849557\n",
      "245     63   0.8536585365853658\n",
      "228     80    0.851528384279476\n",
      "248     60   0.8514056224899599\n",
      "247     61   0.8508064516129032\n",
      "226     82   0.8502202643171806\n",
      "246     62   0.8502024291497976\n",
      "12     296   0.8461538461538461\n",
      "250     58   0.8446215139442231\n",
      "249     59                0.844\n",
      "31     277              0.84375\n",
      "254     54   0.8431372549019608\n",
      "252     56    0.841897233201581\n",
      "30     278   0.8387096774193549\n",
      "253     55   0.8385826771653543\n",
      "251     57   0.8373015873015873\n",
      "261     47   0.8358778625954199\n",
      "258     50    0.833976833976834\n",
      "11     297   0.8333333333333334\n",
      "257     51   0.8333333333333334\n",
      "263     45   0.8333333333333334\n",
      "262     46   0.8326996197718631\n",
      "255     53           0.83203125\n",
      "265     43   0.8308270676691729\n",
      "259     49   0.8307692307692308\n",
      "34     274   0.8285714285714286\n",
      "273     35   0.8284671532846716\n",
      "267     41   0.8283582089552238\n",
      "272     36   0.8278388278388278\n",
      "266     42   0.8277153558052435\n",
      "260     48   0.8275862068965517\n",
      "264     44   0.8264150943396227\n",
      "275     33   0.8260869565217391\n",
      "268     40   0.8252788104089219\n",
      "256     52   0.8249027237354085\n",
      "33     275   0.8235294117647058\n",
      "274     34   0.8218181818181818\n",
      "27     281   0.8214285714285714\n",
      "21     287   0.8181818181818182\n",
      "10     298   0.8181818181818182\n",
      "32     276   0.8181818181818182\n",
      "271     37   0.8161764705882353\n",
      "26     282   0.8148148148148148\n",
      "269     39   0.8148148148148148\n",
      "20     288   0.8095238095238095\n",
      "270     38   0.8081180811808119\n",
      "25     283   0.8076923076923077\n",
      "9      299                  0.8\n",
      "24     284                  0.8\n",
      "4      304                  0.8\n",
      "23     285   0.7916666666666666\n",
      "18     290   0.7894736842105263\n",
      "22     286    0.782608695652174\n",
      "8      300   0.7777777777777778\n",
      "17     291   0.7777777777777778\n",
      "16     292   0.7647058823529411\n",
      "7      301                 0.75\n",
      "19     289                 0.75\n",
      "15     293                 0.75\n",
      "14     294   0.7333333333333333\n",
      "13     295   0.7142857142857143\n",
      "6      302   0.7142857142857143\n",
      "5      303   0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# Read in accuracies for each window size from Accuracies_for_Window_Size_Variations.csv file and sort by accuracy to determine best window size\n",
    "import csv\n",
    "pd.set_option('display.max_columns', None)  # or 1000\n",
    "pd.set_option('display.max_rows', None)  # or 1000\n",
    "mylist = pd.DataFrame(columns=['Window', 'Accuracy'])\n",
    "with open('Accuracies_for_Window_Size_Variations.csv', 'r') as csvfile:\n",
    "    for i,row in enumerate(csv.reader(csvfile, delimiter='\\n')):\n",
    "        mylist.loc[i,'Window'] = row[0].split(':')[0]\n",
    "        mylist.loc[i,'Accuracy'] = row[0].split(':')[1]\n",
    "print(mylist.sort_values('Accuracy', 0,ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Predictions:  128\n",
      "Window Size:  181\n",
      "Iteration:  0\n",
      "X train shape:  181\n",
      "X test shape:  128\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 127, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  1\n",
      "X train shape:  181\n",
      "X test shape:  127\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 126, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  2\n",
      "X train shape:  181\n",
      "X test shape:  126\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 125, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  3\n",
      "X train shape:  181\n",
      "X test shape:  125\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 124, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  4\n",
      "X train shape:  181\n",
      "X test shape:  124\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 123, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  5\n",
      "X train shape:  181\n",
      "X test shape:  123\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 122, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  6\n",
      "X train shape:  181\n",
      "X test shape:  122\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 121, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  7\n",
      "X train shape:  181\n",
      "X test shape:  121\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 120, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  8\n",
      "X train shape:  181\n",
      "X test shape:  120\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 119, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  9\n",
      "X train shape:  181\n",
      "X test shape:  119\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 118, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  10\n",
      "X train shape:  181\n",
      "X test shape:  118\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 117, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  11\n",
      "X train shape:  181\n",
      "X test shape:  117\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 116, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  12\n",
      "X train shape:  181\n",
      "X test shape:  116\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 115, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  13\n",
      "X train shape:  181\n",
      "X test shape:  115\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 114, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  14\n",
      "X train shape:  181\n",
      "X test shape:  114\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 113, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  15\n",
      "X train shape:  181\n",
      "X test shape:  113\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 112, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  16\n",
      "X train shape:  181\n",
      "X test shape:  112\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 111, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  17\n",
      "X train shape:  181\n",
      "X test shape:  111\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 110, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  18\n",
      "X train shape:  181\n",
      "X test shape:  110\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 109, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  19\n",
      "X train shape:  181\n",
      "X test shape:  109\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 108, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  20\n",
      "X train shape:  181\n",
      "X test shape:  108\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 107, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  21\n",
      "X train shape:  181\n",
      "X test shape:  107\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 106, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  22\n",
      "X train shape:  181\n",
      "X test shape:  106\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 105, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  23\n",
      "X train shape:  181\n",
      "X test shape:  105\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 104, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  24\n",
      "X train shape:  181\n",
      "X test shape:  104\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 103, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  25\n",
      "X train shape:  181\n",
      "X test shape:  103\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 102, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  26\n",
      "X train shape:  181\n",
      "X test shape:  102\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 101, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  27\n",
      "X train shape:  181\n",
      "X test shape:  101\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 100, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  28\n",
      "X train shape:  181\n",
      "X test shape:  100\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 99, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  29\n",
      "X train shape:  181\n",
      "X test shape:  99\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 98, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  30\n",
      "X train shape:  181\n",
      "X test shape:  98\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 97, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  31\n",
      "X train shape:  181\n",
      "X test shape:  97\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 96, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  32\n",
      "X train shape:  181\n",
      "X test shape:  96\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 95, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  33\n",
      "X train shape:  181\n",
      "X test shape:  95\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 94, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  34\n",
      "X train shape:  181\n",
      "X test shape:  94\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 93, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  35\n",
      "X train shape:  181\n",
      "X test shape:  93\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 92, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  36\n",
      "X train shape:  181\n",
      "X test shape:  92\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 91, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  37\n",
      "X train shape:  181\n",
      "X test shape:  91\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 90, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  38\n",
      "X train shape:  181\n",
      "X test shape:  90\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 89, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  39\n",
      "X train shape:  181\n",
      "X test shape:  89\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 88, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  40\n",
      "X train shape:  181\n",
      "X test shape:  88\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 87, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  41\n",
      "X train shape:  181\n",
      "X test shape:  87\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 86, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  42\n",
      "X train shape:  181\n",
      "X test shape:  86\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 85, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  43\n",
      "X train shape:  181\n",
      "X test shape:  85\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 84, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  44\n",
      "X train shape:  181\n",
      "X test shape:  84\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 83, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  45\n",
      "X train shape:  181\n",
      "X test shape:  83\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 82, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  46\n",
      "X train shape:  181\n",
      "X test shape:  82\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 81, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  47\n",
      "X train shape:  181\n",
      "X test shape:  81\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 80, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  48\n",
      "X train shape:  181\n",
      "X test shape:  80\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 79, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  49\n",
      "X train shape:  181\n",
      "X test shape:  79\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 78, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  50\n",
      "X train shape:  181\n",
      "X test shape:  78\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 77, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  51\n",
      "X train shape:  181\n",
      "X test shape:  77\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 76, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  52\n",
      "X train shape:  181\n",
      "X test shape:  76\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 75, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  53\n",
      "X train shape:  181\n",
      "X test shape:  75\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 74, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  54\n",
      "X train shape:  181\n",
      "X test shape:  74\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 73, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  55\n",
      "X train shape:  181\n",
      "X test shape:  73\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 72, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  56\n",
      "X train shape:  181\n",
      "X test shape:  72\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 71, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  57\n",
      "X train shape:  181\n",
      "X test shape:  71\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 70, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  58\n",
      "X train shape:  181\n",
      "X test shape:  70\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 69, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  59\n",
      "X train shape:  181\n",
      "X test shape:  69\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 68, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  60\n",
      "X train shape:  181\n",
      "X test shape:  68\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 67, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  61\n",
      "X train shape:  181\n",
      "X test shape:  67\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 66, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  62\n",
      "X train shape:  181\n",
      "X test shape:  66\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 65, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  63\n",
      "X train shape:  181\n",
      "X test shape:  65\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 64, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  64\n",
      "X train shape:  181\n",
      "X test shape:  64\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 63, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  65\n",
      "X train shape:  181\n",
      "X test shape:  63\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 62, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  66\n",
      "X train shape:  181\n",
      "X test shape:  62\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 61, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  67\n",
      "X train shape:  181\n",
      "X test shape:  61\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 60, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  68\n",
      "X train shape:  181\n",
      "X test shape:  60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual:  AvgHR_bin    0.0\n",
      "Name: 59, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  69\n",
      "X train shape:  181\n",
      "X test shape:  59\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 58, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  70\n",
      "X train shape:  181\n",
      "X test shape:  58\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 57, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  71\n",
      "X train shape:  181\n",
      "X test shape:  57\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 56, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  72\n",
      "X train shape:  181\n",
      "X test shape:  56\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 55, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  73\n",
      "X train shape:  181\n",
      "X test shape:  55\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 54, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  74\n",
      "X train shape:  181\n",
      "X test shape:  54\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 53, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  75\n",
      "X train shape:  181\n",
      "X test shape:  53\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 52, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  76\n",
      "X train shape:  181\n",
      "X test shape:  52\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 51, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  77\n",
      "X train shape:  181\n",
      "X test shape:  51\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 50, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  78\n",
      "X train shape:  181\n",
      "X test shape:  50\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 49, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  79\n",
      "X train shape:  181\n",
      "X test shape:  49\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 48, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  80\n",
      "X train shape:  181\n",
      "X test shape:  48\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 47, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  81\n",
      "X train shape:  181\n",
      "X test shape:  47\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 46, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  82\n",
      "X train shape:  181\n",
      "X test shape:  46\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 45, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  83\n",
      "X train shape:  181\n",
      "X test shape:  45\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 44, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  84\n",
      "X train shape:  181\n",
      "X test shape:  44\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 43, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  85\n",
      "X train shape:  181\n",
      "X test shape:  43\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 42, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  86\n",
      "X train shape:  181\n",
      "X test shape:  42\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 41, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  87\n",
      "X train shape:  181\n",
      "X test shape:  41\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 40, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  88\n",
      "X train shape:  181\n",
      "X test shape:  40\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 39, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  89\n",
      "X train shape:  181\n",
      "X test shape:  39\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 38, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  90\n",
      "X train shape:  181\n",
      "X test shape:  38\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 37, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  91\n",
      "X train shape:  181\n",
      "X test shape:  37\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 36, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  92\n",
      "X train shape:  181\n",
      "X test shape:  36\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 35, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  93\n",
      "X train shape:  181\n",
      "X test shape:  35\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 34, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  94\n",
      "X train shape:  181\n",
      "X test shape:  34\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 33, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  95\n",
      "X train shape:  181\n",
      "X test shape:  33\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 32, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  96\n",
      "X train shape:  181\n",
      "X test shape:  32\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 31, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  97\n",
      "X train shape:  181\n",
      "X test shape:  31\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 30, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  98\n",
      "X train shape:  181\n",
      "X test shape:  30\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 29, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  99\n",
      "X train shape:  181\n",
      "X test shape:  29\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 28, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  100\n",
      "X train shape:  181\n",
      "X test shape:  28\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 27, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  101\n",
      "X train shape:  181\n",
      "X test shape:  27\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 26, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  102\n",
      "X train shape:  181\n",
      "X test shape:  26\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 25, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  103\n",
      "X train shape:  181\n",
      "X test shape:  25\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 24, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  104\n",
      "X train shape:  181\n",
      "X test shape:  24\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 23, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  105\n",
      "X train shape:  181\n",
      "X test shape:  23\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 22, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  106\n",
      "X train shape:  181\n",
      "X test shape:  22\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 21, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  107\n",
      "X train shape:  181\n",
      "X test shape:  21\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 20, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  108\n",
      "X train shape:  181\n",
      "X test shape:  20\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 19, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  109\n",
      "X train shape:  181\n",
      "X test shape:  19\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 18, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  110\n",
      "X train shape:  181\n",
      "X test shape:  18\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 17, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  111\n",
      "X train shape:  181\n",
      "X test shape:  17\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 16, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  112\n",
      "X train shape:  181\n",
      "X test shape:  16\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 15, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  113\n",
      "X train shape:  181\n",
      "X test shape:  15\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 14, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  114\n",
      "X train shape:  181\n",
      "X test shape:  14\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 13, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  115\n",
      "X train shape:  181\n",
      "X test shape:  13\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 12, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  116\n",
      "X train shape:  181\n",
      "X test shape:  12\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 11, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  117\n",
      "X train shape:  181\n",
      "X test shape:  11\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 10, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  118\n",
      "X train shape:  181\n",
      "X test shape:  10\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 9, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  119\n",
      "X train shape:  181\n",
      "X test shape:  9\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 8, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  120\n",
      "X train shape:  181\n",
      "X test shape:  8\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 7, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  121\n",
      "X train shape:  181\n",
      "X test shape:  7\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 6, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  122\n",
      "X train shape:  181\n",
      "X test shape:  6\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 5, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  123\n",
      "X train shape:  181\n",
      "X test shape:  5\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 4, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  124\n",
      "X train shape:  181\n",
      "X test shape:  4\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 3, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  125\n",
      "X train shape:  181\n",
      "X test shape:  3\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 2, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  126\n",
      "X train shape:  181\n",
      "X test shape:  2\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 1, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  127\n",
      "X train shape:  181\n",
      "X test shape:  1\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 0, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actuals and preds: \n",
      "      Predictions  Actuals\n",
      "0            0.0      0.0\n",
      "1            1.0      1.0\n",
      "2            0.0      0.0\n",
      "3            0.0      0.0\n",
      "4            0.0      0.0\n",
      "..           ...      ...\n",
      "123          0.0      1.0\n",
      "124          1.0      1.0\n",
      "125          1.0      1.0\n",
      "126          1.0      1.0\n",
      "127          1.0      1.0\n",
      "\n",
      "[128 rows x 2 columns]\n",
      "accuracy for window size 181: 0.9375\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Run LR with optimal window size of 181 (# predictions = 128)\n",
    "window_size = 128\n",
    "print('# Predictions: ', window_size)\n",
    "print('Window Size: ', 309-window_size)\n",
    "# train set\n",
    "X_train = X.iloc[window_size:]\n",
    "y_train = y.iloc[window_size:]\n",
    "# print('train labels: ', y_train)\n",
    "# test set\n",
    "X_test = X.iloc[:window_size]\n",
    "y_test = y.iloc[:window_size]\n",
    "# print('test labels: ', y_test)\n",
    "\n",
    "actuals = pd.DataFrame(y_test)\n",
    "actuals = actuals.rename(columns={'AvgHR_bin':'Actuals'})\n",
    "#print('actuals: \\n', actuals)\n",
    "#print('reverse actuals: ', actuals.iloc[::-1])\n",
    "preds = np.zeros(X_test.shape[0])\n",
    "# print('X_test: \\n', X_test)\n",
    "for i in range(0,y_test.shape[0]):\n",
    "    print('Iteration: ', i)\n",
    "    print('X train shape: ', X_train.shape[0])\n",
    "    print('X test shape: ', y_test.shape[0])\n",
    "    logreg.fit(X_train, y_train.values.ravel())\n",
    "    # Predict test set\n",
    "    y_pred = logreg.predict(np.array(X_test.iloc[-1]).reshape(1,-1))\n",
    "    print('actual: ',y_test.iloc[-1], '\\n pred: ',y_pred, '\\n')\n",
    "    preds[i] = y_pred\n",
    "    # print('Accuracy of logistic regression classifier on test set {}: {:.2f}'.format(i, logreg.score(X_test, y_test)))\n",
    "    #print('test instance: ', X_test.iloc[-1])\n",
    "    # X_train = pd.concat([X_test.iloc[0], X_train]).reset_index(drop = True)\n",
    "    # print('new X train: ', X_train.head())\n",
    "    \n",
    "    #print(\"X test -1: \", X_test.iloc[-1])\n",
    "    X_test_inst = pd.DataFrame(data= [X_test.iloc[-1]],columns=[\"Distance (km)\",\"Avg Pace (/km)\", \"HRSS\", \"Elevation Gain (m)\"])\n",
    "    y_test_inst = pd.DataFrame(data = [y_test.iloc[-1]],columns=[\"AvgHR_bin\"])\n",
    "    #print(\"X test inst: \", X_test_inst)\n",
    "    X_train = X_train.drop(X_train.index[-1]).reset_index(drop=True)\n",
    "    X_train = pd.concat([X_test_inst,X_train]).reset_index(drop = True)\n",
    "    #print(\"X train: \\n\", X_train)\n",
    "\n",
    "    y_train = y_train.drop(y_train.index[-1])\n",
    "    y_train = pd.concat([y_test_inst,y_train])\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    #print(\"y train \\n\", y_train)\n",
    "    \n",
    "    X_test = X_test.drop(X_test.index[-1])\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    #print(\"X test: \\n\", X_test)\n",
    "\n",
    "    y_test = y_test.drop(y_test.index[-1])\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "preds_act_df = pd.DataFrame(preds, columns=['Predictions'])\n",
    "preds_act_df = preds_act_df.join(actuals.iloc[::-1].reset_index(drop = True))\n",
    "print('actuals and preds: \\n', preds_act_df)\n",
    "accuracy = metrics.accuracy_score(preds_act_df.Actuals.ravel(),preds_act_df.Predictions.ravel())\n",
    "print('accuracy for window size {}: {}'.format(309-window_size, accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[65  4]\n",
      " [ 4 55]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix1 = confusion_matrix(actuals.iloc[::-1].reset_index(drop = True), preds)\n",
    "print(confusion_matrix1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true neg:  65 \n",
      "false pos:  4 \n",
      "false neg:  4 \n",
      "true pos:  55\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(actuals.iloc[::-1].reset_index(drop = True), preds).ravel()\n",
    "print('true neg: ', tn, '\\nfalse pos: ', fp, '\\nfalse neg: ', fn, '\\ntrue pos: ',tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.94      0.94        69\n",
      "         1.0       0.93      0.93      0.93        59\n",
      "\n",
      "    accuracy                           0.94       128\n",
      "   macro avg       0.94      0.94      0.94       128\n",
      "weighted avg       0.94      0.94      0.94       128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Precision, recall, f1-score, support (# of test instances per class)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(actuals.iloc[::-1].reset_index(drop = True), preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.metrics import roc_auc_score\\nfrom sklearn.metrics import roc_curve\\nlogit_roc_auc = roc_auc_score(actuals, logreg.predict(np.array(X_test.iloc[0].reshape(1,-1))\\nfpr, tpr, thresholds = roc_curve(y_test.iloc[0], logreg.predict_proba(np.array(X_test.iloc[0].reshape(1,-1))[:,1])\\nplt.figure()\\nplt.plot(fpr, tpr, label=\\'Logistic Regression (area = %0.2f)\\' % logit_roc_auc)\\nplt.plot([0, 1], [0, 1],\\'r--\\')\\nplt.xlim([0.0, 1.0])\\nplt.ylim([0.0, 1.05])\\nplt.xlabel(\\'False Positive Rate\\')\\nplt.ylabel(\\'True Positive Rate\\')\\nplt.title(\\'Receiver operating characteristic\\')\\nplt.legend(loc=\"lower right\")\\nplt.savefig(\\'Log_ROC\\')\\nplt.show()\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ROC curve\n",
    "'''\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "logit_roc_auc = roc_auc_score(actuals, logreg.predict(np.array(X_test.iloc[0].reshape(1,-1))\n",
    "fpr, tpr, thresholds = roc_curve(y_test.iloc[0], logreg.predict_proba(np.array(X_test.iloc[0].reshape(1,-1))[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate Over All Look-Ahead Values for Given Window Size for LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Predictions:  128\n",
      "Window Size:  181\n",
      "\n",
      "\n",
      "look ahead:  1\n",
      "# iterations:  128\n",
      "accuracies:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1.]\n",
      "computations:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "avg accuracy:  0.9375\n",
      "\n",
      "\n",
      "look ahead:  2\n",
      "# iterations:  64\n",
      "accuracies:  [1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      " 0.5 1.  1.  1.  0.5 1.  1.  1.  1.  0.5 1.  1.  1.  1.  1.  1.  1.  1.\n",
      " 1.  0.5 1.  1.  1.  1.  1.  1.  1.  1.  1.  0.5 0.5 1.  1.  1.  1.  1.\n",
      " 1.  1.  1.  0.5 1.  0.5 1.  0.5 1.  1. ]\n",
      "computations:  [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      "avg accuracy:  0.9296875\n",
      "\n",
      "\n",
      "look ahead:  3\n",
      "# iterations:  43\n",
      "accuracies:  [1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.66666667 1.         0.66666667 1.         1.         1.\n",
      " 0.66666667 1.         1.         1.         1.         1.\n",
      " 0.66666667 1.         1.         1.         1.         1.\n",
      " 1.         0.66666667 0.66666667 1.         1.         1.\n",
      " 0.66666667 1.         0.66666667 1.         1.         0.66666667\n",
      " 1.        ]\n",
      "computations:  [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 2.]\n",
      "avg accuracy:  0.9296875\n",
      "\n",
      "\n",
      "look ahead:  4\n",
      "# iterations:  32\n",
      "accuracies:  [1.   1.   1.   1.   1.   1.   1.   1.   1.   0.75 1.   0.75 1.   0.75\n",
      " 1.   1.   1.   1.   0.75 1.   1.   1.   1.   0.75 0.75 1.   1.   0.75\n",
      " 0.75 1.   0.75 1.  ]\n",
      "computations:  [4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.\n",
      " 4. 4. 4. 4. 4. 4. 4. 4.]\n",
      "avg accuracy:  0.9296875\n",
      "\n",
      "\n",
      "look ahead:  5\n",
      "# iterations:  26\n",
      "accuracies:  [1.  1.  1.  1.  1.  1.  1.  0.8 0.8 1.  1.  0.8 1.  1.  0.8 1.  1.  1.\n",
      " 1.  0.6 1.  0.8 0.8 1.  0.6 1. ]\n",
      "computations:  [5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5.\n",
      " 5. 3.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  6\n",
      "# iterations:  22\n",
      "accuracies:  [1.         1.         1.         1.         1.         1.\n",
      " 0.83333333 0.83333333 1.         0.83333333 1.         1.\n",
      " 0.83333333 1.         1.         0.83333333 0.83333333 1.\n",
      " 0.83333333 0.83333333 0.66666667 1.        ]\n",
      "computations:  [6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 2.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  7\n",
      "# iterations:  19\n",
      "accuracies:  [1.         1.         1.         1.         1.         0.85714286\n",
      " 0.85714286 0.85714286 1.         1.         0.85714286 1.\n",
      " 1.         0.71428571 1.         0.85714286 0.71428571 0.71428571\n",
      " 1.        ]\n",
      "computations:  [7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 2.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  8\n",
      "# iterations:  16\n",
      "accuracies:  [1.    1.    1.    1.    0.875 0.875 0.875 1.    1.    0.875 1.    0.875\n",
      " 0.875 0.875 0.75  0.75 ]\n",
      "computations:  [8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  9\n",
      "# iterations:  15\n",
      "accuracies:  [1.         1.         1.         1.         0.77777778 1.\n",
      " 0.88888889 1.         0.88888889 1.         0.77777778 1.\n",
      " 0.77777778 0.77777778 1.        ]\n",
      "computations:  [9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 2.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  10\n",
      "# iterations:  13\n",
      "accuracies:  [1.   1.   1.   0.9  0.9  0.9  1.   0.9  1.   0.8  0.9  0.8  0.75]\n",
      "computations:  [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.  8.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  11\n",
      "# iterations:  12\n",
      "accuracies:  [1.         1.         1.         0.90909091 0.90909091 0.90909091\n",
      " 0.90909091 1.         0.81818182 0.90909091 0.81818182 0.71428571]\n",
      "computations:  [11. 11. 11. 11. 11. 11. 11. 11. 11. 11. 11.  7.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  12\n",
      "# iterations:  11\n",
      "accuracies:  [1.         1.         1.         0.83333333 0.91666667 1.\n",
      " 0.91666667 0.91666667 0.91666667 0.75       0.75      ]\n",
      "computations:  [12. 12. 12. 12. 12. 12. 12. 12. 12. 12.  8.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  13\n",
      "# iterations:  10\n",
      "accuracies:  [1.         1.         0.92307692 0.92307692 0.92307692 0.92307692\n",
      " 1.         0.84615385 0.84615385 0.81818182]\n",
      "computations:  [13. 13. 13. 13. 13. 13. 13. 13. 13. 11.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  14\n",
      "# iterations:  10\n",
      "accuracies:  [1.         1.         0.92857143 0.85714286 1.         0.92857143\n",
      " 0.85714286 0.92857143 0.78571429 1.        ]\n",
      "computations:  [14. 14. 14. 14. 14. 14. 14. 14. 14.  2.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  15\n",
      "# iterations:  9\n",
      "accuracies:  [1.         1.         0.86666667 0.93333333 0.93333333 1.\n",
      " 0.86666667 0.86666667 0.75      ]\n",
      "computations:  [15. 15. 15. 15. 15. 15. 15. 15.  8.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  16\n",
      "# iterations:  8\n",
      "accuracies:  [1.     1.     0.875  0.9375 0.9375 0.9375 0.875  0.75  ]\n",
      "computations:  [16. 16. 16. 16. 16. 16. 16. 16.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  17\n",
      "# iterations:  8\n",
      "accuracies:  [1.         1.         0.88235294 0.94117647 0.94117647 0.88235294\n",
      " 0.88235294 0.77777778]\n",
      "computations:  [17. 17. 17. 17. 17. 17. 17.  9.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  18\n",
      "# iterations:  8\n",
      "accuracies:  [1.         1.         0.88888889 0.94444444 0.94444444 0.88888889\n",
      " 0.77777778 1.        ]\n",
      "computations:  [18. 18. 18. 18. 18. 18. 18.  2.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  19\n",
      "# iterations:  7\n",
      "accuracies:  [1.         0.94736842 0.89473684 0.94736842 1.         0.84210526\n",
      " 0.71428571]\n",
      "computations:  [19. 19. 19. 19. 19. 19. 14.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  20\n",
      "# iterations:  7\n",
      "accuracies:  [1.   0.95 0.9  0.95 0.9  0.9  0.75]\n",
      "computations:  [20. 20. 20. 20. 20. 20.  8.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  21\n",
      "# iterations:  7\n",
      "accuracies:  [1.         0.95238095 0.9047619  0.95238095 0.9047619  0.80952381\n",
      " 1.        ]\n",
      "computations:  [21. 21. 21. 21. 21. 21.  2.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  22\n",
      "# iterations:  6\n",
      "accuracies:  [1.         0.95454545 0.90909091 0.95454545 0.86363636 0.77777778]\n",
      "computations:  [22. 22. 22. 22. 22. 18.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  23\n",
      "# iterations:  6\n",
      "accuracies:  [1.         0.91304348 0.95652174 0.95652174 0.82608696 0.76923077]\n",
      "computations:  [23. 23. 23. 23. 23. 13.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  24\n",
      "# iterations:  6\n",
      "accuracies:  [1.         0.91666667 0.95833333 0.91666667 0.875      0.75      ]\n",
      "computations:  [24. 24. 24. 24. 24.  8.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  25\n",
      "# iterations:  6\n",
      "accuracies:  [1.   0.92 0.92 0.92 0.84 1.  ]\n",
      "computations:  [25. 25. 25. 25. 25.  3.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  26\n",
      "# iterations:  5\n",
      "accuracies:  [1.         0.92307692 0.92307692 0.92307692 0.83333333]\n",
      "computations:  [26. 26. 26. 26. 24.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  27\n",
      "# iterations:  5\n",
      "accuracies:  [1.         0.92592593 0.92592593 0.92592593 0.8       ]\n",
      "computations:  [27. 27. 27. 27. 20.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  28\n",
      "# iterations:  5\n",
      "accuracies:  [1.         0.89285714 0.96428571 0.89285714 0.75      ]\n",
      "computations:  [28. 28. 28. 28. 16.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  29\n",
      "# iterations:  5\n",
      "accuracies:  [1.         0.89655172 0.96551724 0.86206897 0.75      ]\n",
      "computations:  [29. 29. 29. 29. 12.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  30\n",
      "# iterations:  5\n",
      "accuracies:  [1.         0.9        0.96666667 0.86666667 0.75      ]\n",
      "computations:  [30. 30. 30. 30.  8.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  31\n",
      "# iterations:  5\n",
      "accuracies:  [1.         0.90322581 0.96774194 0.80645161 1.        ]\n",
      "computations:  [31. 31. 31. 31.  4.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  32\n",
      "# iterations:  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracies:  [1.      0.90625 0.9375  0.84375]\n",
      "computations:  [32. 32. 32. 32.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  33\n",
      "# iterations:  4\n",
      "accuracies:  [1.         0.90909091 0.90909091 0.86206897]\n",
      "computations:  [33. 33. 33. 29.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  34\n",
      "# iterations:  4\n",
      "accuracies:  [1.         0.91176471 0.91176471 0.84615385]\n",
      "computations:  [34. 34. 34. 26.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  35\n",
      "# iterations:  4\n",
      "accuracies:  [1.         0.91428571 0.91428571 0.82608696]\n",
      "computations:  [35. 35. 35. 23.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  36\n",
      "# iterations:  4\n",
      "accuracies:  [1.         0.91666667 0.91666667 0.8       ]\n",
      "computations:  [36. 36. 36. 20.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  37\n",
      "# iterations:  4\n",
      "accuracies:  [0.97297297 0.94594595 0.89189189 0.76470588]\n",
      "computations:  [37. 37. 37. 17.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  38\n",
      "# iterations:  4\n",
      "accuracies:  [0.97368421 0.92105263 0.92105263 0.71428571]\n",
      "computations:  [38. 38. 38. 14.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  39\n",
      "# iterations:  4\n",
      "accuracies:  [0.97435897 0.92307692 0.8974359  0.81818182]\n",
      "computations:  [39. 39. 39. 11.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  40\n",
      "# iterations:  4\n",
      "accuracies:  [0.975 0.925 0.9   0.75 ]\n",
      "computations:  [40. 40. 40.  8.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  41\n",
      "# iterations:  4\n",
      "accuracies:  [0.97560976 0.92682927 0.87804878 0.8       ]\n",
      "computations:  [41. 41. 41.  5.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  42\n",
      "# iterations:  4\n",
      "accuracies:  [0.97619048 0.92857143 0.85714286 1.        ]\n",
      "computations:  [42. 42. 42.  2.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  43\n",
      "# iterations:  3\n",
      "accuracies:  [0.97674419 0.93023256 0.85714286]\n",
      "computations:  [43. 43. 42.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  44\n",
      "# iterations:  3\n",
      "accuracies:  [0.97727273 0.93181818 0.85      ]\n",
      "computations:  [44. 44. 40.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  45\n",
      "# iterations:  3\n",
      "accuracies:  [0.95555556 0.95555556 0.84210526]\n",
      "computations:  [45. 45. 38.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  46\n",
      "# iterations:  3\n",
      "accuracies:  [0.95652174 0.95652174 0.83333333]\n",
      "computations:  [46. 46. 36.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  47\n",
      "# iterations:  3\n",
      "accuracies:  [0.95744681 0.95744681 0.82352941]\n",
      "computations:  [47. 47. 34.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  48\n",
      "# iterations:  3\n",
      "accuracies:  [0.95833333 0.9375     0.84375   ]\n",
      "computations:  [48. 48. 32.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  49\n",
      "# iterations:  3\n",
      "accuracies:  [0.95918367 0.91836735 0.86666667]\n",
      "computations:  [49. 49. 30.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  50\n",
      "# iterations:  3\n",
      "accuracies:  [0.96       0.92       0.85714286]\n",
      "computations:  [50. 50. 28.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  51\n",
      "# iterations:  3\n",
      "accuracies:  [0.96078431 0.92156863 0.84615385]\n",
      "computations:  [51. 51. 26.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  52\n",
      "# iterations:  3\n",
      "accuracies:  [0.96153846 0.92307692 0.83333333]\n",
      "computations:  [52. 52. 24.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  53\n",
      "# iterations:  3\n",
      "accuracies:  [0.96226415 0.9245283  0.81818182]\n",
      "computations:  [53. 53. 22.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  54\n",
      "# iterations:  3\n",
      "accuracies:  [0.96296296 0.92592593 0.8       ]\n",
      "computations:  [54. 54. 20.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  55\n",
      "# iterations:  3\n",
      "accuracies:  [0.96363636 0.90909091 0.77777778]\n",
      "computations:  [55. 55. 18.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  56\n",
      "# iterations:  3\n",
      "accuracies:  [0.94642857 0.92857143 0.8125    ]\n",
      "computations:  [56. 56. 16.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  57\n",
      "# iterations:  3\n",
      "accuracies:  [0.94736842 0.92982456 0.71428571]\n",
      "computations:  [57. 57. 14.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  58\n",
      "# iterations:  3\n",
      "accuracies:  [0.94827586 0.9137931  0.83333333]\n",
      "computations:  [58. 58. 12.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  59\n",
      "# iterations:  3\n",
      "accuracies:  [0.94915254 0.91525424 0.8       ]\n",
      "computations:  [59. 59. 10.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  60\n",
      "# iterations:  3\n",
      "accuracies:  [0.95 0.9  0.75]\n",
      "computations:  [60. 60.  8.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  61\n",
      "# iterations:  3\n",
      "accuracies:  [0.95081967 0.90163934 0.66666667]\n",
      "computations:  [61. 61.  6.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  62\n",
      "# iterations:  3\n",
      "accuracies:  [0.9516129  0.87096774 1.        ]\n",
      "computations:  [62. 62.  4.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  63\n",
      "# iterations:  3\n",
      "accuracies:  [0.95238095 0.87301587 1.        ]\n",
      "computations:  [63. 63.  2.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  64\n",
      "# iterations:  2\n",
      "accuracies:  [0.953125 0.875   ]\n",
      "computations:  [64. 64.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  65\n",
      "# iterations:  2\n",
      "accuracies:  [0.95384615 0.87301587]\n",
      "computations:  [65. 63.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  66\n",
      "# iterations:  2\n",
      "accuracies:  [0.95454545 0.87096774]\n",
      "computations:  [66. 62.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  67\n",
      "# iterations:  2\n",
      "accuracies:  [0.95522388 0.86885246]\n",
      "computations:  [67. 61.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  68\n",
      "# iterations:  2\n",
      "accuracies:  [0.95588235 0.86666667]\n",
      "computations:  [68. 60.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  69\n",
      "# iterations:  2\n",
      "accuracies:  [0.95652174 0.86440678]\n",
      "computations:  [69. 59.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  70\n",
      "# iterations:  2\n",
      "accuracies:  [0.95714286 0.86206897]\n",
      "computations:  [70. 58.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  71\n",
      "# iterations:  2\n",
      "accuracies:  [0.95774648 0.85964912]\n",
      "computations:  [71. 57.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  72\n",
      "# iterations:  2\n",
      "accuracies:  [0.95833333 0.85714286]\n",
      "computations:  [72. 56.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  73\n",
      "# iterations:  2\n",
      "accuracies:  [0.95890411 0.85454545]\n",
      "computations:  [73. 55.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  74\n",
      "# iterations:  2\n",
      "accuracies:  [0.95945946 0.85185185]\n",
      "computations:  [74. 54.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  75\n",
      "# iterations:  2\n",
      "accuracies:  [0.94666667 0.88679245]\n",
      "computations:  [75. 53.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  76\n",
      "# iterations:  2\n",
      "accuracies:  [0.94736842 0.88461538]\n",
      "computations:  [76. 52.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  77\n",
      "# iterations:  2\n",
      "accuracies:  [0.94805195 0.88235294]\n",
      "computations:  [77. 51.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  78\n",
      "# iterations:  2\n",
      "accuracies:  [0.94871795 0.88      ]\n",
      "computations:  [78. 50.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  79\n",
      "# iterations:  2\n",
      "accuracies:  [0.94936709 0.87755102]\n",
      "computations:  [79. 49.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  80\n",
      "# iterations:  2\n",
      "accuracies:  [0.95  0.875]\n",
      "computations:  [80. 48.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  81\n",
      "# iterations:  2\n",
      "accuracies:  [0.95061728 0.87234043]\n",
      "computations:  [81. 47.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  82\n",
      "# iterations:  2\n",
      "accuracies:  [0.95121951 0.86956522]\n",
      "computations:  [82. 46.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  83\n",
      "# iterations:  2\n",
      "accuracies:  [0.95180723 0.86666667]\n",
      "computations:  [83. 45.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  84\n",
      "# iterations:  2\n",
      "accuracies:  [0.95238095 0.86363636]\n",
      "computations:  [84. 44.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  85\n",
      "# iterations:  2\n",
      "accuracies:  [0.95294118 0.86046512]\n",
      "computations:  [85. 43.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  86\n",
      "# iterations:  2\n",
      "accuracies:  [0.95348837 0.85714286]\n",
      "computations:  [86. 42.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  87\n",
      "# iterations:  2\n",
      "accuracies:  [0.95402299 0.85365854]\n",
      "computations:  [87. 41.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  88\n",
      "# iterations:  2\n",
      "accuracies:  [0.95454545 0.85      ]\n",
      "computations:  [88. 40.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  89\n",
      "# iterations:  2\n",
      "accuracies:  [0.95505618 0.84615385]\n",
      "computations:  [89. 39.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  90\n",
      "# iterations:  2\n",
      "accuracies:  [0.95555556 0.84210526]\n",
      "computations:  [90. 38.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  91\n",
      "# iterations:  2\n",
      "accuracies:  [0.95604396 0.83783784]\n",
      "computations:  [91. 37.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  92\n",
      "# iterations:  2\n",
      "accuracies:  [0.95652174 0.83333333]\n",
      "computations:  [92. 36.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  93\n",
      "# iterations:  2\n",
      "accuracies:  [0.95698925 0.82857143]\n",
      "computations:  [93. 35.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  94\n",
      "# iterations:  2\n",
      "accuracies:  [0.95744681 0.82352941]\n",
      "computations:  [94. 34.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  95\n",
      "# iterations:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracies:  [0.95789474 0.81818182]\n",
      "computations:  [95. 33.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  96\n",
      "# iterations:  2\n",
      "accuracies:  [0.94791667 0.84375   ]\n",
      "computations:  [96. 32.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  97\n",
      "# iterations:  2\n",
      "accuracies:  [0.94845361 0.83870968]\n",
      "computations:  [97. 31.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  98\n",
      "# iterations:  2\n",
      "accuracies:  [0.93877551 0.86666667]\n",
      "computations:  [98. 30.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  99\n",
      "# iterations:  2\n",
      "accuracies:  [0.93939394 0.86206897]\n",
      "computations:  [99. 29.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  100\n",
      "# iterations:  2\n",
      "accuracies:  [0.94       0.85714286]\n",
      "computations:  [100.  28.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  101\n",
      "# iterations:  2\n",
      "accuracies:  [0.94059406 0.85185185]\n",
      "computations:  [101.  27.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  102\n",
      "# iterations:  2\n",
      "accuracies:  [0.94117647 0.84615385]\n",
      "computations:  [102.  26.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  103\n",
      "# iterations:  2\n",
      "accuracies:  [0.94174757 0.84      ]\n",
      "computations:  [103.  25.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  104\n",
      "# iterations:  2\n",
      "accuracies:  [0.94230769 0.83333333]\n",
      "computations:  [104.  24.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  105\n",
      "# iterations:  2\n",
      "accuracies:  [0.94285714 0.82608696]\n",
      "computations:  [105.  23.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  106\n",
      "# iterations:  2\n",
      "accuracies:  [0.94339623 0.81818182]\n",
      "computations:  [106.  22.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  107\n",
      "# iterations:  2\n",
      "accuracies:  [0.94392523 0.80952381]\n",
      "computations:  [107.  21.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  108\n",
      "# iterations:  2\n",
      "accuracies:  [0.94444444 0.8       ]\n",
      "computations:  [108.  20.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  109\n",
      "# iterations:  2\n",
      "accuracies:  [0.93577982 0.78947368]\n",
      "computations:  [109.  19.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  110\n",
      "# iterations:  2\n",
      "accuracies:  [0.93636364 0.77777778]\n",
      "computations:  [110.  18.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  111\n",
      "# iterations:  2\n",
      "accuracies:  [0.93693694 0.76470588]\n",
      "computations:  [111.  17.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  112\n",
      "# iterations:  2\n",
      "accuracies:  [0.9375 0.8125]\n",
      "computations:  [112.  16.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  113\n",
      "# iterations:  2\n",
      "accuracies:  [0.9380531 0.8      ]\n",
      "computations:  [113.  15.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  114\n",
      "# iterations:  2\n",
      "accuracies:  [0.93859649 0.71428571]\n",
      "computations:  [114.  14.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  115\n",
      "# iterations:  2\n",
      "accuracies:  [0.93043478 0.84615385]\n",
      "computations:  [115.  13.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  116\n",
      "# iterations:  2\n",
      "accuracies:  [0.93103448 0.83333333]\n",
      "computations:  [116.  12.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  117\n",
      "# iterations:  2\n",
      "accuracies:  [0.93162393 0.81818182]\n",
      "computations:  [117.  11.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  118\n",
      "# iterations:  2\n",
      "accuracies:  [0.93220339 0.8       ]\n",
      "computations:  [118.  10.]\n",
      "avg accuracy:  0.921875\n",
      "\n",
      "\n",
      "look ahead:  119\n",
      "# iterations:  2\n",
      "accuracies:  [0.92436975 0.77777778]\n",
      "computations:  [119.   9.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  120\n",
      "# iterations:  2\n",
      "accuracies:  [0.925 0.75 ]\n",
      "computations:  [120.   8.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  121\n",
      "# iterations:  2\n",
      "accuracies:  [0.92561983 0.71428571]\n",
      "computations:  [121.   7.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  122\n",
      "# iterations:  2\n",
      "accuracies:  [0.92622951 0.66666667]\n",
      "computations:  [122.   6.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  123\n",
      "# iterations:  2\n",
      "accuracies:  [0.91869919 0.8       ]\n",
      "computations:  [123.   5.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  124\n",
      "# iterations:  2\n",
      "accuracies:  [0.91129032 1.        ]\n",
      "computations:  [124.   4.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  125\n",
      "# iterations:  2\n",
      "accuracies:  [0.912 1.   ]\n",
      "computations:  [125.   3.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  126\n",
      "# iterations:  2\n",
      "accuracies:  [0.91269841 1.        ]\n",
      "computations:  [126.   2.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  127\n",
      "# iterations:  2\n",
      "accuracies:  [0.91338583 1.        ]\n",
      "computations:  [127.   1.]\n",
      "avg accuracy:  0.9140625\n",
      "\n",
      "\n",
      "look ahead:  128\n",
      "# iterations:  1\n",
      "accuracies:  [0.9140625]\n",
      "computations:  [128.]\n",
      "avg accuracy:  0.9140625\n",
      "Averages:       Avg Accuracy\n",
      "0        0.937500\n",
      "1        0.929688\n",
      "2        0.929688\n",
      "3        0.929688\n",
      "4        0.921875\n",
      "5        0.921875\n",
      "6        0.914062\n",
      "7        0.914062\n",
      "8        0.921875\n",
      "9        0.914062\n",
      "10       0.914062\n",
      "11       0.914062\n",
      "12       0.921875\n",
      "13       0.921875\n",
      "14       0.921875\n",
      "15       0.914062\n",
      "16       0.921875\n",
      "17       0.921875\n",
      "18       0.914062\n",
      "19       0.921875\n",
      "20       0.921875\n",
      "21       0.914062\n",
      "22       0.914062\n",
      "23       0.921875\n",
      "24       0.921875\n",
      "25       0.921875\n",
      "26       0.921875\n",
      "27       0.914062\n",
      "28       0.914062\n",
      "29       0.921875\n",
      "30       0.921875\n",
      "31       0.921875\n",
      "32       0.921875\n",
      "33       0.921875\n",
      "34       0.921875\n",
      "35       0.921875\n",
      "36       0.914062\n",
      "37       0.914062\n",
      "38       0.921875\n",
      "39       0.921875\n",
      "40       0.921875\n",
      "41       0.921875\n",
      "42       0.921875\n",
      "43       0.921875\n",
      "44       0.921875\n",
      "45       0.921875\n",
      "46       0.921875\n",
      "47       0.921875\n",
      "48       0.921875\n",
      "49       0.921875\n",
      "50       0.921875\n",
      "51       0.921875\n",
      "52       0.921875\n",
      "53       0.921875\n",
      "54       0.914062\n",
      "55       0.921875\n",
      "56       0.914062\n",
      "57       0.921875\n",
      "58       0.921875\n",
      "59       0.914062\n",
      "60       0.914062\n",
      "61       0.914062\n",
      "62       0.914062\n",
      "63       0.914062\n",
      "64       0.914062\n",
      "65       0.914062\n",
      "66       0.914062\n",
      "67       0.914062\n",
      "68       0.914062\n",
      "69       0.914062\n",
      "70       0.914062\n",
      "71       0.914062\n",
      "72       0.914062\n",
      "73       0.914062\n",
      "74       0.921875\n",
      "75       0.921875\n",
      "76       0.921875\n",
      "77       0.921875\n",
      "78       0.921875\n",
      "79       0.921875\n",
      "80       0.921875\n",
      "81       0.921875\n",
      "82       0.921875\n",
      "83       0.921875\n",
      "84       0.921875\n",
      "85       0.921875\n",
      "86       0.921875\n",
      "87       0.921875\n",
      "88       0.921875\n",
      "89       0.921875\n",
      "90       0.921875\n",
      "91       0.921875\n",
      "92       0.921875\n",
      "93       0.921875\n",
      "94       0.921875\n",
      "95       0.921875\n",
      "96       0.921875\n",
      "97       0.921875\n",
      "98       0.921875\n",
      "99       0.921875\n",
      "100      0.921875\n",
      "101      0.921875\n",
      "102      0.921875\n",
      "103      0.921875\n",
      "104      0.921875\n",
      "105      0.921875\n",
      "106      0.921875\n",
      "107      0.921875\n",
      "108      0.914062\n",
      "109      0.914062\n",
      "110      0.914062\n",
      "111      0.921875\n",
      "112      0.921875\n",
      "113      0.914062\n",
      "114      0.921875\n",
      "115      0.921875\n",
      "116      0.921875\n",
      "117      0.921875\n",
      "118      0.914062\n",
      "119      0.914062\n",
      "120      0.914062\n",
      "121      0.914062\n",
      "122      0.914062\n",
      "123      0.914062\n",
      "124      0.914062\n",
      "125      0.914062\n",
      "126      0.914062\n",
      "127      0.914062\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import statistics as stats\n",
    "# Run LR with optimal window size of 15 (# predictions = 294)\n",
    "\n",
    "window_size = 128 # num of preds, window_size = 309-window_size param\n",
    "print('# Predictions: ', window_size)\n",
    "print('Window Size: ', 309-window_size)\n",
    "average_accuracies = np.zeros(window_size)\n",
    "predictions=[] #np.zeros(window_size)\n",
    "\n",
    "for m in range(1,window_size+1):\n",
    "    # train set\n",
    "    X_train = X.iloc[window_size:]\n",
    "    y_train = y.iloc[window_size:]\n",
    "    #print('train labels: ', y_train)\n",
    "    # test set\n",
    "    X_test = X.iloc[:window_size]\n",
    "    y_test = y.iloc[:window_size]\n",
    "    #print('test labels: ', y_test)\n",
    "    \n",
    "    #look_ahead = 7\n",
    "    look_ahead=m\n",
    "    print(\"\\n\\nlook ahead: \", m)\n",
    "    print(\"# iterations: \", math.ceil(y_test.shape[0]/look_ahead))\n",
    "    \n",
    "    actuals = pd.DataFrame(y_test)\n",
    "    actuals = actuals.rename(columns={'AvgHR_bin':'Actuals'})\n",
    "    accuracies = np.zeros(math.ceil(y_test.shape[0]/look_ahead))\n",
    "    computations = np.zeros(math.ceil(y_test.shape[0]/look_ahead))\n",
    "\n",
    "    # print('actuals: \\n', actuals)\n",
    "    # preds = np.zeros((math.ceil(y_test.shape[0]/look_ahead),look_ahead))\n",
    "    # actuals_look_ahead = np.zeros((math.ceil(y_test.shape[0]/look_ahead),look_ahead))\n",
    "    # print('X_test: \\n', X_test)\n",
    "    for i in range(0,math.ceil(y_test.shape[0]/look_ahead)):\n",
    "        preds = np.zeros(min(look_ahead,y_test.shape[0]))\n",
    "        actuals_look_ahead = np.zeros(min(look_ahead,y_test.shape[0]))\n",
    "        computations[i] = min(look_ahead, y_test.shape[0])\n",
    "        # print('Iteration: ', i)\n",
    "#         print('X train shape: ', X_train.shape[0])\n",
    "#         print('X test shape: ', y_test.shape[0])\n",
    "        logreg.fit(X_train, y_train.values.ravel())\n",
    "        # Predict test set\n",
    "        for j in range(1,min(look_ahead+1,y_test.shape[0]+1)):\n",
    "            y_pred = logreg.predict(np.array(X_test.iloc[-j]).reshape(1,-1))\n",
    "#             print('actual: ',y_test.iloc[-j], '\\n pred: ',y_pred, '\\n')\n",
    "            preds[j-1] = y_pred\n",
    "            # predictions[(j*(i+1))-1] = y_pred\n",
    "            actuals_look_ahead[j-1] = y_test.iloc[-j]\n",
    "        # print('Accuracy of logistic regression classifier on test set {}: {:.2f}'.format(i, logreg.score(X_test, y_test)))\n",
    "        #print('test instance: ', X_test.iloc[-1])\n",
    "        # X_train = pd.concat([X_test.iloc[0], X_train]).reset_index(drop = True)\n",
    "        # print('new X train: ', X_train.head())\n",
    "\n",
    "        #print(\"X test -1: \", X_test.iloc[-1])\n",
    "        X_test_inst = pd.DataFrame(columns=[\"Distance (km)\",\"Avg Pace (/km)\", \"HRSS\", \"Elevation Gain (m)\"])\n",
    "        y_test_inst = pd.DataFrame(columns=[\"AvgHR_bin\"])\n",
    "        for k in range(1,min(look_ahead+1,y_test.shape[0]+1)):\n",
    "            X_test_inst = X_test_inst.append(X_test.iloc[-k])\n",
    "            y_test_inst = y_test_inst.append(y_test.iloc[-k])        \n",
    "        #print(\"X test inst: \", X_test_inst)\n",
    "\n",
    "        X_train = X_train.drop(X_train.index[-1]).reset_index(drop=True)\n",
    "        X_train = pd.concat([X_test_inst,X_train]).reset_index(drop = True)\n",
    "        #print(\"X train: \\n\", X_train)\n",
    "\n",
    "        y_train = y_train.drop(y_train.index[-1])\n",
    "        y_train = pd.concat([y_test_inst,y_train]).reset_index(drop=True)\n",
    "        #print(\"y train \\n\", y_train)\n",
    "\n",
    "        X_test = X_test.drop(X_test_inst.index.values)\n",
    "        X_test = X_test.reset_index(drop=True)\n",
    "        #print(\"X test: \\n\", X_test)\n",
    "\n",
    "        y_test = y_test.drop(y_test_inst.index.values)\n",
    "        y_test = y_test.reset_index(drop=True)\n",
    "        \n",
    "        preds_act_df = pd.DataFrame(preds)\n",
    "        act_df = pd.DataFrame(actuals_look_ahead)\n",
    "        # print(\"predictions: \", preds)\n",
    "        # print(\"actuals: \", actuals_look_ahead)\n",
    "        #preds_act_df = preds_act_df.join(act_df)\n",
    "        #print('actuals and preds: \\n', preds_act_df)\n",
    "        accuracy = metrics.accuracy_score(preds,actuals_look_ahead)\n",
    "        accuracies[i] = accuracy\n",
    "        # print('accuracy for window size {}: {}'.format(window_size, accuracy))\n",
    "        # predictions.append(preds)\n",
    "    print('accuracies: ', accuracies)\n",
    "    print('computations: ', computations)\n",
    "    h=len(accuracies)\n",
    "    avg_accuracy = 0\n",
    "    for g in range(len(accuracies)):\n",
    "        avg_accuracy += (computations[g]/window_size)*accuracies[g] \n",
    "    # avg_accuracy = stats.mean(accuracies)\n",
    "    print('avg accuracy: ', avg_accuracy)\n",
    "    average_accuracies[m-1] = avg_accuracy\n",
    "    #print('total predictions: ', predictions)\n",
    "    #print('total actuals: ', actuals.iloc[::-1].reset_index(drop = True))\n",
    "    #tot_acc = metrics.accuracy_score(predictions,actuals.iloc[::-1].reset_index(drop = True))\n",
    "    #print('tot accuracy: ', tot_acc)\n",
    "\n",
    "avg_acc_df = pd.DataFrame(data=average_accuracies, columns=['Avg Accuracy'])\n",
    "print('Averages: ', avg_acc_df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output accuracies for each window size to Accuracies_for_Look_Ahead_Variations.csv file \n",
    "with open('Accuracies_for_Look_Ahead_Variations.csv', 'w') as f:\n",
    "    # f.write(\"Look Ahead : Accuracy \\n\")\n",
    "    for i in range(0,len(average_accuracies)):\n",
    "        f.write(str(i+1) + ': ' + str(average_accuracies[i]))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Look Ahead    Accuracy\n",
      "0    Look Ahead    Accuracy \n",
      "1              1      0.9375\n",
      "2              2   0.9296875\n",
      "3              3   0.9296875\n",
      "4              4   0.9296875\n",
      "100          100    0.921875\n",
      "101          101    0.921875\n",
      "102          102    0.921875\n",
      "103          103    0.921875\n",
      "104          104    0.921875\n",
      "105          105    0.921875\n",
      "106          106    0.921875\n",
      "107          107    0.921875\n",
      "108          108    0.921875\n",
      "59            59    0.921875\n",
      "112          112    0.921875\n",
      "75            75    0.921875\n",
      "58            58    0.921875\n",
      "113          113    0.921875\n",
      "56            56    0.921875\n",
      "54            54    0.921875\n",
      "53            53    0.921875\n",
      "52            52    0.921875\n",
      "51            51    0.921875\n",
      "99            99    0.921875\n",
      "77            77    0.921875\n",
      "76            76    0.921875\n",
      "98            98    0.921875\n",
      "96            96    0.921875\n",
      "95            95    0.921875\n",
      "94            94    0.921875\n",
      "93            93    0.921875\n",
      "92            92    0.921875\n",
      "91            91    0.921875\n",
      "90            90    0.921875\n",
      "89            89    0.921875\n",
      "88            88    0.921875\n",
      "87            87    0.921875\n",
      "86            86    0.921875\n",
      "85            85    0.921875\n",
      "84            84    0.921875\n",
      "83            83    0.921875\n",
      "82            82    0.921875\n",
      "81            81    0.921875\n",
      "80            80    0.921875\n",
      "79            79    0.921875\n",
      "78            78    0.921875\n",
      "50            50    0.921875\n",
      "49            49    0.921875\n",
      "48            48    0.921875\n",
      "32            32    0.921875\n",
      "30            30    0.921875\n",
      "117          117    0.921875\n",
      "47            47    0.921875\n",
      "27            27    0.921875\n",
      "26            26    0.921875\n",
      "25            25    0.921875\n",
      "24            24    0.921875\n",
      "21            21    0.921875\n",
      "20            20    0.921875\n",
      "18            18    0.921875\n",
      "17            17    0.921875\n",
      "15            15    0.921875\n",
      "14            14    0.921875\n",
      "13            13    0.921875\n",
      "9              9    0.921875\n",
      "6              6    0.921875\n",
      "5              5    0.921875\n",
      "31            31    0.921875\n",
      "118          118    0.921875\n",
      "33            33    0.921875\n",
      "115          115    0.921875\n",
      "46            46    0.921875\n",
      "45            45    0.921875\n",
      "44            44    0.921875\n",
      "43            43    0.921875\n",
      "42            42    0.921875\n",
      "34            34    0.921875\n",
      "40            40    0.921875\n",
      "39            39    0.921875\n",
      "41            41    0.921875\n",
      "97            97    0.921875\n",
      "35            35    0.921875\n",
      "36            36    0.921875\n",
      "116          116    0.921875\n",
      "124          124   0.9140625\n",
      "127          127   0.9140625\n",
      "126          126   0.9140625\n",
      "125          125   0.9140625\n",
      "120          120   0.9140625\n",
      "121          121   0.9140625\n",
      "109          109   0.9140625\n",
      "114          114   0.9140625\n",
      "110          110   0.9140625\n",
      "122          122   0.9140625\n",
      "111          111   0.9140625\n",
      "119          119   0.9140625\n",
      "123          123   0.9140625\n",
      "64            64   0.9140625\n",
      "74            74   0.9140625\n",
      "73            73   0.9140625\n",
      "7              7   0.9140625\n",
      "8              8   0.9140625\n",
      "10            10   0.9140625\n",
      "11            11   0.9140625\n",
      "12            12   0.9140625\n",
      "16            16   0.9140625\n",
      "19            19   0.9140625\n",
      "22            22   0.9140625\n",
      "23            23   0.9140625\n",
      "28            28   0.9140625\n",
      "29            29   0.9140625\n",
      "37            37   0.9140625\n",
      "38            38   0.9140625\n",
      "55            55   0.9140625\n",
      "57            57   0.9140625\n",
      "60            60   0.9140625\n",
      "61            61   0.9140625\n",
      "62            62   0.9140625\n",
      "63            63   0.9140625\n",
      "65            65   0.9140625\n",
      "66            66   0.9140625\n",
      "67            67   0.9140625\n",
      "68            68   0.9140625\n",
      "69            69   0.9140625\n",
      "70            70   0.9140625\n",
      "71            71   0.9140625\n",
      "72            72   0.9140625\n",
      "128          128   0.9140625\n"
     ]
    }
   ],
   "source": [
    "# Read in accuracies for each look ahead value from Accuracies_for_Look_Ahead_Variations.csv file and sort by accuracy to determine best look ahead value\n",
    "import csv\n",
    "pd.set_option('display.max_columns', None)  # or 1000\n",
    "pd.set_option('display.max_rows', None)  # or 1000\n",
    "mylist = pd.DataFrame(columns=[])\n",
    "with open('Accuracies_for_Look_Ahead_Variations.csv', 'r') as csvfile:\n",
    "    for i,row in enumerate(csv.reader(csvfile, delimiter='\\n')):\n",
    "        mylist.loc[i,'Look Ahead'] = row[0].split(':')[0]\n",
    "        mylist.loc[i,'Accuracy'] = row[0].split(':')[1]\n",
    "print(mylist.sort_values('Accuracy', 0,ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all accuracies are above 0.8. The highest accuracy is 0.957 and looks ahead 127 days, a value equal to almost the entire test set size. Thus, rather than iterate through each test instance and leave one out, it is best to predict the entire test set at once, rather than leaving a group out at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR Implementation with Optimal Window Size = 181 and Optimal Look Ahead = 118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Predictions:  128\n",
      "Window Size:  181\n",
      "train labels:       AvgHR_bin\n",
      "128        1.0\n",
      "129        1.0\n",
      "130        1.0\n",
      "131        0.0\n",
      "132        1.0\n",
      "133        0.0\n",
      "134        1.0\n",
      "135        1.0\n",
      "136        0.0\n",
      "137        1.0\n",
      "138        1.0\n",
      "139        1.0\n",
      "140        1.0\n",
      "141        1.0\n",
      "142        0.0\n",
      "143        1.0\n",
      "144        1.0\n",
      "145        0.0\n",
      "146        1.0\n",
      "147        1.0\n",
      "148        1.0\n",
      "149        1.0\n",
      "150        1.0\n",
      "151        0.0\n",
      "152        0.0\n",
      "153        1.0\n",
      "154        0.0\n",
      "155        1.0\n",
      "156        1.0\n",
      "157        1.0\n",
      "158        0.0\n",
      "159        0.0\n",
      "160        1.0\n",
      "161        1.0\n",
      "162        1.0\n",
      "163        0.0\n",
      "164        1.0\n",
      "165        1.0\n",
      "166        0.0\n",
      "167        1.0\n",
      "168        1.0\n",
      "169        0.0\n",
      "170        0.0\n",
      "171        1.0\n",
      "172        1.0\n",
      "173        1.0\n",
      "174        1.0\n",
      "175        1.0\n",
      "176        1.0\n",
      "177        0.0\n",
      "178        0.0\n",
      "179        0.0\n",
      "180        1.0\n",
      "181        1.0\n",
      "182        0.0\n",
      "183        1.0\n",
      "184        1.0\n",
      "185        0.0\n",
      "186        1.0\n",
      "187        1.0\n",
      "188        1.0\n",
      "189        0.0\n",
      "190        1.0\n",
      "191        1.0\n",
      "192        0.0\n",
      "193        1.0\n",
      "194        1.0\n",
      "195        1.0\n",
      "196        1.0\n",
      "197        0.0\n",
      "198        1.0\n",
      "199        0.0\n",
      "200        0.0\n",
      "201        1.0\n",
      "202        0.0\n",
      "203        1.0\n",
      "204        1.0\n",
      "205        0.0\n",
      "206        1.0\n",
      "207        1.0\n",
      "208        1.0\n",
      "209        0.0\n",
      "210        0.0\n",
      "211        1.0\n",
      "212        1.0\n",
      "213        1.0\n",
      "214        1.0\n",
      "215        0.0\n",
      "216        1.0\n",
      "217        1.0\n",
      "218        1.0\n",
      "219        1.0\n",
      "220        1.0\n",
      "221        1.0\n",
      "222        1.0\n",
      "223        1.0\n",
      "224        1.0\n",
      "225        1.0\n",
      "226        0.0\n",
      "227        1.0\n",
      "228        1.0\n",
      "229        1.0\n",
      "230        1.0\n",
      "231        0.0\n",
      "232        0.0\n",
      "233        0.0\n",
      "234        1.0\n",
      "235        1.0\n",
      "236        1.0\n",
      "237        1.0\n",
      "238        1.0\n",
      "239        1.0\n",
      "240        1.0\n",
      "241        1.0\n",
      "242        0.0\n",
      "243        1.0\n",
      "244        0.0\n",
      "245        0.0\n",
      "246        1.0\n",
      "247        1.0\n",
      "248        0.0\n",
      "249        0.0\n",
      "250        0.0\n",
      "251        1.0\n",
      "252        1.0\n",
      "253        1.0\n",
      "254        0.0\n",
      "255        0.0\n",
      "256        0.0\n",
      "257        1.0\n",
      "258        1.0\n",
      "259        0.0\n",
      "260        1.0\n",
      "261        1.0\n",
      "262        0.0\n",
      "263        1.0\n",
      "264        0.0\n",
      "265        1.0\n",
      "266        0.0\n",
      "267        0.0\n",
      "268        0.0\n",
      "269        0.0\n",
      "270        1.0\n",
      "271        0.0\n",
      "272        0.0\n",
      "273        0.0\n",
      "274        0.0\n",
      "275        0.0\n",
      "276        1.0\n",
      "277        0.0\n",
      "278        1.0\n",
      "279        0.0\n",
      "280        0.0\n",
      "281        0.0\n",
      "282        0.0\n",
      "283        1.0\n",
      "284        1.0\n",
      "285        1.0\n",
      "286        0.0\n",
      "287        1.0\n",
      "288        0.0\n",
      "289        0.0\n",
      "290        0.0\n",
      "291        1.0\n",
      "292        1.0\n",
      "293        0.0\n",
      "294        0.0\n",
      "295        1.0\n",
      "296        0.0\n",
      "297        0.0\n",
      "298        1.0\n",
      "299        0.0\n",
      "300        0.0\n",
      "301        0.0\n",
      "302        0.0\n",
      "303        0.0\n",
      "304        0.0\n",
      "305        0.0\n",
      "306        0.0\n",
      "307        0.0\n",
      "308        0.0\n",
      "test labels:       AvgHR_bin\n",
      "0          1.0\n",
      "1          1.0\n",
      "2          1.0\n",
      "3          1.0\n",
      "4          1.0\n",
      "5          1.0\n",
      "6          1.0\n",
      "7          0.0\n",
      "8          0.0\n",
      "9          0.0\n",
      "10         1.0\n",
      "11         1.0\n",
      "12         1.0\n",
      "13         0.0\n",
      "14         1.0\n",
      "15         0.0\n",
      "16         0.0\n",
      "17         1.0\n",
      "18         0.0\n",
      "19         1.0\n",
      "20         1.0\n",
      "21         0.0\n",
      "22         0.0\n",
      "23         0.0\n",
      "24         0.0\n",
      "25         0.0\n",
      "26         0.0\n",
      "27         0.0\n",
      "28         0.0\n",
      "29         0.0\n",
      "30         0.0\n",
      "31         0.0\n",
      "32         0.0\n",
      "33         0.0\n",
      "34         0.0\n",
      "35         0.0\n",
      "36         0.0\n",
      "37         0.0\n",
      "38         0.0\n",
      "39         0.0\n",
      "40         0.0\n",
      "41         0.0\n",
      "42         0.0\n",
      "43         0.0\n",
      "44         0.0\n",
      "45         0.0\n",
      "46         0.0\n",
      "47         0.0\n",
      "48         0.0\n",
      "49         0.0\n",
      "50         0.0\n",
      "51         0.0\n",
      "52         1.0\n",
      "53         1.0\n",
      "54         0.0\n",
      "55         0.0\n",
      "56         0.0\n",
      "57         0.0\n",
      "58         0.0\n",
      "59         0.0\n",
      "60         0.0\n",
      "61         0.0\n",
      "62         0.0\n",
      "63         0.0\n",
      "64         0.0\n",
      "65         1.0\n",
      "66         0.0\n",
      "67         0.0\n",
      "68         1.0\n",
      "69         0.0\n",
      "70         0.0\n",
      "71         1.0\n",
      "72         1.0\n",
      "73         1.0\n",
      "74         0.0\n",
      "75         0.0\n",
      "76         1.0\n",
      "77         0.0\n",
      "78         1.0\n",
      "79         0.0\n",
      "80         1.0\n",
      "81         0.0\n",
      "82         0.0\n",
      "83         0.0\n",
      "84         1.0\n",
      "85         1.0\n",
      "86         1.0\n",
      "87         1.0\n",
      "88         1.0\n",
      "89         0.0\n",
      "90         1.0\n",
      "91         1.0\n",
      "92         1.0\n",
      "93         1.0\n",
      "94         1.0\n",
      "95         1.0\n",
      "96         1.0\n",
      "97         0.0\n",
      "98         1.0\n",
      "99         1.0\n",
      "100        1.0\n",
      "101        1.0\n",
      "102        1.0\n",
      "103        1.0\n",
      "104        0.0\n",
      "105        1.0\n",
      "106        0.0\n",
      "107        0.0\n",
      "108        1.0\n",
      "109        1.0\n",
      "110        1.0\n",
      "111        1.0\n",
      "112        1.0\n",
      "113        1.0\n",
      "114        1.0\n",
      "115        1.0\n",
      "116        1.0\n",
      "117        1.0\n",
      "118        1.0\n",
      "119        1.0\n",
      "120        1.0\n",
      "121        1.0\n",
      "122        1.0\n",
      "123        0.0\n",
      "124        0.0\n",
      "125        0.0\n",
      "126        1.0\n",
      "127        0.0\n",
      "Iteration:  0\n",
      "X train shape:  181\n",
      "X test shape:  128\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 127, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 126, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 125, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 124, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 123, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 122, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 121, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 120, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 119, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 118, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 117, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 116, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 115, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 114, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 113, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 112, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 111, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 110, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 109, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 108, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 107, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 106, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 105, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 104, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 103, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 102, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 101, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 100, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 99, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 98, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 97, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 96, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 95, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 94, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 93, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 92, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 91, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 90, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 89, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 88, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 87, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 86, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 85, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 84, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 83, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 82, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 81, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 80, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 79, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 78, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 77, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 76, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 75, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 74, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 73, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 72, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 71, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 70, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 69, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 68, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 67, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 66, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 65, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 64, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 63, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 62, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 61, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 60, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 59, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 58, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 57, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 56, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 55, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 54, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 53, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 52, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 51, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 50, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 49, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 48, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 47, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 46, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 45, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 44, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 43, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 42, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 41, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual:  AvgHR_bin    0.0\n",
      "Name: 40, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 39, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 38, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 37, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 36, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 35, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 34, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 33, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 32, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 31, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 30, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 29, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 28, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 27, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 26, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 25, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 24, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 23, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 22, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 21, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 20, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 19, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 18, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 17, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 16, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 15, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 14, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 13, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 12, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 11, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 10, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "X test inst:       Distance (km)  Avg Pace (/km)   HRSS  Elevation Gain (m)\n",
      "127           40.2           125.0   87.0               447.0\n",
      "126           31.0           116.0   83.0                 0.0\n",
      "125           80.2           125.0  194.0              1086.0\n",
      "124          100.8           130.0  220.0              1442.0\n",
      "123           80.4           131.0  189.0              1108.0\n",
      "122           80.2           138.0  260.0              1136.0\n",
      "121           80.3           124.0  304.0              1131.0\n",
      "120           30.1           119.0   88.0                 0.0\n",
      "119           21.0            89.0   84.0                89.0\n",
      "118           30.2           119.0   92.0                 0.0\n",
      "117           85.4           135.0  227.0              1184.0\n",
      "116           30.1           119.0  101.0                 0.0\n",
      "115           34.7           123.0  127.0               558.0\n",
      "114           48.2           103.0  167.0               692.0\n",
      "113           15.6            97.0   64.0                68.0\n",
      "112           30.1           119.0  101.0                 0.0\n",
      "111           19.5            92.0   73.0               153.0\n",
      "110           80.3           126.0  224.0              1081.0\n",
      "109           46.0            99.0  151.0               640.0\n",
      "108           20.2            92.0   79.0                 7.0\n",
      "107           80.8           137.0  188.0              1094.0\n",
      "106          105.9           128.0  240.0              1609.0\n",
      "105          111.4           120.0  294.0              1527.0\n",
      "104           38.1           155.0   68.0               153.0\n",
      "103           20.3            94.0   81.0               179.0\n",
      "102           30.1           119.0  101.0                 0.0\n",
      "101           57.4           107.0  203.0               898.0\n",
      "100           18.7           102.0   75.0               192.0\n",
      "99            31.1            86.0  111.0                 0.0\n",
      "98            50.8           112.0  136.0               571.0\n",
      "97           114.4           155.0  311.0              2419.0\n",
      "96            31.2           115.0   94.0                 0.0\n",
      "95            30.1           117.0   99.0                 0.0\n",
      "94            56.4           117.0  155.0               666.0\n",
      "93             8.1           134.0   43.0               230.6\n",
      "92            40.3           108.0  112.0               383.0\n",
      "91            55.6           124.0  134.0               714.0\n",
      "90            28.7            91.0   82.0                 0.0\n",
      "89            40.5           114.0   86.0               393.0\n",
      "88            32.2           114.0  100.0               292.0\n",
      "87            80.3           130.0  207.0              1036.0\n",
      "86            31.7           116.0   75.0               238.0\n",
      "85            56.3           121.0  142.0               668.0\n",
      "84            40.2           113.0   91.0               379.0\n",
      "83            55.8           126.0  125.0               496.0\n",
      "82            80.3           137.0  164.0              1019.0\n",
      "81            56.1           134.0  123.0               665.0\n",
      "80            30.0            91.0  111.0                13.0\n",
      "79            40.0           118.0   87.0               371.0\n",
      "78            60.6           116.0  137.0               546.0\n",
      "77            50.8           116.0  107.0               510.0\n",
      "76            32.5           109.0   86.0               285.0\n",
      "75            80.3           133.0  171.0              1077.0\n",
      "74            40.1           118.0   69.0               388.0\n",
      "73            73.9           148.0  292.0              1986.0\n",
      "72            20.1           177.0   73.0               488.0\n",
      "71            29.1           112.0  102.0               412.0\n",
      "70            50.5           163.0  135.0              1306.0\n",
      "69            33.1           170.0   98.0               852.0\n",
      "68            90.2            87.0  213.0               461.0\n",
      "67            30.1           158.0    6.0               678.0\n",
      "66            51.5           161.0  148.0              1258.0\n",
      "65            33.1           155.0  123.0               878.0\n",
      "64           107.6           160.0  335.0              2925.0\n",
      "63            33.2           174.0   93.0               866.0\n",
      "62            41.8           188.0  117.0              1094.0\n",
      "61            34.3           160.0  111.0               894.0\n",
      "60           151.2           165.0  344.0              3142.0\n",
      "59            50.8           150.0  137.0               783.0\n",
      "58            43.8           140.0   90.0               387.0\n",
      "57            33.3           158.0  104.0               882.0\n",
      "56            55.2           146.0  157.0              1126.0\n",
      "55            50.4           162.0  137.0              1293.0\n",
      "54            40.1           156.0  103.0               859.0\n",
      "53            73.2           146.0  227.0              2031.0\n",
      "52           144.1           124.0  467.0              3120.0\n",
      "51            50.4           165.0  126.0              1239.0\n",
      "50            73.6           177.0  181.0              1922.0\n",
      "49            64.3           180.0  164.0              1053.0\n",
      "48            60.4           157.0  134.0               679.0\n",
      "47            40.6           207.0   96.0               833.0\n",
      "46            52.9           176.0  135.0              1313.0\n",
      "45           105.5           169.0  309.0              3067.0\n",
      "44            53.0           164.0  137.0              1400.0\n",
      "43            53.0           155.0  138.0              1387.0\n",
      "42           146.9           168.0  445.0              3617.0\n",
      "41            93.8           153.0  205.0              1446.0\n",
      "40            50.4           165.0  108.0              1327.0\n",
      "39           101.6           146.0  213.0              1790.0\n",
      "38            76.0           172.0  197.0              2003.0\n",
      "37            50.4           180.0  100.0              1310.0\n",
      "36            30.4           117.0   52.0               247.0\n",
      "35            93.4           173.0  300.0              2624.0\n",
      "34            76.3           173.0  189.0              2067.0\n",
      "33            76.3           168.0  172.0              2074.0\n",
      "32            25.6           208.0  109.0               685.0\n",
      "31           170.6           149.0  471.0              3388.0\n",
      "30            26.0           307.0  128.0               715.0\n",
      "29            27.2           221.0   82.0               636.0\n",
      "28           141.0           167.0  354.0              3366.0\n",
      "27            15.1           477.0   71.0               492.0\n",
      "26            52.9           193.0  137.0              1150.0\n",
      "25            64.2           195.0  195.0              2194.0\n",
      "24            40.3           266.0  166.0              1326.0\n",
      "23           139.8           151.0  390.0              2768.3\n",
      "22            55.2           149.0  116.0               884.0\n",
      "21            50.6           187.0  133.0               981.0\n",
      "20            52.8           116.0  138.0               605.0\n",
      "19            55.4           108.0  123.0               541.0\n",
      "18            63.0           119.0  128.0               744.0\n",
      "17            47.5           110.0  132.0               406.0\n",
      "16            32.5           119.0   66.0               282.0\n",
      "15            45.9           148.0   79.0               447.0\n",
      "14            30.3           127.0   92.0               550.0\n",
      "13            13.2           301.0   74.0               184.0\n",
      "12            52.0           116.0  152.0               560.0\n",
      "11           100.9           130.0  337.0              1395.0\n",
      "10            52.6           122.0  172.0               535.0\n",
      "predictions:  [0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1.\n",
      " 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1.]\n",
      "actuals:  [0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1.\n",
      " 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1.]\n",
      "accuracy for iteration 1 with look ahead value 118: 0.9322033898305084\n",
      "Iteration:  1\n",
      "X train shape:  298\n",
      "X test shape:  10\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 9, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 8, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 7, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 6, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 5, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 4, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 3, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 2, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 1, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 0, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "X test inst:     Distance (km)  Avg Pace (/km)   HRSS  Elevation Gain (m)\n",
      "9           47.2           116.0  106.0               447.0\n",
      "8           55.4           114.0  116.0               502.0\n",
      "7           51.4           128.0   90.0               576.0\n",
      "6           81.0           129.0  225.0              1096.0\n",
      "5           61.4           122.0  146.0               692.0\n",
      "4           41.1           120.0   96.0               454.0\n",
      "3           45.6           114.0  119.0               447.0\n",
      "2           35.2           116.0   87.0               314.0\n",
      "1           80.1           125.0  217.0               890.0\n",
      "0           62.5           113.0  144.0               589.0\n",
      "predictions:  [0. 0. 0. 1. 0. 0. 1. 1. 1. 1.]\n",
      "actuals:  [0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "accuracy for iteration 2 with look ahead value 118: 0.8\n",
      "avg accuracy:  0.921875\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import statistics as stats\n",
    "# Run LR with optimal window size of 15 (# predictions = 294)\n",
    "\n",
    "window_size = 128 # num of preds, window_size = 309-window_size param\n",
    "print('# Predictions: ', window_size)\n",
    "print('Window Size: ', 309-window_size)\n",
    "# train set\n",
    "X_train = X.iloc[window_size:]\n",
    "y_train = y.iloc[window_size:]\n",
    "print('train labels: ', y_train)\n",
    "# test set\n",
    "X_test = X.iloc[:window_size]\n",
    "y_test = y.iloc[:window_size]\n",
    "print('test labels: ', y_test)\n",
    "\n",
    "look_ahead = 118\n",
    "\n",
    "actuals = pd.DataFrame(y_test)\n",
    "accuracies = np.zeros(math.ceil(y_test.shape[0]/look_ahead))\n",
    "computations = np.zeros(math.ceil(y_test.shape[0]/look_ahead))\n",
    "predictions = []\n",
    "# print('X_test: \\n', X_test)\n",
    "\n",
    "for i in range(0,math.ceil(y_test.shape[0]/look_ahead)):\n",
    "    preds = np.zeros(min(look_ahead,y_test.shape[0]))\n",
    "    actuals_look_ahead = np.zeros(min(look_ahead,y_test.shape[0]))\n",
    "    computations[i] = min(look_ahead,y_test.shape[0])\n",
    "    print('Iteration: ', i)\n",
    "    print('X train shape: ', X_train.shape[0])\n",
    "    print('X test shape: ', y_test.shape[0])\n",
    "    logreg.fit(X_train, y_train.values.ravel())\n",
    "    # Predict test set\n",
    "    for j in range(1,min(look_ahead+1,y_test.shape[0]+1)):\n",
    "        y_pred = logreg.predict(np.array(X_test.iloc[-j]).reshape(1,-1))\n",
    "        print('actual: ',y_test.iloc[-j], '\\n pred: ',y_pred, '\\n')\n",
    "        preds[j-1] = y_pred\n",
    "        predictions.append(y_pred)\n",
    "        actuals_look_ahead[j-1] = y_test.iloc[-j]\n",
    "    # print('Accuracy of logistic regression classifier on test set {}: {:.2f}'.format(i, logreg.score(X_test, y_test)))\n",
    "    #print('test instance: ', X_test.iloc[-1])\n",
    "    # X_train = pd.concat([X_test.iloc[0], X_train]).reset_index(drop = True)\n",
    "    # print('new X train: ', X_train.head())\n",
    "    \n",
    "    #print(\"X test -1: \", X_test.iloc[-1])\n",
    "    X_test_inst = pd.DataFrame(columns=[\"Distance (km)\",\"Avg Pace (/km)\", \"HRSS\", \"Elevation Gain (m)\"])\n",
    "    y_test_inst = pd.DataFrame(columns=[\"AvgHR_bin\"])\n",
    "    for k in range(1,min(look_ahead+1,y_test.shape[0]+1)):\n",
    "        X_test_inst = X_test_inst.append(X_test.iloc[-k])\n",
    "        y_test_inst = y_test_inst.append(y_test.iloc[-k])        \n",
    "    print(\"X test inst: \", X_test_inst)\n",
    "\n",
    "    X_train = X_train.drop(X_train.index[-1]).reset_index(drop=True)\n",
    "    X_train = pd.concat([X_test_inst,X_train]).reset_index(drop = True)\n",
    "    #print(\"X train: \\n\", X_train)\n",
    "\n",
    "    y_train = y_train.drop(y_train.index[-1])\n",
    "    y_train = pd.concat([y_test_inst,y_train]).reset_index(drop=True)\n",
    "    #print(\"y train \\n\", y_train)\n",
    "    \n",
    "    X_test = X_test.drop(X_test_inst.index.values)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    #print(\"X test: \\n\", X_test)\n",
    "\n",
    "    y_test = y_test.drop(y_test_inst.index.values)\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "    \n",
    "    preds_act_df = pd.DataFrame(preds)\n",
    "    act_df = pd.DataFrame(actuals_look_ahead)\n",
    "    print(\"predictions: \", preds)\n",
    "    print(\"actuals: \", actuals_look_ahead)\n",
    "    #preds_act_df = preds_act_df.join(act_df)\n",
    "    #print('actuals and preds: \\n', preds_act_df)\n",
    "    accuracy = metrics.accuracy_score(preds,actuals_look_ahead)\n",
    "    accuracies[i] = accuracy\n",
    "    print('accuracy for iteration {} with look ahead value {}: {}'.format(i+1, look_ahead, accuracy))\n",
    "#print('accuracies: ', accuracies)\n",
    "h=len(accuracies)\n",
    "avg_accuracy = 0\n",
    "for g in range(len(accuracies)):\n",
    "    avg_accuracy += (computations[g]/window_size)*accuracies[g] \n",
    "print('avg accuracy: ', avg_accuracy)\n",
    "# preds_act_df = pd.DataFrame(preds)\n",
    "# preds_act_df = preds_act_df.join(actuals)\n",
    "# print('actuals and preds: \\n', preds_act_df)\n",
    "# accuracy = metrics.accuracy_score(preds_act_df.Actuals.ravel(),preds_act_df.Predictions.ravel())\n",
    "# print('accuracy for window size {}: {}'.format(window_size, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output predictions and actuals for final window size to LR.csv file \n",
    "with open('LR.csv', 'w') as f:\n",
    "    f.write(\"Prediction, Actual \\n\")\n",
    "    for i in range(0,window_size):\n",
    "        f.write(str(predictions[i][0]) + ', ' + str(actuals.iloc[::-1].reset_index(drop = True).iloc[i][0]))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "#### https://towardsdatascience.com/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate Over All Window Sizes for RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Predictions:  1\n",
      "accuracy for window size 308: 1.0\n",
      "# Predictions:  2\n",
      "accuracy for window size 307: 1.0\n",
      "# Predictions:  3\n",
      "accuracy for window size 306: 0.6666666666666666\n",
      "# Predictions:  4\n",
      "accuracy for window size 305: 0.75\n",
      "# Predictions:  5\n",
      "accuracy for window size 304: 0.6\n",
      "# Predictions:  6\n",
      "accuracy for window size 303: 0.8333333333333334\n",
      "# Predictions:  7\n",
      "accuracy for window size 302: 0.8571428571428571\n",
      "# Predictions:  8\n",
      "accuracy for window size 301: 0.875\n",
      "# Predictions:  9\n",
      "accuracy for window size 300: 0.7777777777777778\n",
      "# Predictions:  10\n",
      "accuracy for window size 299: 0.9\n",
      "# Predictions:  11\n",
      "accuracy for window size 298: 0.9090909090909091\n",
      "# Predictions:  12\n",
      "accuracy for window size 297: 0.9166666666666666\n",
      "# Predictions:  13\n",
      "accuracy for window size 296: 0.9230769230769231\n",
      "# Predictions:  14\n",
      "accuracy for window size 295: 0.8571428571428571\n",
      "# Predictions:  15\n",
      "accuracy for window size 294: 0.9333333333333333\n",
      "# Predictions:  16\n",
      "accuracy for window size 293: 0.8125\n",
      "# Predictions:  17\n",
      "accuracy for window size 292: 0.8823529411764706\n",
      "# Predictions:  18\n",
      "accuracy for window size 291: 0.9444444444444444\n",
      "# Predictions:  19\n",
      "accuracy for window size 290: 0.8947368421052632\n",
      "# Predictions:  20\n",
      "accuracy for window size 289: 0.9\n",
      "# Predictions:  21\n",
      "accuracy for window size 288: 0.8571428571428571\n",
      "# Predictions:  22\n",
      "accuracy for window size 287: 0.9090909090909091\n",
      "# Predictions:  23\n",
      "accuracy for window size 286: 0.9130434782608695\n",
      "# Predictions:  24\n",
      "accuracy for window size 285: 0.9166666666666666\n",
      "# Predictions:  25\n",
      "accuracy for window size 284: 0.88\n",
      "# Predictions:  26\n",
      "accuracy for window size 283: 0.9230769230769231\n",
      "# Predictions:  27\n",
      "accuracy for window size 282: 0.9259259259259259\n",
      "# Predictions:  28\n",
      "accuracy for window size 281: 0.9285714285714286\n",
      "# Predictions:  29\n",
      "accuracy for window size 280: 0.9310344827586207\n",
      "# Predictions:  30\n",
      "accuracy for window size 279: 0.9333333333333333\n",
      "# Predictions:  31\n",
      "accuracy for window size 278: 0.9354838709677419\n",
      "# Predictions:  32\n",
      "accuracy for window size 277: 0.875\n",
      "# Predictions:  33\n",
      "accuracy for window size 276: 0.8787878787878788\n",
      "# Predictions:  34\n",
      "accuracy for window size 275: 0.9117647058823529\n",
      "# Predictions:  35\n",
      "accuracy for window size 274: 0.9142857142857143\n",
      "# Predictions:  36\n",
      "accuracy for window size 273: 0.9166666666666666\n",
      "# Predictions:  37\n",
      "accuracy for window size 272: 0.8918918918918919\n",
      "# Predictions:  38\n",
      "accuracy for window size 271: 0.868421052631579\n",
      "# Predictions:  39\n",
      "accuracy for window size 270: 0.8974358974358975\n",
      "# Predictions:  40\n",
      "accuracy for window size 269: 0.875\n",
      "# Predictions:  41\n",
      "accuracy for window size 268: 0.9024390243902439\n",
      "# Predictions:  42\n",
      "accuracy for window size 267: 0.8809523809523809\n",
      "# Predictions:  43\n",
      "accuracy for window size 266: 0.8604651162790697\n",
      "# Predictions:  44\n",
      "accuracy for window size 265: 0.8863636363636364\n",
      "# Predictions:  45\n",
      "accuracy for window size 264: 0.8666666666666667\n",
      "# Predictions:  46\n",
      "accuracy for window size 263: 0.8913043478260869\n",
      "# Predictions:  47\n",
      "accuracy for window size 262: 0.8936170212765957\n",
      "# Predictions:  48\n",
      "accuracy for window size 261: 0.8541666666666666\n",
      "# Predictions:  49\n",
      "accuracy for window size 260: 0.8367346938775511\n",
      "# Predictions:  50\n",
      "accuracy for window size 259: 0.9\n",
      "# Predictions:  51\n",
      "accuracy for window size 258: 0.8627450980392157\n",
      "# Predictions:  52\n",
      "accuracy for window size 257: 0.8653846153846154\n",
      "# Predictions:  53\n",
      "accuracy for window size 256: 0.8867924528301887\n",
      "# Predictions:  54\n",
      "accuracy for window size 255: 0.8703703703703703\n",
      "# Predictions:  55\n",
      "accuracy for window size 254: 0.8545454545454545\n",
      "# Predictions:  56\n",
      "accuracy for window size 253: 0.8928571428571429\n",
      "# Predictions:  57\n",
      "accuracy for window size 252: 0.8596491228070176\n",
      "# Predictions:  58\n",
      "accuracy for window size 251: 0.8793103448275862\n",
      "# Predictions:  59\n",
      "accuracy for window size 250: 0.847457627118644\n",
      "# Predictions:  60\n",
      "accuracy for window size 249: 0.8333333333333334\n",
      "# Predictions:  61\n",
      "accuracy for window size 248: 0.8032786885245902\n",
      "# Predictions:  62\n",
      "accuracy for window size 247: 0.8870967741935484\n",
      "# Predictions:  63\n",
      "accuracy for window size 246: 0.8888888888888888\n",
      "# Predictions:  64\n",
      "accuracy for window size 245: 0.859375\n",
      "# Predictions:  65\n",
      "accuracy for window size 244: 0.8769230769230769\n",
      "# Predictions:  66\n",
      "accuracy for window size 243: 0.8787878787878788\n",
      "# Predictions:  67\n",
      "accuracy for window size 242: 0.8955223880597015\n",
      "# Predictions:  68\n",
      "accuracy for window size 241: 0.8970588235294118\n",
      "# Predictions:  69\n",
      "accuracy for window size 240: 0.8840579710144928\n",
      "# Predictions:  70\n",
      "accuracy for window size 239: 0.8571428571428571\n",
      "# Predictions:  71\n",
      "accuracy for window size 238: 0.8732394366197183\n",
      "# Predictions:  72\n",
      "accuracy for window size 237: 0.8888888888888888\n",
      "# Predictions:  73\n",
      "accuracy for window size 236: 0.9041095890410958\n",
      "# Predictions:  74\n",
      "accuracy for window size 235: 0.8918918918918919\n",
      "# Predictions:  75\n",
      "accuracy for window size 234: 0.88\n",
      "# Predictions:  76\n",
      "accuracy for window size 233: 0.8947368421052632\n",
      "# Predictions:  77\n",
      "accuracy for window size 232: 0.8571428571428571\n",
      "# Predictions:  78\n",
      "accuracy for window size 231: 0.8974358974358975\n",
      "# Predictions:  79\n",
      "accuracy for window size 230: 0.8987341772151899\n",
      "# Predictions:  80\n",
      "accuracy for window size 229: 0.8625\n",
      "# Predictions:  81\n",
      "accuracy for window size 228: 0.8888888888888888\n",
      "# Predictions:  82\n",
      "accuracy for window size 227: 0.8902439024390244\n",
      "# Predictions:  83\n",
      "accuracy for window size 226: 0.8674698795180723\n",
      "# Predictions:  84\n",
      "accuracy for window size 225: 0.8809523809523809\n",
      "# Predictions:  85\n",
      "accuracy for window size 224: 0.8823529411764706\n",
      "# Predictions:  86\n",
      "accuracy for window size 223: 0.872093023255814\n",
      "# Predictions:  87\n",
      "accuracy for window size 222: 0.9080459770114943\n",
      "# Predictions:  88\n",
      "accuracy for window size 221: 0.8977272727272727\n",
      "# Predictions:  89\n",
      "accuracy for window size 220: 0.8764044943820225\n",
      "# Predictions:  90\n",
      "accuracy for window size 219: 0.8888888888888888\n",
      "# Predictions:  91\n",
      "accuracy for window size 218: 0.8681318681318682\n",
      "# Predictions:  92\n",
      "accuracy for window size 217: 0.8804347826086957\n",
      "# Predictions:  93\n",
      "accuracy for window size 216: 0.8709677419354839\n",
      "# Predictions:  94\n",
      "accuracy for window size 215: 0.8829787234042553\n",
      "# Predictions:  95\n",
      "accuracy for window size 214: 0.8631578947368421\n",
      "# Predictions:  96\n",
      "accuracy for window size 213: 0.8645833333333334\n",
      "# Predictions:  97\n",
      "accuracy for window size 212: 0.8556701030927835\n",
      "# Predictions:  98\n",
      "accuracy for window size 211: 0.8571428571428571\n",
      "# Predictions:  99\n",
      "accuracy for window size 210: 0.8585858585858586\n",
      "# Predictions:  100\n",
      "accuracy for window size 209: 0.89\n",
      "# Predictions:  101\n",
      "accuracy for window size 208: 0.8811881188118812\n",
      "# Predictions:  102\n",
      "accuracy for window size 207: 0.8627450980392157\n",
      "# Predictions:  103\n",
      "accuracy for window size 206: 0.8640776699029126\n",
      "# Predictions:  104\n",
      "accuracy for window size 205: 0.8653846153846154\n",
      "# Predictions:  105\n",
      "accuracy for window size 204: 0.8571428571428571\n",
      "# Predictions:  106\n",
      "accuracy for window size 203: 0.8679245283018868\n",
      "# Predictions:  107\n",
      "accuracy for window size 202: 0.8785046728971962\n",
      "# Predictions:  108\n",
      "accuracy for window size 201: 0.8611111111111112\n",
      "# Predictions:  109\n",
      "accuracy for window size 200: 0.8348623853211009\n",
      "# Predictions:  110\n",
      "accuracy for window size 199: 0.8727272727272727\n",
      "# Predictions:  111\n",
      "accuracy for window size 198: 0.8828828828828829\n",
      "# Predictions:  112\n",
      "accuracy for window size 197: 0.8571428571428571\n",
      "# Predictions:  113\n",
      "accuracy for window size 196: 0.8407079646017699\n",
      "# Predictions:  114\n",
      "accuracy for window size 195: 0.8421052631578947\n",
      "# Predictions:  115\n",
      "accuracy for window size 194: 0.8521739130434782\n",
      "# Predictions:  116\n",
      "accuracy for window size 193: 0.8362068965517241\n",
      "# Predictions:  117\n",
      "accuracy for window size 192: 0.8632478632478633\n",
      "# Predictions:  118\n",
      "accuracy for window size 191: 0.8305084745762712\n",
      "# Predictions:  119\n",
      "accuracy for window size 190: 0.8319327731092437\n",
      "# Predictions:  120\n",
      "accuracy for window size 189: 0.8083333333333333\n",
      "# Predictions:  121\n",
      "accuracy for window size 188: 0.8429752066115702\n",
      "# Predictions:  122\n",
      "accuracy for window size 187: 0.8278688524590164\n",
      "# Predictions:  123\n",
      "accuracy for window size 186: 0.8373983739837398\n",
      "# Predictions:  124\n",
      "accuracy for window size 185: 0.8548387096774194\n",
      "# Predictions:  125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for window size 184: 0.84\n",
      "# Predictions:  126\n",
      "accuracy for window size 183: 0.8174603174603174\n",
      "# Predictions:  127\n",
      "accuracy for window size 182: 0.8188976377952756\n",
      "# Predictions:  128\n",
      "accuracy for window size 181: 0.8203125\n",
      "# Predictions:  129\n",
      "accuracy for window size 180: 0.8217054263565892\n",
      "# Predictions:  130\n",
      "accuracy for window size 179: 0.8307692307692308\n",
      "# Predictions:  131\n",
      "accuracy for window size 178: 0.8244274809160306\n",
      "# Predictions:  132\n",
      "accuracy for window size 177: 0.8409090909090909\n",
      "# Predictions:  133\n",
      "accuracy for window size 176: 0.8195488721804511\n",
      "# Predictions:  134\n",
      "accuracy for window size 175: 0.8432835820895522\n",
      "# Predictions:  135\n",
      "accuracy for window size 174: 0.8444444444444444\n",
      "# Predictions:  136\n",
      "accuracy for window size 173: 0.8308823529411765\n",
      "# Predictions:  137\n",
      "accuracy for window size 172: 0.8175182481751825\n",
      "# Predictions:  138\n",
      "accuracy for window size 171: 0.8260869565217391\n",
      "# Predictions:  139\n",
      "accuracy for window size 170: 0.8201438848920863\n",
      "# Predictions:  140\n",
      "accuracy for window size 169: 0.8285714285714286\n",
      "# Predictions:  141\n",
      "accuracy for window size 168: 0.8156028368794326\n",
      "# Predictions:  142\n",
      "accuracy for window size 167: 0.8309859154929577\n",
      "# Predictions:  143\n",
      "accuracy for window size 166: 0.8251748251748252\n",
      "# Predictions:  144\n",
      "accuracy for window size 165: 0.8055555555555556\n",
      "# Predictions:  145\n",
      "accuracy for window size 164: 0.8137931034482758\n",
      "# Predictions:  146\n",
      "accuracy for window size 163: 0.821917808219178\n",
      "# Predictions:  147\n",
      "accuracy for window size 162: 0.8163265306122449\n",
      "# Predictions:  148\n",
      "accuracy for window size 161: 0.8040540540540541\n",
      "# Predictions:  149\n",
      "accuracy for window size 160: 0.8389261744966443\n",
      "# Predictions:  150\n",
      "accuracy for window size 159: 0.82\n",
      "# Predictions:  151\n",
      "accuracy for window size 158: 0.8211920529801324\n",
      "# Predictions:  152\n",
      "accuracy for window size 157: 0.8157894736842105\n",
      "# Predictions:  153\n",
      "accuracy for window size 156: 0.8169934640522876\n",
      "# Predictions:  154\n",
      "accuracy for window size 155: 0.8181818181818182\n",
      "# Predictions:  155\n",
      "accuracy for window size 154: 0.7935483870967742\n",
      "# Predictions:  156\n",
      "accuracy for window size 153: 0.8141025641025641\n",
      "# Predictions:  157\n",
      "accuracy for window size 152: 0.802547770700637\n",
      "# Predictions:  158\n",
      "accuracy for window size 151: 0.7911392405063291\n",
      "# Predictions:  159\n",
      "accuracy for window size 150: 0.7987421383647799\n",
      "# Predictions:  160\n",
      "accuracy for window size 149: 0.78125\n",
      "# Predictions:  161\n",
      "accuracy for window size 148: 0.7888198757763976\n",
      "# Predictions:  162\n",
      "accuracy for window size 147: 0.8024691358024691\n",
      "# Predictions:  163\n",
      "accuracy for window size 146: 0.7914110429447853\n",
      "# Predictions:  164\n",
      "accuracy for window size 145: 0.8170731707317073\n",
      "# Predictions:  165\n",
      "accuracy for window size 144: 0.8121212121212121\n",
      "# Predictions:  166\n",
      "accuracy for window size 143: 0.8072289156626506\n",
      "# Predictions:  167\n",
      "accuracy for window size 142: 0.7964071856287425\n",
      "# Predictions:  168\n",
      "accuracy for window size 141: 0.7916666666666666\n",
      "# Predictions:  169\n",
      "accuracy for window size 140: 0.8047337278106509\n",
      "# Predictions:  170\n",
      "accuracy for window size 139: 0.7941176470588235\n",
      "# Predictions:  171\n",
      "accuracy for window size 138: 0.8011695906432749\n",
      "# Predictions:  172\n",
      "accuracy for window size 137: 0.7732558139534884\n",
      "# Predictions:  173\n",
      "accuracy for window size 136: 0.7687861271676301\n",
      "# Predictions:  174\n",
      "accuracy for window size 135: 0.7816091954022989\n",
      "# Predictions:  175\n",
      "accuracy for window size 134: 0.7885714285714286\n",
      "# Predictions:  176\n",
      "accuracy for window size 133: 0.7840909090909091\n",
      "# Predictions:  177\n",
      "accuracy for window size 132: 0.8022598870056498\n",
      "# Predictions:  178\n",
      "accuracy for window size 131: 0.7808988764044944\n",
      "# Predictions:  179\n",
      "accuracy for window size 130: 0.770949720670391\n",
      "# Predictions:  180\n",
      "accuracy for window size 129: 0.7833333333333333\n",
      "# Predictions:  181\n",
      "accuracy for window size 128: 0.7734806629834254\n",
      "# Predictions:  182\n",
      "accuracy for window size 127: 0.7582417582417582\n",
      "# Predictions:  183\n",
      "accuracy for window size 126: 0.8032786885245902\n",
      "# Predictions:  184\n",
      "accuracy for window size 125: 0.7880434782608695\n",
      "# Predictions:  185\n",
      "accuracy for window size 124: 0.7891891891891892\n",
      "# Predictions:  186\n",
      "accuracy for window size 123: 0.7688172043010753\n",
      "# Predictions:  187\n",
      "accuracy for window size 122: 0.7700534759358288\n",
      "# Predictions:  188\n",
      "accuracy for window size 121: 0.7819148936170213\n",
      "# Predictions:  189\n",
      "accuracy for window size 120: 0.7671957671957672\n",
      "# Predictions:  190\n",
      "accuracy for window size 119: 0.7736842105263158\n",
      "# Predictions:  191\n",
      "accuracy for window size 118: 0.774869109947644\n",
      "# Predictions:  192\n",
      "accuracy for window size 117: 0.7708333333333334\n",
      "# Predictions:  193\n",
      "accuracy for window size 116: 0.7668393782383419\n",
      "# Predictions:  194\n",
      "accuracy for window size 115: 0.7731958762886598\n",
      "# Predictions:  195\n",
      "accuracy for window size 114: 0.7897435897435897\n",
      "# Predictions:  196\n",
      "accuracy for window size 113: 0.7755102040816326\n",
      "# Predictions:  197\n",
      "accuracy for window size 112: 0.7969543147208121\n",
      "# Predictions:  198\n",
      "accuracy for window size 111: 0.797979797979798\n",
      "# Predictions:  199\n",
      "accuracy for window size 110: 0.7738693467336684\n",
      "# Predictions:  200\n",
      "accuracy for window size 109: 0.795\n",
      "# Predictions:  201\n",
      "accuracy for window size 108: 0.7711442786069652\n",
      "# Predictions:  202\n",
      "accuracy for window size 107: 0.7871287128712872\n",
      "# Predictions:  203\n",
      "accuracy for window size 106: 0.7980295566502463\n",
      "# Predictions:  204\n",
      "accuracy for window size 105: 0.7696078431372549\n",
      "# Predictions:  205\n",
      "accuracy for window size 104: 0.7707317073170732\n",
      "# Predictions:  206\n",
      "accuracy for window size 103: 0.7815533980582524\n",
      "# Predictions:  207\n",
      "accuracy for window size 102: 0.7729468599033816\n",
      "# Predictions:  208\n",
      "accuracy for window size 101: 0.7692307692307693\n",
      "# Predictions:  209\n",
      "accuracy for window size 100: 0.7799043062200957\n",
      "# Predictions:  210\n",
      "accuracy for window size 99: 0.8\n",
      "# Predictions:  211\n",
      "accuracy for window size 98: 0.7772511848341233\n",
      "# Predictions:  212\n",
      "accuracy for window size 97: 0.7877358490566038\n",
      "# Predictions:  213\n",
      "accuracy for window size 96: 0.784037558685446\n",
      "# Predictions:  214\n",
      "accuracy for window size 95: 0.7897196261682243\n",
      "# Predictions:  215\n",
      "accuracy for window size 94: 0.786046511627907\n",
      "# Predictions:  216\n",
      "accuracy for window size 93: 0.7870370370370371\n",
      "# Predictions:  217\n",
      "accuracy for window size 92: 0.7557603686635944\n",
      "# Predictions:  218\n",
      "accuracy for window size 91: 0.7798165137614679\n",
      "# Predictions:  219\n",
      "accuracy for window size 90: 0.8036529680365296\n",
      "# Predictions:  220\n",
      "accuracy for window size 89: 0.7818181818181819\n",
      "# Predictions:  221\n",
      "accuracy for window size 88: 0.7828054298642534\n",
      "# Predictions:  222\n",
      "accuracy for window size 87: 0.7927927927927928\n",
      "# Predictions:  223\n",
      "accuracy for window size 86: 0.7937219730941704\n",
      "# Predictions:  224\n",
      "accuracy for window size 85: 0.7991071428571429\n",
      "# Predictions:  225\n",
      "accuracy for window size 84: 0.7911111111111111\n",
      "# Predictions:  226\n",
      "accuracy for window size 83: 0.8053097345132744\n",
      "# Predictions:  227\n",
      "accuracy for window size 82: 0.8061674008810573\n",
      "# Predictions:  228\n",
      "accuracy for window size 81: 0.8026315789473685\n",
      "# Predictions:  229\n",
      "accuracy for window size 80: 0.7991266375545851\n",
      "# Predictions:  230\n",
      "accuracy for window size 79: 0.7956521739130434\n",
      "# Predictions:  231\n",
      "accuracy for window size 78: 0.8008658008658008\n",
      "# Predictions:  232\n",
      "accuracy for window size 77: 0.7887931034482759\n",
      "# Predictions:  233\n",
      "accuracy for window size 76: 0.7854077253218884\n",
      "# Predictions:  234\n",
      "accuracy for window size 75: 0.7905982905982906\n",
      "# Predictions:  235\n",
      "accuracy for window size 74: 0.8\n",
      "# Predictions:  236\n",
      "accuracy for window size 73: 0.788135593220339\n",
      "# Predictions:  237\n",
      "accuracy for window size 72: 0.7974683544303798\n",
      "# Predictions:  238\n",
      "accuracy for window size 71: 0.773109243697479\n",
      "# Predictions:  239\n",
      "accuracy for window size 70: 0.7824267782426778\n",
      "# Predictions:  240\n",
      "accuracy for window size 69: 0.7708333333333334\n",
      "# Predictions:  241\n",
      "accuracy for window size 68: 0.7966804979253111\n",
      "# Predictions:  242\n",
      "accuracy for window size 67: 0.78099173553719\n",
      "# Predictions:  243\n",
      "accuracy for window size 66: 0.7736625514403292\n",
      "# Predictions:  244\n",
      "accuracy for window size 65: 0.7786885245901639\n",
      "# Predictions:  245\n",
      "accuracy for window size 64: 0.7714285714285715\n",
      "# Predictions:  246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for window size 63: 0.7642276422764228\n",
      "# Predictions:  247\n",
      "accuracy for window size 62: 0.7611336032388664\n",
      "# Predictions:  248\n",
      "accuracy for window size 61: 0.7620967741935484\n",
      "# Predictions:  249\n",
      "accuracy for window size 60: 0.7590361445783133\n",
      "# Predictions:  250\n",
      "accuracy for window size 59: 0.776\n",
      "# Predictions:  251\n",
      "accuracy for window size 58: 0.7649402390438247\n",
      "# Predictions:  252\n",
      "accuracy for window size 57: 0.7658730158730159\n",
      "# Predictions:  253\n",
      "accuracy for window size 56: 0.7509881422924901\n",
      "# Predictions:  254\n",
      "accuracy for window size 55: 0.7716535433070866\n",
      "# Predictions:  255\n",
      "accuracy for window size 54: 0.7686274509803922\n",
      "# Predictions:  256\n",
      "accuracy for window size 53: 0.76171875\n",
      "# Predictions:  257\n",
      "accuracy for window size 52: 0.7782101167315175\n",
      "# Predictions:  258\n",
      "accuracy for window size 51: 0.7713178294573644\n",
      "# Predictions:  259\n",
      "accuracy for window size 50: 0.7683397683397684\n",
      "# Predictions:  260\n",
      "accuracy for window size 49: 0.7615384615384615\n",
      "# Predictions:  261\n",
      "accuracy for window size 48: 0.7662835249042146\n",
      "# Predictions:  262\n",
      "accuracy for window size 47: 0.7480916030534351\n",
      "# Predictions:  263\n",
      "accuracy for window size 46: 0.7490494296577946\n",
      "# Predictions:  264\n",
      "accuracy for window size 45: 0.7386363636363636\n",
      "# Predictions:  265\n",
      "accuracy for window size 44: 0.7509433962264151\n",
      "# Predictions:  266\n",
      "accuracy for window size 43: 0.7406015037593985\n",
      "# Predictions:  267\n",
      "accuracy for window size 42: 0.7453183520599251\n",
      "# Predictions:  268\n",
      "accuracy for window size 41: 0.7425373134328358\n",
      "# Predictions:  269\n",
      "accuracy for window size 40: 0.7509293680297398\n",
      "# Predictions:  270\n",
      "accuracy for window size 39: 0.762962962962963\n",
      "# Predictions:  271\n",
      "accuracy for window size 38: 0.7601476014760148\n",
      "# Predictions:  272\n",
      "accuracy for window size 37: 0.7720588235294118\n",
      "# Predictions:  273\n",
      "accuracy for window size 36: 0.7619047619047619\n",
      "# Predictions:  274\n",
      "accuracy for window size 35: 0.7518248175182481\n",
      "# Predictions:  275\n",
      "accuracy for window size 34: 0.7527272727272727\n",
      "# Predictions:  276\n",
      "accuracy for window size 33: 0.7536231884057971\n",
      "Accuracies:  [1.         1.         0.66666667 0.75       0.6        0.83333333\n",
      " 0.85714286 0.875      0.77777778 0.9        0.90909091 0.91666667\n",
      " 0.92307692 0.85714286 0.93333333 0.8125     0.88235294 0.94444444\n",
      " 0.89473684 0.9        0.85714286 0.90909091 0.91304348 0.91666667\n",
      " 0.88       0.92307692 0.92592593 0.92857143 0.93103448 0.93333333\n",
      " 0.93548387 0.875      0.87878788 0.91176471 0.91428571 0.91666667\n",
      " 0.89189189 0.86842105 0.8974359  0.875      0.90243902 0.88095238\n",
      " 0.86046512 0.88636364 0.86666667 0.89130435 0.89361702 0.85416667\n",
      " 0.83673469 0.9        0.8627451  0.86538462 0.88679245 0.87037037\n",
      " 0.85454545 0.89285714 0.85964912 0.87931034 0.84745763 0.83333333\n",
      " 0.80327869 0.88709677 0.88888889 0.859375   0.87692308 0.87878788\n",
      " 0.89552239 0.89705882 0.88405797 0.85714286 0.87323944 0.88888889\n",
      " 0.90410959 0.89189189 0.88       0.89473684 0.85714286 0.8974359\n",
      " 0.89873418 0.8625     0.88888889 0.8902439  0.86746988 0.88095238\n",
      " 0.88235294 0.87209302 0.90804598 0.89772727 0.87640449 0.88888889\n",
      " 0.86813187 0.88043478 0.87096774 0.88297872 0.86315789 0.86458333\n",
      " 0.8556701  0.85714286 0.85858586 0.89       0.88118812 0.8627451\n",
      " 0.86407767 0.86538462 0.85714286 0.86792453 0.87850467 0.86111111\n",
      " 0.83486239 0.87272727 0.88288288 0.85714286 0.84070796 0.84210526\n",
      " 0.85217391 0.8362069  0.86324786 0.83050847 0.83193277 0.80833333\n",
      " 0.84297521 0.82786885 0.83739837 0.85483871 0.84       0.81746032\n",
      " 0.81889764 0.8203125  0.82170543 0.83076923 0.82442748 0.84090909\n",
      " 0.81954887 0.84328358 0.84444444 0.83088235 0.81751825 0.82608696\n",
      " 0.82014388 0.82857143 0.81560284 0.83098592 0.82517483 0.80555556\n",
      " 0.8137931  0.82191781 0.81632653 0.80405405 0.83892617 0.82\n",
      " 0.82119205 0.81578947 0.81699346 0.81818182 0.79354839 0.81410256\n",
      " 0.80254777 0.79113924 0.79874214 0.78125    0.78881988 0.80246914\n",
      " 0.79141104 0.81707317 0.81212121 0.80722892 0.79640719 0.79166667\n",
      " 0.80473373 0.79411765 0.80116959 0.77325581 0.76878613 0.7816092\n",
      " 0.78857143 0.78409091 0.80225989 0.78089888 0.77094972 0.78333333\n",
      " 0.77348066 0.75824176 0.80327869 0.78804348 0.78918919 0.7688172\n",
      " 0.77005348 0.78191489 0.76719577 0.77368421 0.77486911 0.77083333\n",
      " 0.76683938 0.77319588 0.78974359 0.7755102  0.79695431 0.7979798\n",
      " 0.77386935 0.795      0.77114428 0.78712871 0.79802956 0.76960784\n",
      " 0.77073171 0.7815534  0.77294686 0.76923077 0.77990431 0.8\n",
      " 0.77725118 0.78773585 0.78403756 0.78971963 0.78604651 0.78703704\n",
      " 0.75576037 0.77981651 0.80365297 0.78181818 0.78280543 0.79279279\n",
      " 0.79372197 0.79910714 0.79111111 0.80530973 0.8061674  0.80263158\n",
      " 0.79912664 0.79565217 0.8008658  0.7887931  0.78540773 0.79059829\n",
      " 0.8        0.78813559 0.79746835 0.77310924 0.78242678 0.77083333\n",
      " 0.7966805  0.78099174 0.77366255 0.77868852 0.77142857 0.76422764\n",
      " 0.7611336  0.76209677 0.75903614 0.776      0.76494024 0.76587302\n",
      " 0.75098814 0.77165354 0.76862745 0.76171875 0.77821012 0.77131783\n",
      " 0.76833977 0.76153846 0.76628352 0.7480916  0.74904943 0.73863636\n",
      " 0.7509434  0.7406015  0.74531835 0.74253731 0.75092937 0.76296296\n",
      " 0.7601476  0.77205882 0.76190476 0.75182482 0.75272727 0.75362319]\n"
     ]
    }
   ],
   "source": [
    "# Iterate over all window sizes to determine optimal size for RF\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import math\n",
    "import statistics as stats\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Create the model with 100 trees\n",
    "RF = RandomForestClassifier(n_estimators=100, \n",
    "                               bootstrap = True,\n",
    "                               max_features = 'sqrt')\n",
    "\n",
    "\n",
    "\n",
    "# Change window_size_optimization flag to perform iterations\n",
    "window_size_optimization = False\n",
    "\n",
    "if window_size_optimization == True:\n",
    "    accuracies = np.zeros(276)\n",
    "    roc_values = np.zeros(276)\n",
    "    #298 accuracy indices\n",
    "\n",
    "    # Iterate over all window sizes and compute accuracy for each LR model\n",
    "    for window_size in range(1,277):\n",
    "    #window_size = 299\n",
    "        print('# Predictions: ', window_size)\n",
    "        X_train = X.iloc[window_size:]\n",
    "        y_train = y.iloc[window_size:]\n",
    "        # print('train labels: ', y_train)\n",
    "        X_test = X.iloc[:window_size]\n",
    "        y_test = y.iloc[:window_size]\n",
    "        # print('test labels: ', y_test)\n",
    "\n",
    "        actuals = pd.DataFrame(y_test)\n",
    "        actuals = actuals.rename(columns={'AvgHR_bin':'Actuals'})\n",
    "        # print('actuals: \\n', actuals)\n",
    "        preds = np.zeros(X_test.shape[0])\n",
    "        rf_probs = np.zeros(X_test.shape[0])\n",
    "        # print('X_test: \\n', X_test)\n",
    "        for i in range(0,y_test.shape[0]):\n",
    "            # print('X train shape: ', X_train.shape[0])\n",
    "            # print('X train: ', X_train.head())\n",
    "            RF.fit(X_train, y_train.values.ravel())\n",
    "            # Predict test set\n",
    "            # print('X test: ', X_test.iloc[0])\n",
    "            y_pred = RF.predict(np.array(X_test.iloc[-1]).reshape(1,-1))\n",
    "            # print('actual: ',y_test.loc[0, 'AvgHR_bin'], '\\n pred: ',y_pred, '\\n')\n",
    "            preds[i] = y_pred\n",
    "#             rf_probs[i] = RF.predict_proba(np.array(X_test.iloc[-i]).reshape(1,-1))[:, 1]\n",
    "            #print('Accuracy of logistic regression classifier on test set {}: {:.2f}'.format(i, logreg.score(X_test, y_test)))\n",
    "            #print(\"X test -1: \", X_test.iloc[-1])\n",
    "            \n",
    "            X_test_inst = pd.DataFrame(data= [X_test.iloc[-1]],columns=[\"Distance (km)\",\"Avg Pace (/km)\", \"HRSS\", \"Elevation Gain (m)\"])\n",
    "            y_test_inst = pd.DataFrame(data = [y_test.iloc[-1]],columns=[\"AvgHR_bin\"])\n",
    "            #print(\"X test inst: \", X_test_inst)\n",
    "            X_train = X_train.drop(X_train.index[-1]).reset_index(drop=True)\n",
    "            X_train = pd.concat([X_test_inst,X_train]).reset_index(drop = True)\n",
    "            #print(\"X train: \\n\", X_train)\n",
    "\n",
    "            y_train = y_train.drop(y_train.index[-1])\n",
    "            y_train = pd.concat([y_test_inst,y_train])\n",
    "            y_train = y_train.reset_index(drop=True)\n",
    "            #print(\"y train \\n\", y_train)\n",
    "\n",
    "            X_test = X_test.drop(X_test.index[-1])\n",
    "            X_test = X_test.reset_index(drop=True)\n",
    "            #print(\"X test: \\n\", X_test)\n",
    "            y_test = y_test.drop(y_test.index[-1])\n",
    "            y_test = y_test.reset_index(drop=True)\n",
    "            \n",
    "        preds_act_df = pd.DataFrame(preds, columns=['Predictions'])\n",
    "        preds_act_df = preds_act_df = preds_act_df.join(actuals.iloc[::-1].reset_index(drop = True))\n",
    "        # print('actuals and preds: \\n', preds_act_df)\n",
    "        accuracy = metrics.accuracy_score(preds_act_df.Actuals.ravel(),preds_act_df.Predictions.ravel())\n",
    "        print('accuracy for window size {}: {}'.format(309-window_size, accuracy))\n",
    "        accuracies[window_size-1] = accuracy\n",
    "    print('Accuracies: ', accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('RF_Accuracies_for_Window_Size_Variations_2.csv', 'w') as f:\n",
    "        for i in range(0,len(accuracies)):\n",
    "            f.write(str(308-i) + ': ' + str(accuracies[i]))\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Window             Accuracy\n",
      "0      308                  1.0\n",
      "1      307                  1.0\n",
      "17     291   0.9444444444444444\n",
      "30     278   0.9354838709677419\n",
      "14     294   0.9333333333333333\n",
      "29     279   0.9333333333333333\n",
      "28     280   0.9310344827586207\n",
      "27     281   0.9285714285714286\n",
      "26     282   0.9259259259259259\n",
      "12     296   0.9230769230769231\n",
      "25     283   0.9230769230769231\n",
      "35     273   0.9166666666666666\n",
      "11     297   0.9166666666666666\n",
      "23     285   0.9166666666666666\n",
      "34     274   0.9142857142857143\n",
      "22     286   0.9130434782608695\n",
      "33     275   0.9117647058823529\n",
      "21     287   0.9090909090909091\n",
      "10     298   0.9090909090909091\n",
      "86     222   0.9080459770114943\n",
      "72     236   0.9041095890410958\n",
      "40     268   0.9024390243902439\n",
      "19     289                  0.9\n",
      "49     259                  0.9\n",
      "9      299                  0.9\n",
      "78     230   0.8987341772151899\n",
      "87     221   0.8977272727272727\n",
      "38     270   0.8974358974358975\n",
      "77     231   0.8974358974358975\n",
      "67     241   0.8970588235294118\n",
      "66     242   0.8955223880597015\n",
      "75     233   0.8947368421052632\n",
      "18     290   0.8947368421052632\n",
      "46     262   0.8936170212765957\n",
      "55     253   0.8928571428571429\n",
      "36     272   0.8918918918918919\n",
      "73     235   0.8918918918918919\n",
      "45     263   0.8913043478260869\n",
      "81     227   0.8902439024390244\n",
      "99     209                 0.89\n",
      "71     237   0.8888888888888888\n",
      "89     219   0.8888888888888888\n",
      "80     228   0.8888888888888888\n",
      "62     246   0.8888888888888888\n",
      "61     247   0.8870967741935484\n",
      "52     256   0.8867924528301887\n",
      "43     265   0.8863636363636364\n",
      "68     240   0.8840579710144928\n",
      "93     215   0.8829787234042553\n",
      "110    198   0.8828828828828829\n",
      "16     292   0.8823529411764706\n",
      "84     224   0.8823529411764706\n",
      "100    208   0.8811881188118812\n",
      "41     267   0.8809523809523809\n",
      "83     225   0.8809523809523809\n",
      "91     217   0.8804347826086957\n",
      "74     234                 0.88\n",
      "24     284                 0.88\n",
      "57     251   0.8793103448275862\n",
      "32     276   0.8787878787878788\n",
      "65     243   0.8787878787878788\n",
      "106    202   0.8785046728971962\n",
      "64     244   0.8769230769230769\n",
      "88     220   0.8764044943820225\n",
      "7      301                0.875\n",
      "31     277                0.875\n",
      "39     269                0.875\n",
      "70     238   0.8732394366197183\n",
      "109    199   0.8727272727272727\n",
      "85     223    0.872093023255814\n",
      "92     216   0.8709677419354839\n",
      "53     255   0.8703703703703703\n",
      "37     271    0.868421052631579\n",
      "90     218   0.8681318681318682\n",
      "105    203   0.8679245283018868\n",
      "82     226   0.8674698795180723\n",
      "44     264   0.8666666666666667\n",
      "51     257   0.8653846153846154\n",
      "103    205   0.8653846153846154\n",
      "95     213   0.8645833333333334\n",
      "102    206   0.8640776699029126\n",
      "116    192   0.8632478632478633\n",
      "94     214   0.8631578947368421\n",
      "50     258   0.8627450980392157\n",
      "101    207   0.8627450980392157\n",
      "79     229               0.8625\n",
      "107    201   0.8611111111111112\n",
      "42     266   0.8604651162790697\n",
      "56     252   0.8596491228070176\n",
      "63     245             0.859375\n",
      "98     210   0.8585858585858586\n",
      "76     232   0.8571428571428571\n",
      "97     211   0.8571428571428571\n",
      "20     288   0.8571428571428571\n",
      "111    197   0.8571428571428571\n",
      "6      302   0.8571428571428571\n",
      "104    204   0.8571428571428571\n",
      "69     239   0.8571428571428571\n",
      "13     295   0.8571428571428571\n",
      "96     212   0.8556701030927835\n",
      "123    185   0.8548387096774194\n",
      "54     254   0.8545454545454545\n",
      "47     261   0.8541666666666666\n",
      "114    194   0.8521739130434782\n",
      "58     250    0.847457627118644\n",
      "134    174   0.8444444444444444\n",
      "133    175   0.8432835820895522\n",
      "120    188   0.8429752066115702\n",
      "113    195   0.8421052631578947\n",
      "131    177   0.8409090909090909\n",
      "112    196   0.8407079646017699\n",
      "124    184                 0.84\n",
      "148    160   0.8389261744966443\n",
      "122    186   0.8373983739837398\n",
      "48     260   0.8367346938775511\n",
      "115    193   0.8362068965517241\n",
      "108    200   0.8348623853211009\n",
      "5      303   0.8333333333333334\n",
      "59     249   0.8333333333333334\n",
      "118    190   0.8319327731092437\n",
      "141    167   0.8309859154929577\n",
      "135    173   0.8308823529411765\n",
      "129    179   0.8307692307692308\n",
      "117    191   0.8305084745762712\n",
      "139    169   0.8285714285714286\n",
      "121    187   0.8278688524590164\n",
      "137    171   0.8260869565217391\n",
      "142    166   0.8251748251748252\n",
      "130    178   0.8244274809160306\n",
      "145    163    0.821917808219178\n",
      "128    180   0.8217054263565892\n",
      "150    158   0.8211920529801324\n",
      "127    181            0.8203125\n",
      "138    170   0.8201438848920863\n",
      "149    159                 0.82\n",
      "132    176   0.8195488721804511\n",
      "126    182   0.8188976377952756\n",
      "153    155   0.8181818181818182\n",
      "136    172   0.8175182481751825\n",
      "125    183   0.8174603174603174\n",
      "163    145   0.8170731707317073\n",
      "152    156   0.8169934640522876\n",
      "146    162   0.8163265306122449\n",
      "151    157   0.8157894736842105\n",
      "140    168   0.8156028368794326\n",
      "155    153   0.8141025641025641\n",
      "144    164   0.8137931034482758\n",
      "15     293               0.8125\n",
      "164    144   0.8121212121212121\n",
      "119    189   0.8083333333333333\n",
      "165    143   0.8072289156626506\n",
      "226     82   0.8061674008810573\n",
      "143    165   0.8055555555555556\n",
      "225     83   0.8053097345132744\n",
      "168    140   0.8047337278106509\n",
      "147    161   0.8040540540540541\n",
      "218     90   0.8036529680365296\n",
      "182    126   0.8032786885245902\n",
      "60     248   0.8032786885245902\n",
      "227     81   0.8026315789473685\n",
      "156    152    0.802547770700637\n",
      "161    147   0.8024691358024691\n",
      "176    132   0.8022598870056498\n",
      "170    138   0.8011695906432749\n",
      "230     78   0.8008658008658008\n",
      "234     74                  0.8\n",
      "209     99                  0.8\n",
      "228     80   0.7991266375545851\n",
      "223     85   0.7991071428571429\n",
      "158    150   0.7987421383647799\n",
      "202    106   0.7980295566502463\n",
      "197    111    0.797979797979798\n",
      "236     72   0.7974683544303798\n",
      "196    112   0.7969543147208121\n",
      "240     68   0.7966804979253111\n",
      "166    142   0.7964071856287425\n",
      "229     79   0.7956521739130434\n",
      "199    109                0.795\n",
      "169    139   0.7941176470588235\n",
      "222     86   0.7937219730941704\n",
      "154    154   0.7935483870967742\n",
      "221     87   0.7927927927927928\n",
      "167    141   0.7916666666666666\n",
      "162    146   0.7914110429447853\n",
      "157    151   0.7911392405063291\n",
      "224     84   0.7911111111111111\n",
      "233     75   0.7905982905982906\n",
      "194    114   0.7897435897435897\n",
      "213     95   0.7897196261682243\n",
      "184    124   0.7891891891891892\n",
      "160    148   0.7888198757763976\n",
      "231     77   0.7887931034482759\n",
      "174    134   0.7885714285714286\n",
      "235     73    0.788135593220339\n",
      "183    125   0.7880434782608695\n",
      "211     97   0.7877358490566038\n",
      "201    107   0.7871287128712872\n",
      "215     93   0.7870370370370371\n",
      "214     94    0.786046511627907\n",
      "232     76   0.7854077253218884\n",
      "175    133   0.7840909090909091\n",
      "212     96    0.784037558685446\n",
      "179    129   0.7833333333333333\n",
      "220     88   0.7828054298642534\n",
      "238     70   0.7824267782426778\n",
      "187    121   0.7819148936170213\n",
      "219     89   0.7818181818181819\n",
      "173    135   0.7816091954022989\n",
      "205    103   0.7815533980582524\n",
      "159    149              0.78125\n",
      "241     67     0.78099173553719\n",
      "177    131   0.7808988764044944\n",
      "208    100   0.7799043062200957\n",
      "217     91   0.7798165137614679\n",
      "243     65   0.7786885245901639\n",
      "256     52   0.7782101167315175\n",
      "8      300   0.7777777777777778\n",
      "210     98   0.7772511848341233\n",
      "249     59                0.776\n",
      "195    113   0.7755102040816326\n",
      "190    118    0.774869109947644\n",
      "198    110   0.7738693467336684\n",
      "189    119   0.7736842105263158\n",
      "242     66   0.7736625514403292\n",
      "180    128   0.7734806629834254\n",
      "171    137   0.7732558139534884\n",
      "193    115   0.7731958762886598\n",
      "237     71    0.773109243697479\n",
      "206    102   0.7729468599033816\n",
      "271     37   0.7720588235294118\n",
      "253     55   0.7716535433070866\n",
      "244     64   0.7714285714285715\n",
      "257     51   0.7713178294573644\n",
      "200    108   0.7711442786069652\n",
      "178    130    0.770949720670391\n",
      "239     69   0.7708333333333334\n",
      "191    117   0.7708333333333334\n",
      "204    104   0.7707317073170732\n",
      "186    122   0.7700534759358288\n",
      "203    105   0.7696078431372549\n",
      "207    101   0.7692307692307693\n",
      "185    123   0.7688172043010753\n",
      "172    136   0.7687861271676301\n",
      "254     54   0.7686274509803922\n",
      "258     50   0.7683397683397684\n",
      "188    120   0.7671957671957672\n",
      "192    116   0.7668393782383419\n",
      "260     48   0.7662835249042146\n",
      "251     57   0.7658730158730159\n",
      "250     58   0.7649402390438247\n",
      "245     63   0.7642276422764228\n",
      "269     39    0.762962962962963\n",
      "247     61   0.7620967741935484\n",
      "272     36   0.7619047619047619\n",
      "255     53           0.76171875\n",
      "259     49   0.7615384615384615\n",
      "246     62   0.7611336032388664\n",
      "270     38   0.7601476014760148\n",
      "248     60   0.7590361445783133\n",
      "181    127   0.7582417582417582\n",
      "216     92   0.7557603686635944\n",
      "275     33   0.7536231884057971\n",
      "274     34   0.7527272727272727\n",
      "273     35   0.7518248175182481\n",
      "252     56   0.7509881422924901\n",
      "264     44   0.7509433962264151\n",
      "268     40   0.7509293680297398\n",
      "3      305                 0.75\n",
      "262     46   0.7490494296577946\n",
      "261     47   0.7480916030534351\n",
      "266     42   0.7453183520599251\n",
      "267     41   0.7425373134328358\n",
      "265     43   0.7406015037593985\n",
      "263     45   0.7386363636363636\n",
      "2      306   0.6666666666666666\n",
      "4      304                  0.6\n"
     ]
    }
   ],
   "source": [
    "# Read in accuracies for each window size from Accuracies_for_Window_Size_Variations.csv file and sort by accuracy to determine best window size\n",
    "import csv\n",
    "pd.set_option('display.max_columns', None)  # or 1000\n",
    "pd.set_option('display.max_rows', None)  # or 1000\n",
    "mylist = pd.DataFrame(columns=['Window', 'Accuracy'])\n",
    "with open('RF_Accuracies_for_Window_Size_Variations_2.csv', 'r') as csvfile:\n",
    "    for i,row in enumerate(csv.reader(csvfile, delimiter='\\n')):\n",
    "        mylist.loc[i,'Window'] = row[0].split(':')[0]\n",
    "        mylist.loc[i,'Accuracy'] = row[0].split(':')[1]\n",
    "print(mylist.sort_values('Accuracy', 0,ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate Over All Look Ahead Values from 1 to window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Predictions:  18\n",
      "Window Size:  291\n",
      "\n",
      "\n",
      "look ahead:  1\n",
      "# iterations:  18\n",
      "accuracies:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
      "computations:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "avg accuracy:  0.8888888888888892\n",
      "\n",
      "\n",
      "look ahead:  2\n",
      "# iterations:  9\n",
      "accuracies:  [1.  1.  1.  1.  1.  1.  0.5 0.5 1. ]\n",
      "computations:  [2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      "avg accuracy:  0.8888888888888891\n",
      "\n",
      "\n",
      "look ahead:  3\n",
      "# iterations:  6\n",
      "accuracies:  [1.         1.         1.         0.66666667 0.66666667 1.        ]\n",
      "computations:  [3. 3. 3. 3. 3. 3.]\n",
      "avg accuracy:  0.888888888888889\n",
      "\n",
      "\n",
      "look ahead:  4\n",
      "# iterations:  5\n",
      "accuracies:  [0.5  1.   1.   0.75 1.  ]\n",
      "computations:  [4. 4. 4. 4. 2.]\n",
      "avg accuracy:  0.8333333333333333\n",
      "\n",
      "\n",
      "look ahead:  5\n",
      "# iterations:  4\n",
      "accuracies:  [1.  1.  0.8 1. ]\n",
      "computations:  [5. 5. 5. 3.]\n",
      "avg accuracy:  0.9444444444444444\n",
      "\n",
      "\n",
      "look ahead:  6\n",
      "# iterations:  3\n",
      "accuracies:  [1.         1.         0.83333333]\n",
      "computations:  [6. 6. 6.]\n",
      "avg accuracy:  0.9444444444444444\n",
      "\n",
      "\n",
      "look ahead:  7\n",
      "# iterations:  3\n",
      "accuracies:  [1.         0.85714286 1.        ]\n",
      "computations:  [7. 7. 4.]\n",
      "avg accuracy:  0.9444444444444444\n",
      "\n",
      "\n",
      "look ahead:  8\n",
      "# iterations:  3\n",
      "accuracies:  [1.    0.875 1.   ]\n",
      "computations:  [8. 8. 2.]\n",
      "avg accuracy:  0.9444444444444444\n",
      "\n",
      "\n",
      "look ahead:  9\n",
      "# iterations:  2\n",
      "accuracies:  [1.         0.88888889]\n",
      "computations:  [9. 9.]\n",
      "avg accuracy:  0.9444444444444444\n",
      "\n",
      "\n",
      "look ahead:  10\n",
      "# iterations:  2\n",
      "accuracies:  [1.    0.875]\n",
      "computations:  [10.  8.]\n",
      "avg accuracy:  0.9444444444444444\n",
      "\n",
      "\n",
      "look ahead:  11\n",
      "# iterations:  2\n",
      "accuracies:  [1.         0.85714286]\n",
      "computations:  [11.  7.]\n",
      "avg accuracy:  0.9444444444444444\n",
      "\n",
      "\n",
      "look ahead:  12\n",
      "# iterations:  2\n",
      "accuracies:  [1.         0.83333333]\n",
      "computations:  [12.  6.]\n",
      "avg accuracy:  0.9444444444444444\n",
      "\n",
      "\n",
      "look ahead:  13\n",
      "# iterations:  2\n",
      "accuracies:  [0.92307692 0.8       ]\n",
      "computations:  [13.  5.]\n",
      "avg accuracy:  0.888888888888889\n",
      "\n",
      "\n",
      "look ahead:  14\n",
      "# iterations:  2\n",
      "accuracies:  [0.85714286 1.        ]\n",
      "computations:  [14.  4.]\n",
      "avg accuracy:  0.8888888888888888\n",
      "\n",
      "\n",
      "look ahead:  15\n",
      "# iterations:  2\n",
      "accuracies:  [0.93333333 1.        ]\n",
      "computations:  [15.  3.]\n",
      "avg accuracy:  0.9444444444444444\n",
      "\n",
      "\n",
      "look ahead:  16\n",
      "# iterations:  2\n",
      "accuracies:  [0.875 1.   ]\n",
      "computations:  [16.  2.]\n",
      "avg accuracy:  0.8888888888888888\n",
      "\n",
      "\n",
      "look ahead:  17\n",
      "# iterations:  2\n",
      "accuracies:  [0.88235294 1.        ]\n",
      "computations:  [17.  1.]\n",
      "avg accuracy:  0.8888888888888888\n",
      "\n",
      "\n",
      "look ahead:  18\n",
      "# iterations:  1\n",
      "accuracies:  [0.94444444]\n",
      "computations:  [18.]\n",
      "avg accuracy:  0.9444444444444444\n",
      "Averages:      Avg Accuracy\n",
      "0       0.888889\n",
      "1       0.888889\n",
      "2       0.888889\n",
      "3       0.833333\n",
      "4       0.944444\n",
      "5       0.944444\n",
      "6       0.944444\n",
      "7       0.944444\n",
      "8       0.944444\n",
      "9       0.944444\n",
      "10      0.944444\n",
      "11      0.944444\n",
      "12      0.888889\n",
      "13      0.888889\n",
      "14      0.944444\n",
      "15      0.888889\n",
      "16      0.888889\n",
      "17      0.944444\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import statistics as stats\n",
    "# Run RF with optimal window size of and iterate over all look ahead values\n",
    "\n",
    "window_size = 18 # num of preds, window_size = 309-window_size param = 291 (before, 91 = window size)\n",
    "print('# Predictions: ', window_size)\n",
    "print('Window Size: ', 309-window_size)\n",
    "average_accuracies = np.zeros(window_size)\n",
    "\n",
    "for m in range(1,window_size+1):\n",
    "    # train set\n",
    "    X_train = X.iloc[window_size:]\n",
    "    y_train = y.iloc[window_size:]\n",
    "    #print('train labels: ', y_train)\n",
    "    # test set\n",
    "    X_test = X.iloc[:window_size]\n",
    "    y_test = y.iloc[:window_size]\n",
    "    #print('test labels: ', y_test)\n",
    "    \n",
    "    #look_ahead = 7\n",
    "    look_ahead=m\n",
    "    print(\"\\n\\nlook ahead: \", m)\n",
    "    print(\"# iterations: \", math.ceil(y_test.shape[0]/look_ahead))\n",
    "    \n",
    "    #actuals = pd.DataFrame(y_test)\n",
    "    #actuals = actuals.rename(columns={'AvgHR_bin':'Actuals'})\n",
    "    accuracies = np.zeros(math.ceil(y_test.shape[0]/look_ahead))\n",
    "    computations = np.zeros(math.ceil(y_test.shape[0]/look_ahead))\n",
    "    # print('actuals: \\n', actuals)\n",
    "    # preds = np.zeros((math.ceil(y_test.shape[0]/look_ahead),look_ahead))\n",
    "    # actuals_look_ahead = np.zeros((math.ceil(y_test.shape[0]/look_ahead),look_ahead))\n",
    "    # print('X_test: \\n', X_test)\n",
    "    for i in range(0,math.ceil(y_test.shape[0]/look_ahead)):\n",
    "        preds = np.zeros(min(look_ahead,y_test.shape[0]))\n",
    "        computations[i] = min(look_ahead, y_test.shape[0])\n",
    "        actuals_look_ahead = np.zeros(min(look_ahead,y_test.shape[0]))\n",
    "#         print('Iteration: ', i)\n",
    "#         print('X train shape: ', X_train.shape[0])\n",
    "#         print('X test shape: ', y_test.shape[0])\n",
    "        RF.fit(X_train, y_train.values.ravel())\n",
    "        # Predict test set\n",
    "        for j in range(1,min(look_ahead+1,y_test.shape[0]+1)):\n",
    "            y_pred = RF.predict(np.array(X_test.iloc[-j]).reshape(1,-1))\n",
    "#             print('actual: ',y_test.iloc[-j], '\\n pred: ',y_pred, '\\n')\n",
    "            preds[j-1] = y_pred\n",
    "            actuals_look_ahead[j-1] = y_test.iloc[-j]\n",
    "        # print('Accuracy of logistic regression classifier on test set {}: {:.2f}'.format(i, logreg.score(X_test, y_test)))\n",
    "        #print('test instance: ', X_test.iloc[-1])\n",
    "        # X_train = pd.concat([X_test.iloc[0], X_train]).reset_index(drop = True)\n",
    "        # print('new X train: ', X_train.head())\n",
    "\n",
    "        #print(\"X test -1: \", X_test.iloc[-1])\n",
    "        X_test_inst = pd.DataFrame(columns=[\"Distance (km)\",\"Avg Pace (/km)\", \"HRSS\", \"Elevation Gain (m)\"])\n",
    "        y_test_inst = pd.DataFrame(columns=[\"AvgHR_bin\"])\n",
    "        for k in range(1,min(look_ahead+1,y_test.shape[0]+1)):\n",
    "            X_test_inst = X_test_inst.append(X_test.iloc[-k])\n",
    "            y_test_inst = y_test_inst.append(y_test.iloc[-k])        \n",
    "        #print(\"X test inst: \", X_test_inst)\n",
    "\n",
    "        X_train = X_train.drop(X_train.index[-1]).reset_index(drop=True)\n",
    "        X_train = pd.concat([X_test_inst,X_train]).reset_index(drop = True)\n",
    "        #print(\"X train: \\n\", X_train)\n",
    "\n",
    "        y_train = y_train.drop(y_train.index[-1])\n",
    "        y_train = pd.concat([y_test_inst,y_train]).reset_index(drop=True)\n",
    "        #print(\"y train \\n\", y_train)\n",
    "\n",
    "        X_test = X_test.drop(X_test_inst.index.values)\n",
    "        X_test = X_test.reset_index(drop=True)\n",
    "        #print(\"X test: \\n\", X_test)\n",
    "\n",
    "        y_test = y_test.drop(y_test_inst.index.values)\n",
    "        y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "        preds_act_df = pd.DataFrame(preds)\n",
    "        act_df = pd.DataFrame(actuals_look_ahead)\n",
    "#         print(\"predictions: \", preds)\n",
    "#         print(\"actuals: \", actuals_look_ahead)\n",
    "        #preds_act_df = preds_act_df.join(act_df)\n",
    "        #print('actuals and preds: \\n', preds_act_df)\n",
    "        accuracy = metrics.accuracy_score(preds,actuals_look_ahead)\n",
    "    \n",
    "        accuracies[i] = accuracy\n",
    "        # print('accuracy for window size {}: {}'.format(window_size, accuracy))\n",
    "    \n",
    "    print('accuracies: ', accuracies)\n",
    "    print('computations: ', computations)\n",
    "    h=len(accuracies)\n",
    "    avg_accuracy = 0\n",
    "    for g in range(len(accuracies)):\n",
    "        avg_accuracy += (computations[g]/window_size)*accuracies[g] \n",
    "    # avg_accuracy = stats.mean(accuracies)\n",
    "    print('avg accuracy: ', avg_accuracy)\n",
    "    average_accuracies[m-1] = avg_accuracy\n",
    "\n",
    "avg_acc_df = pd.DataFrame(data=average_accuracies, columns=['Avg Accuracy'])\n",
    "print('Averages: ', avg_acc_df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Look Ahead             Accuracy\n",
      "0           1   0.9444444444444448\n",
      "1           2   0.9444444444444446\n",
      "8           9   0.9444444444444444\n",
      "16         17   0.9444444444444444\n",
      "15         16   0.9444444444444444\n",
      "13         14   0.9444444444444444\n",
      "12         13   0.9444444444444444\n",
      "11         12   0.9444444444444444\n",
      "9          10   0.9444444444444444\n",
      "7           8   0.9444444444444444\n",
      "6           7   0.9444444444444444\n",
      "5           6   0.9444444444444444\n",
      "4           5   0.9444444444444444\n",
      "3           4   0.9444444444444444\n",
      "17         18   0.9444444444444444\n",
      "2           3   0.9444444444444443\n",
      "10         11   0.8888888888888888\n",
      "14         15   0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "# Output accuracies for each look ahead value to RF_Accuracies_for_Look_Ahead_Variations.csv file \n",
    "with open('RF_Accuracies_for_Look_Ahead_Variations_2.csv', 'w') as f:\n",
    "    #f.write(\"Look Ahead : Accuracy \\n\")\n",
    "    for i in range(0,len(average_accuracies)):\n",
    "        f.write(str(i+1) + ': ' + str(average_accuracies[i]))\n",
    "        f.write('\\n')\n",
    "# Read in accuracies for each window size from Accuracies_for_Window_Size_Variations.csv file and sort by accuracy to determine best window size\n",
    "import csv\n",
    "pd.set_option('display.max_columns', None)  # or 1000\n",
    "pd.set_option('display.max_rows', None)  # or 1000\n",
    "mylist = pd.DataFrame(columns=['Look Ahead', 'Accuracy'])\n",
    "with open('RF_Accuracies_for_Look_Ahead_Variations_2.csv', 'r') as csvfile:\n",
    "    for i,row in enumerate(csv.reader(csvfile, delimiter='\\n')):\n",
    "        mylist.loc[i,'Look Ahead'] = row[0].split(':')[0]\n",
    "        mylist.loc[i,'Accuracy'] = row[0].split(':')[1]\n",
    "print(mylist.sort_values('Accuracy', 0,ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Implementation (window size = 291, look ahead = 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Predictions:  18\n",
      "Window Size:  291\n",
      "train labels:       AvgHR_bin\n",
      "18         0.0\n",
      "19         1.0\n",
      "20         1.0\n",
      "21         0.0\n",
      "22         0.0\n",
      "23         0.0\n",
      "24         0.0\n",
      "25         0.0\n",
      "26         0.0\n",
      "27         0.0\n",
      "28         0.0\n",
      "29         0.0\n",
      "30         0.0\n",
      "31         0.0\n",
      "32         0.0\n",
      "33         0.0\n",
      "34         0.0\n",
      "35         0.0\n",
      "36         0.0\n",
      "37         0.0\n",
      "38         0.0\n",
      "39         0.0\n",
      "40         0.0\n",
      "41         0.0\n",
      "42         0.0\n",
      "43         0.0\n",
      "44         0.0\n",
      "45         0.0\n",
      "46         0.0\n",
      "47         0.0\n",
      "48         0.0\n",
      "49         0.0\n",
      "50         0.0\n",
      "51         0.0\n",
      "52         1.0\n",
      "53         1.0\n",
      "54         0.0\n",
      "55         0.0\n",
      "56         0.0\n",
      "57         0.0\n",
      "58         0.0\n",
      "59         0.0\n",
      "60         0.0\n",
      "61         0.0\n",
      "62         0.0\n",
      "63         0.0\n",
      "64         0.0\n",
      "65         1.0\n",
      "66         0.0\n",
      "67         0.0\n",
      "68         1.0\n",
      "69         0.0\n",
      "70         0.0\n",
      "71         1.0\n",
      "72         1.0\n",
      "73         1.0\n",
      "74         0.0\n",
      "75         0.0\n",
      "76         1.0\n",
      "77         0.0\n",
      "78         1.0\n",
      "79         0.0\n",
      "80         1.0\n",
      "81         0.0\n",
      "82         0.0\n",
      "83         0.0\n",
      "84         1.0\n",
      "85         1.0\n",
      "86         1.0\n",
      "87         1.0\n",
      "88         1.0\n",
      "89         0.0\n",
      "90         1.0\n",
      "91         1.0\n",
      "92         1.0\n",
      "93         1.0\n",
      "94         1.0\n",
      "95         1.0\n",
      "96         1.0\n",
      "97         0.0\n",
      "98         1.0\n",
      "99         1.0\n",
      "100        1.0\n",
      "101        1.0\n",
      "102        1.0\n",
      "103        1.0\n",
      "104        0.0\n",
      "105        1.0\n",
      "106        0.0\n",
      "107        0.0\n",
      "108        1.0\n",
      "109        1.0\n",
      "110        1.0\n",
      "111        1.0\n",
      "112        1.0\n",
      "113        1.0\n",
      "114        1.0\n",
      "115        1.0\n",
      "116        1.0\n",
      "117        1.0\n",
      "118        1.0\n",
      "119        1.0\n",
      "120        1.0\n",
      "121        1.0\n",
      "122        1.0\n",
      "123        0.0\n",
      "124        0.0\n",
      "125        0.0\n",
      "126        1.0\n",
      "127        0.0\n",
      "128        1.0\n",
      "129        1.0\n",
      "130        1.0\n",
      "131        0.0\n",
      "132        1.0\n",
      "133        0.0\n",
      "134        1.0\n",
      "135        1.0\n",
      "136        0.0\n",
      "137        1.0\n",
      "138        1.0\n",
      "139        1.0\n",
      "140        1.0\n",
      "141        1.0\n",
      "142        0.0\n",
      "143        1.0\n",
      "144        1.0\n",
      "145        0.0\n",
      "146        1.0\n",
      "147        1.0\n",
      "148        1.0\n",
      "149        1.0\n",
      "150        1.0\n",
      "151        0.0\n",
      "152        0.0\n",
      "153        1.0\n",
      "154        0.0\n",
      "155        1.0\n",
      "156        1.0\n",
      "157        1.0\n",
      "158        0.0\n",
      "159        0.0\n",
      "160        1.0\n",
      "161        1.0\n",
      "162        1.0\n",
      "163        0.0\n",
      "164        1.0\n",
      "165        1.0\n",
      "166        0.0\n",
      "167        1.0\n",
      "168        1.0\n",
      "169        0.0\n",
      "170        0.0\n",
      "171        1.0\n",
      "172        1.0\n",
      "173        1.0\n",
      "174        1.0\n",
      "175        1.0\n",
      "176        1.0\n",
      "177        0.0\n",
      "178        0.0\n",
      "179        0.0\n",
      "180        1.0\n",
      "181        1.0\n",
      "182        0.0\n",
      "183        1.0\n",
      "184        1.0\n",
      "185        0.0\n",
      "186        1.0\n",
      "187        1.0\n",
      "188        1.0\n",
      "189        0.0\n",
      "190        1.0\n",
      "191        1.0\n",
      "192        0.0\n",
      "193        1.0\n",
      "194        1.0\n",
      "195        1.0\n",
      "196        1.0\n",
      "197        0.0\n",
      "198        1.0\n",
      "199        0.0\n",
      "200        0.0\n",
      "201        1.0\n",
      "202        0.0\n",
      "203        1.0\n",
      "204        1.0\n",
      "205        0.0\n",
      "206        1.0\n",
      "207        1.0\n",
      "208        1.0\n",
      "209        0.0\n",
      "210        0.0\n",
      "211        1.0\n",
      "212        1.0\n",
      "213        1.0\n",
      "214        1.0\n",
      "215        0.0\n",
      "216        1.0\n",
      "217        1.0\n",
      "218        1.0\n",
      "219        1.0\n",
      "220        1.0\n",
      "221        1.0\n",
      "222        1.0\n",
      "223        1.0\n",
      "224        1.0\n",
      "225        1.0\n",
      "226        0.0\n",
      "227        1.0\n",
      "228        1.0\n",
      "229        1.0\n",
      "230        1.0\n",
      "231        0.0\n",
      "232        0.0\n",
      "233        0.0\n",
      "234        1.0\n",
      "235        1.0\n",
      "236        1.0\n",
      "237        1.0\n",
      "238        1.0\n",
      "239        1.0\n",
      "240        1.0\n",
      "241        1.0\n",
      "242        0.0\n",
      "243        1.0\n",
      "244        0.0\n",
      "245        0.0\n",
      "246        1.0\n",
      "247        1.0\n",
      "248        0.0\n",
      "249        0.0\n",
      "250        0.0\n",
      "251        1.0\n",
      "252        1.0\n",
      "253        1.0\n",
      "254        0.0\n",
      "255        0.0\n",
      "256        0.0\n",
      "257        1.0\n",
      "258        1.0\n",
      "259        0.0\n",
      "260        1.0\n",
      "261        1.0\n",
      "262        0.0\n",
      "263        1.0\n",
      "264        0.0\n",
      "265        1.0\n",
      "266        0.0\n",
      "267        0.0\n",
      "268        0.0\n",
      "269        0.0\n",
      "270        1.0\n",
      "271        0.0\n",
      "272        0.0\n",
      "273        0.0\n",
      "274        0.0\n",
      "275        0.0\n",
      "276        1.0\n",
      "277        0.0\n",
      "278        1.0\n",
      "279        0.0\n",
      "280        0.0\n",
      "281        0.0\n",
      "282        0.0\n",
      "283        1.0\n",
      "284        1.0\n",
      "285        1.0\n",
      "286        0.0\n",
      "287        1.0\n",
      "288        0.0\n",
      "289        0.0\n",
      "290        0.0\n",
      "291        1.0\n",
      "292        1.0\n",
      "293        0.0\n",
      "294        0.0\n",
      "295        1.0\n",
      "296        0.0\n",
      "297        0.0\n",
      "298        1.0\n",
      "299        0.0\n",
      "300        0.0\n",
      "301        0.0\n",
      "302        0.0\n",
      "303        0.0\n",
      "304        0.0\n",
      "305        0.0\n",
      "306        0.0\n",
      "307        0.0\n",
      "308        0.0\n",
      "test labels:      AvgHR_bin\n",
      "0         1.0\n",
      "1         1.0\n",
      "2         1.0\n",
      "3         1.0\n",
      "4         1.0\n",
      "5         1.0\n",
      "6         1.0\n",
      "7         0.0\n",
      "8         0.0\n",
      "9         0.0\n",
      "10        1.0\n",
      "11        1.0\n",
      "12        1.0\n",
      "13        0.0\n",
      "14        1.0\n",
      "15        0.0\n",
      "16        0.0\n",
      "17        1.0\n",
      "Iteration:  0\n",
      "X train shape:  291\n",
      "X test shape:  18\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 17, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 16, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 15, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 14, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 13, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 12, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 11, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 10, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 9, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 8, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 7, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 6, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 5, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 4, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 3, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 2, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 1, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 0, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "X test inst:      Distance (km)  Avg Pace (/km)   HRSS  Elevation Gain (m)\n",
      "17           47.5           110.0  132.0               406.0\n",
      "16           32.5           119.0   66.0               282.0\n",
      "15           45.9           148.0   79.0               447.0\n",
      "14           30.3           127.0   92.0               550.0\n",
      "13           13.2           301.0   74.0               184.0\n",
      "12           52.0           116.0  152.0               560.0\n",
      "11          100.9           130.0  337.0              1395.0\n",
      "10           52.6           122.0  172.0               535.0\n",
      "9            47.2           116.0  106.0               447.0\n",
      "8            55.4           114.0  116.0               502.0\n",
      "7            51.4           128.0   90.0               576.0\n",
      "6            81.0           129.0  225.0              1096.0\n",
      "5            61.4           122.0  146.0               692.0\n",
      "4            41.1           120.0   96.0               454.0\n",
      "3            45.6           114.0  119.0               447.0\n",
      "2            35.2           116.0   87.0               314.0\n",
      "1            80.1           125.0  217.0               890.0\n",
      "0            62.5           113.0  144.0               589.0\n",
      "predictions:  [1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
      "actuals:  [1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "accuracy for window size 291 with look ahead value 18: 0.9444444444444444\n",
      "roc for window size 291 with look ahead value 18: 0.9722222222222223\n",
      "accuracies:  [0.94444444]\n",
      "avg accuracy:  0.9444444444444444\n",
      "roc values:  [0.97222222]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import math\n",
    "import statistics as stats\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Create the model with 100 trees\n",
    "RF = RandomForestClassifier(n_estimators=100, \n",
    "                               bootstrap = True,\n",
    "                               max_features = 'sqrt')\n",
    "window_size = 18 # num of preds, window_size = 309-window_size param = 281\n",
    "print('# Predictions: ', window_size)\n",
    "print('Window Size: ', 309-window_size)\n",
    "# train set\n",
    "X_train = X.iloc[window_size:]\n",
    "y_train = y.iloc[window_size:]\n",
    "print('train labels: ', y_train)\n",
    "# test set\n",
    "X_test = X.iloc[:window_size]\n",
    "y_test = y.iloc[:window_size]\n",
    "print('test labels: ', y_test)\n",
    "\n",
    "look_ahead = 18\n",
    "predictions = []\n",
    "actuals = pd.DataFrame(y_test)\n",
    "accuracies = np.zeros(math.ceil(y_test.shape[0]/look_ahead))\n",
    "# print('X_test: \\n', X_test)\n",
    "roc_values = np.zeros(math.ceil(y_test.shape[0]/look_ahead))\n",
    "\n",
    "for i in range(0,math.ceil(y_test.shape[0]/look_ahead)):\n",
    "    preds = np.zeros(min(look_ahead,y_test.shape[0]))\n",
    "    rf_probs = np.zeros(min(look_ahead,y_test.shape[0]))\n",
    "    actuals_look_ahead = np.zeros(min(look_ahead,y_test.shape[0]))\n",
    "    print('Iteration: ', i)\n",
    "    print('X train shape: ', X_train.shape[0])\n",
    "    print('X test shape: ', y_test.shape[0])\n",
    "    RF.fit(X_train, y_train.values.ravel())\n",
    "    # Predict test set\n",
    "    for j in range(1,min(look_ahead+1,y_test.shape[0]+1)):\n",
    "        y_pred = RF.predict(np.array(X_test.iloc[-j]).reshape(1,-1))\n",
    "        print('actual: ',y_test.iloc[-j], '\\n pred: ',y_pred, '\\n')\n",
    "        preds[j-1] = y_pred\n",
    "        predictions.append(y_pred)\n",
    "        actuals_look_ahead[j-1] = y_test.iloc[-j]\n",
    "        rf_probs[j-1] = RF.predict_proba(np.array(X_test.iloc[-j]).reshape(1,-1))[:, 1]\n",
    "    # print('Accuracy of logistic regression classifier on test set {}: {:.2f}'.format(i, logreg.score(X_test, y_test)))\n",
    "    #print('test instance: ', X_test.iloc[-1])\n",
    "    # X_train = pd.concat([X_test.iloc[0], X_train]).reset_index(drop = True)\n",
    "    # print('new X train: ', X_train.head())\n",
    "    \n",
    "    #print(\"X test -1: \", X_test.iloc[-1])\n",
    "    X_test_inst = pd.DataFrame(columns=[\"Distance (km)\",\"Avg Pace (/km)\", \"HRSS\", \"Elevation Gain (m)\"])\n",
    "    y_test_inst = pd.DataFrame(columns=[\"AvgHR_bin\"])\n",
    "    for k in range(1,min(look_ahead+1,y_test.shape[0]+1)):\n",
    "        X_test_inst = X_test_inst.append(X_test.iloc[-k])\n",
    "        y_test_inst = y_test_inst.append(y_test.iloc[-k])        \n",
    "    print(\"X test inst: \", X_test_inst)\n",
    "\n",
    "    X_train = X_train.drop(X_train.index[-1]).reset_index(drop=True)\n",
    "    X_train = pd.concat([X_test_inst,X_train]).reset_index(drop = True)\n",
    "    #print(\"X train: \\n\", X_train)\n",
    "\n",
    "    y_train = y_train.drop(y_train.index[-1])\n",
    "    y_train = pd.concat([y_test_inst,y_train]).reset_index(drop=True)\n",
    "    #print(\"y train \\n\", y_train)\n",
    "    \n",
    "    X_test = X_test.drop(X_test_inst.index.values)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    #print(\"X test: \\n\", X_test)\n",
    "\n",
    "    y_test = y_test.drop(y_test_inst.index.values)\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "    \n",
    "    preds_act_df = pd.DataFrame(preds)\n",
    "    act_df = pd.DataFrame(actuals_look_ahead)\n",
    "    print(\"predictions: \", preds)\n",
    "    print(\"actuals: \", actuals_look_ahead)\n",
    "    #preds_act_df = preds_act_df.join(act_df)\n",
    "    #print('actuals and preds: \\n', preds_act_df)\n",
    "    accuracy = metrics.accuracy_score(preds,actuals_look_ahead)\n",
    "    accuracies[i] = accuracy\n",
    "    print('accuracy for window size {} with look ahead value {}: {}'.format(309-window_size, look_ahead, accuracy))\n",
    "    roc_value = roc_auc_score(actuals_look_ahead, rf_probs)\n",
    "    print('roc for window size {} with look ahead value {}: {}'.format(309-window_size, look_ahead, roc_value))\n",
    "    roc_values[i] = roc_value\n",
    "    \n",
    "print('accuracies: ', accuracies)\n",
    "print('avg accuracy: ', stats.mean(accuracies))\n",
    "print('roc values: ', roc_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5  1]\n",
      " [ 1 11]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix1 = confusion_matrix(actuals_look_ahead, preds)\n",
    "print(confusion_matrix1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      1.00      0.92         6\n",
      "         1.0       1.00      0.92      0.96        12\n",
      "\n",
      "    accuracy                           0.94        18\n",
      "   macro avg       0.93      0.96      0.94        18\n",
      "weighted avg       0.95      0.94      0.95        18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Precision, recall, f1-score, support (# of test instances per class)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(actuals_look_ahead, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest performs slightly better than LR when looking ahead 1 value for their respective window sizes of 291 and 181 and using the 'simple moving average' method of dropping the oldest record and adding the newest test record for the next prediction (0.944 accuracy vs. 0.938 for LR). When using looking ahead, RF scores 0.944 accuracy for look ahead of 18 (vs. 0.922 for LR for look ahead of 118). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output predictions and actuals for final window size to LR.csv file \n",
    "with open('RF.csv', 'w') as f:\n",
    "    f.write(\"Prediction, Actual \\n\")\n",
    "    for i in range(0,window_size):\n",
    "        f.write(str(predictions[i][0]) + ', ' + str(actuals.iloc[::-1].reset_index(drop = True).iloc[i][0]))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Distance (km)</th>\n",
       "      <th>Avg Pace (/km)</th>\n",
       "      <th>HRSS</th>\n",
       "      <th>Elevation Gain (m)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>309.000000</td>\n",
       "      <td>309.000000</td>\n",
       "      <td>309.000000</td>\n",
       "      <td>309.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>52.237540</td>\n",
       "      <td>150.624595</td>\n",
       "      <td>152.022654</td>\n",
       "      <td>888.765372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>27.117491</td>\n",
       "      <td>42.469105</td>\n",
       "      <td>82.021593</td>\n",
       "      <td>650.796847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>32.900000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>492.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>50.100000</td>\n",
       "      <td>151.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>715.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>60.400000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>1128.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>170.600000</td>\n",
       "      <td>477.000000</td>\n",
       "      <td>471.000000</td>\n",
       "      <td>3617.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Distance (km)  Avg Pace (/km)        HRSS  Elevation Gain (m)\n",
       "count     309.000000      309.000000  309.000000          309.000000\n",
       "mean       52.237540      150.624595  152.022654          888.765372\n",
       "std        27.117491       42.469105   82.021593          650.796847\n",
       "min         1.400000       86.000000    3.000000            0.000000\n",
       "25%        32.900000      119.000000   98.000000          492.000000\n",
       "50%        50.100000      151.000000  128.000000          715.000000\n",
       "75%        60.400000      168.000000  187.000000         1128.900000\n",
       "max       170.600000      477.000000  471.000000         3617.000000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Only include Ride and VirtualRide types\n",
    "types_ride = ['Ride']\n",
    "types_virtual = ['VirtualRide']\n",
    "df_virtual = df[df.Type.isin(types_virtual)]\n",
    "df_ride = df[df.Type.isin(types_ride)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AvgHR</th>\n",
       "      <th>Date</th>\n",
       "      <th>Type</th>\n",
       "      <th>Distance (km)</th>\n",
       "      <th>Avg Pace (/km)</th>\n",
       "      <th>Calories</th>\n",
       "      <th>HRSS</th>\n",
       "      <th>Elevation Gain (m)</th>\n",
       "      <th>Time</th>\n",
       "      <th>AvgHR_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>162</td>\n",
       "      <td>2019-09-03</td>\n",
       "      <td>VirtualRide</td>\n",
       "      <td>30.3</td>\n",
       "      <td>127</td>\n",
       "      <td>729</td>\n",
       "      <td>92</td>\n",
       "      <td>550.0</td>\n",
       "      <td>17:27:57</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>186</td>\n",
       "      <td>2019-05-11</td>\n",
       "      <td>VirtualRide</td>\n",
       "      <td>8.1</td>\n",
       "      <td>134</td>\n",
       "      <td>475</td>\n",
       "      <td>43</td>\n",
       "      <td>230.6</td>\n",
       "      <td>16:30:28</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>169</td>\n",
       "      <td>2019-04-02</td>\n",
       "      <td>VirtualRide</td>\n",
       "      <td>34.7</td>\n",
       "      <td>123</td>\n",
       "      <td>1016</td>\n",
       "      <td>127</td>\n",
       "      <td>558.0</td>\n",
       "      <td>18:15:16</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>178</td>\n",
       "      <td>2019-02-11</td>\n",
       "      <td>VirtualRide</td>\n",
       "      <td>28.1</td>\n",
       "      <td>99</td>\n",
       "      <td>706</td>\n",
       "      <td>93</td>\n",
       "      <td>350.0</td>\n",
       "      <td>18:35:21</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>156</td>\n",
       "      <td>2019-02-08</td>\n",
       "      <td>VirtualRide</td>\n",
       "      <td>34.3</td>\n",
       "      <td>108</td>\n",
       "      <td>676</td>\n",
       "      <td>72</td>\n",
       "      <td>287.0</td>\n",
       "      <td>18:21:19</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>144</td>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>VirtualRide</td>\n",
       "      <td>25.1</td>\n",
       "      <td>146</td>\n",
       "      <td>518</td>\n",
       "      <td>52</td>\n",
       "      <td>500.0</td>\n",
       "      <td>17:38:16</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>190</td>\n",
       "      <td>2019-01-28</td>\n",
       "      <td>VirtualRide</td>\n",
       "      <td>29.9</td>\n",
       "      <td>98</td>\n",
       "      <td>639</td>\n",
       "      <td>113</td>\n",
       "      <td>322.0</td>\n",
       "      <td>18:31:23</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>166</td>\n",
       "      <td>2019-01-18</td>\n",
       "      <td>VirtualRide</td>\n",
       "      <td>34.9</td>\n",
       "      <td>103</td>\n",
       "      <td>660</td>\n",
       "      <td>91</td>\n",
       "      <td>294.0</td>\n",
       "      <td>18:33:46</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>191</td>\n",
       "      <td>2019-01-21</td>\n",
       "      <td>VirtualRide</td>\n",
       "      <td>27.7</td>\n",
       "      <td>104</td>\n",
       "      <td>628</td>\n",
       "      <td>122</td>\n",
       "      <td>324.0</td>\n",
       "      <td>18:30:07</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>156</td>\n",
       "      <td>2019-01-25</td>\n",
       "      <td>VirtualRide</td>\n",
       "      <td>41.1</td>\n",
       "      <td>111</td>\n",
       "      <td>776</td>\n",
       "      <td>94</td>\n",
       "      <td>391.0</td>\n",
       "      <td>18:39:47</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>135</td>\n",
       "      <td>2018-11-07</td>\n",
       "      <td>VirtualRide</td>\n",
       "      <td>29.3</td>\n",
       "      <td>123</td>\n",
       "      <td>681</td>\n",
       "      <td>46</td>\n",
       "      <td>500.0</td>\n",
       "      <td>18:40:51</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>159</td>\n",
       "      <td>2018-11-14</td>\n",
       "      <td>VirtualRide</td>\n",
       "      <td>39.0</td>\n",
       "      <td>110</td>\n",
       "      <td>783</td>\n",
       "      <td>94</td>\n",
       "      <td>387.0</td>\n",
       "      <td>18:42:44</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>164</td>\n",
       "      <td>2018-10-24</td>\n",
       "      <td>VirtualRide</td>\n",
       "      <td>30.0</td>\n",
       "      <td>122</td>\n",
       "      <td>721</td>\n",
       "      <td>91</td>\n",
       "      <td>548.0</td>\n",
       "      <td>18:41:01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>161</td>\n",
       "      <td>2018-10-17</td>\n",
       "      <td>VirtualRide</td>\n",
       "      <td>33.8</td>\n",
       "      <td>107</td>\n",
       "      <td>668</td>\n",
       "      <td>84</td>\n",
       "      <td>285.0</td>\n",
       "      <td>18:48:06</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     AvgHR        Date         Type  Distance (km)  Avg Pace (/km)  Calories  \\\n",
       "14     162  2019-09-03  VirtualRide           30.3             127       729   \n",
       "93     186  2019-05-11  VirtualRide            8.1             134       475   \n",
       "115    169  2019-04-02  VirtualRide           34.7             123      1016   \n",
       "139    178  2019-02-11  VirtualRide           28.1              99       706   \n",
       "141    156  2019-02-08  VirtualRide           34.3             108       676   \n",
       "142    144  2019-02-01  VirtualRide           25.1             146       518   \n",
       "146    190  2019-01-28  VirtualRide           29.9              98       639   \n",
       "148    166  2019-01-18  VirtualRide           34.9             103       660   \n",
       "149    191  2019-01-21  VirtualRide           27.7             104       628   \n",
       "150    156  2019-01-25  VirtualRide           41.1             111       776   \n",
       "163    135  2018-11-07  VirtualRide           29.3             123       681   \n",
       "164    159  2018-11-14  VirtualRide           39.0             110       783   \n",
       "172    164  2018-10-24  VirtualRide           30.0             122       721   \n",
       "173    161  2018-10-17  VirtualRide           33.8             107       668   \n",
       "\n",
       "     HRSS  Elevation Gain (m)      Time  AvgHR_bin  \n",
       "14     92               550.0  17:27:57        1.0  \n",
       "93     43               230.6  16:30:28        1.0  \n",
       "115   127               558.0  18:15:16        1.0  \n",
       "139    93               350.0  18:35:21        1.0  \n",
       "141    72               287.0  18:21:19        1.0  \n",
       "142    52               500.0  17:38:16        0.0  \n",
       "146   113               322.0  18:31:23        1.0  \n",
       "148    91               294.0  18:33:46        1.0  \n",
       "149   122               324.0  18:30:07        1.0  \n",
       "150    94               391.0  18:39:47        1.0  \n",
       "163    46               500.0  18:40:51        0.0  \n",
       "164    94               387.0  18:42:44        1.0  \n",
       "172    91               548.0  18:41:01        1.0  \n",
       "173    84               285.0  18:48:06        1.0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_virtual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AvgHR</th>\n",
       "      <th>Date</th>\n",
       "      <th>Type</th>\n",
       "      <th>Distance (km)</th>\n",
       "      <th>Avg Pace (/km)</th>\n",
       "      <th>Calories</th>\n",
       "      <th>HRSS</th>\n",
       "      <th>Elevation Gain (m)</th>\n",
       "      <th>Time</th>\n",
       "      <th>AvgHR_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>158</td>\n",
       "      <td>2019-09-22</td>\n",
       "      <td>Ride</td>\n",
       "      <td>62.5</td>\n",
       "      <td>113</td>\n",
       "      <td>1772</td>\n",
       "      <td>144</td>\n",
       "      <td>589.0</td>\n",
       "      <td>16:27:01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>2019-09-21</td>\n",
       "      <td>Ride</td>\n",
       "      <td>80.1</td>\n",
       "      <td>125</td>\n",
       "      <td>2432</td>\n",
       "      <td>217</td>\n",
       "      <td>890.0</td>\n",
       "      <td>11:57:23</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>159</td>\n",
       "      <td>2019-09-20</td>\n",
       "      <td>Ride</td>\n",
       "      <td>35.2</td>\n",
       "      <td>116</td>\n",
       "      <td>1029</td>\n",
       "      <td>87</td>\n",
       "      <td>314.0</td>\n",
       "      <td>17:55:47</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>161</td>\n",
       "      <td>2019-09-18</td>\n",
       "      <td>Ride</td>\n",
       "      <td>45.6</td>\n",
       "      <td>114</td>\n",
       "      <td>1326</td>\n",
       "      <td>119</td>\n",
       "      <td>447.0</td>\n",
       "      <td>17:41:48</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>155</td>\n",
       "      <td>2019-09-17</td>\n",
       "      <td>Ride</td>\n",
       "      <td>41.1</td>\n",
       "      <td>120</td>\n",
       "      <td>1156</td>\n",
       "      <td>96</td>\n",
       "      <td>454.0</td>\n",
       "      <td>16:47:31</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>149</td>\n",
       "      <td>2017-08-18</td>\n",
       "      <td>Ride</td>\n",
       "      <td>70.9</td>\n",
       "      <td>146</td>\n",
       "      <td>1817</td>\n",
       "      <td>175</td>\n",
       "      <td>1202.7</td>\n",
       "      <td>10:09:39</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>148</td>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>Ride</td>\n",
       "      <td>70.1</td>\n",
       "      <td>159</td>\n",
       "      <td>1827</td>\n",
       "      <td>191</td>\n",
       "      <td>1185.3</td>\n",
       "      <td>11:44:59</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>306</td>\n",
       "      <td>150</td>\n",
       "      <td>2017-08-14</td>\n",
       "      <td>Ride</td>\n",
       "      <td>53.7</td>\n",
       "      <td>163</td>\n",
       "      <td>1432</td>\n",
       "      <td>161</td>\n",
       "      <td>1015.8</td>\n",
       "      <td>13:41:10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>307</td>\n",
       "      <td>131</td>\n",
       "      <td>2017-08-13</td>\n",
       "      <td>Ride</td>\n",
       "      <td>40.5</td>\n",
       "      <td>161</td>\n",
       "      <td>941</td>\n",
       "      <td>70</td>\n",
       "      <td>350.4</td>\n",
       "      <td>13:14:20</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>308</td>\n",
       "      <td>151</td>\n",
       "      <td>2017-08-05</td>\n",
       "      <td>Ride</td>\n",
       "      <td>52.0</td>\n",
       "      <td>138</td>\n",
       "      <td>1332</td>\n",
       "      <td>126</td>\n",
       "      <td>811.4</td>\n",
       "      <td>09:32:39</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>295 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     AvgHR        Date  Type  Distance (km)  Avg Pace (/km)  Calories  HRSS  \\\n",
       "0      158  2019-09-22  Ride           62.5             113      1772   144   \n",
       "1      158  2019-09-21  Ride           80.1             125      2432   217   \n",
       "2      159  2019-09-20  Ride           35.2             116      1029    87   \n",
       "3      161  2019-09-18  Ride           45.6             114      1326   119   \n",
       "4      155  2019-09-17  Ride           41.1             120      1156    96   \n",
       "..     ...         ...   ...            ...             ...       ...   ...   \n",
       "304    149  2017-08-18  Ride           70.9             146      1817   175   \n",
       "305    148  2017-08-16  Ride           70.1             159      1827   191   \n",
       "306    150  2017-08-14  Ride           53.7             163      1432   161   \n",
       "307    131  2017-08-13  Ride           40.5             161       941    70   \n",
       "308    151  2017-08-05  Ride           52.0             138      1332   126   \n",
       "\n",
       "     Elevation Gain (m)      Time  AvgHR_bin  \n",
       "0                 589.0  16:27:01        1.0  \n",
       "1                 890.0  11:57:23        1.0  \n",
       "2                 314.0  17:55:47        1.0  \n",
       "3                 447.0  17:41:48        1.0  \n",
       "4                 454.0  16:47:31        1.0  \n",
       "..                  ...       ...        ...  \n",
       "304              1202.7  10:09:39        0.0  \n",
       "305              1185.3  11:44:59        0.0  \n",
       "306              1015.8  13:41:10        0.0  \n",
       "307               350.4  13:14:20        0.0  \n",
       "308               811.4  09:32:39        0.0  \n",
       "\n",
       "[295 rows x 10 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
