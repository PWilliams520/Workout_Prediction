{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference for building Logistic Regression model: https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please update the path to the activities.csv file from the repository\n",
    "df = pd.read_csv('activities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Moving Time</th>\n",
       "      <th>Distance (km)</th>\n",
       "      <th>Elevation Gain (m)</th>\n",
       "      <th>Avg Moving Speed (kph)</th>\n",
       "      <th>Avg Pace (/km)</th>\n",
       "      <th>Calories</th>\n",
       "      <th>Best 20min Speed (kph)</th>\n",
       "      <th>...</th>\n",
       "      <th>HRSS / h</th>\n",
       "      <th>Best 20min HR (bpm)</th>\n",
       "      <th>Cadence Avg Moving (rpm or spm)</th>\n",
       "      <th>Avg Watts (w)</th>\n",
       "      <th>Avg Watts / Kilograms (w/kg)</th>\n",
       "      <th>Best 20min Power (w)</th>\n",
       "      <th>Power Stress Score</th>\n",
       "      <th>Power Stress Score / h</th>\n",
       "      <th>Athlete Settings</th>\n",
       "      <th>Delete</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2019-09-22T16:27:01-04:00</td>\n",
       "      <td>Último día de verano</td>\n",
       "      <td>Ride</td>\n",
       "      <td>01:58:06</td>\n",
       "      <td>62.5</td>\n",
       "      <td>589.0</td>\n",
       "      <td>31.7</td>\n",
       "      <td>01:53</td>\n",
       "      <td>1772</td>\n",
       "      <td>34</td>\n",
       "      <td>...</td>\n",
       "      <td>73</td>\n",
       "      <td>163</td>\n",
       "      <td>95</td>\n",
       "      <td>179</td>\n",
       "      <td>2.56</td>\n",
       "      <td>201</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>MaxHr 190bpm. RestHr 65bpm. Weight 70kg.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2019-09-21T11:57:23-04:00</td>\n",
       "      <td>Dos loops</td>\n",
       "      <td>Ride</td>\n",
       "      <td>02:38:51</td>\n",
       "      <td>80.1</td>\n",
       "      <td>890.0</td>\n",
       "      <td>28.7</td>\n",
       "      <td>02:05</td>\n",
       "      <td>2432</td>\n",
       "      <td>36.7</td>\n",
       "      <td>...</td>\n",
       "      <td>77</td>\n",
       "      <td>173</td>\n",
       "      <td>89</td>\n",
       "      <td>174</td>\n",
       "      <td>2.49</td>\n",
       "      <td>225</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>MaxHr 190bpm. RestHr 65bpm. Weight 70kg.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2019-09-20T17:55:47-04:00</td>\n",
       "      <td>Con poco tiempo</td>\n",
       "      <td>Ride</td>\n",
       "      <td>01:07:52</td>\n",
       "      <td>35.2</td>\n",
       "      <td>314.0</td>\n",
       "      <td>30.9</td>\n",
       "      <td>01:56</td>\n",
       "      <td>1029</td>\n",
       "      <td>34.5</td>\n",
       "      <td>...</td>\n",
       "      <td>76</td>\n",
       "      <td>163</td>\n",
       "      <td>91</td>\n",
       "      <td>169</td>\n",
       "      <td>2.42</td>\n",
       "      <td>188</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>MaxHr 190bpm. RestHr 65bpm. Weight 70kg.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2019-09-19T23:45:00-04:00</td>\n",
       "      <td>Complimentary calisthenics</td>\n",
       "      <td>Workout</td>\n",
       "      <td>01:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>Weight 70kg.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2019-09-18T17:41:48-04:00</td>\n",
       "      <td>Afternoon Ride</td>\n",
       "      <td>Ride</td>\n",
       "      <td>01:26:05</td>\n",
       "      <td>45.6</td>\n",
       "      <td>447.0</td>\n",
       "      <td>31.4</td>\n",
       "      <td>01:54</td>\n",
       "      <td>1326</td>\n",
       "      <td>36</td>\n",
       "      <td>...</td>\n",
       "      <td>81</td>\n",
       "      <td>173</td>\n",
       "      <td>97</td>\n",
       "      <td>181</td>\n",
       "      <td>2.59</td>\n",
       "      <td>221</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>MaxHr 190bpm. RestHr 65bpm. Weight 70kg.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Date                        Name     Type Moving Time  \\\n",
       "0  2019-09-22T16:27:01-04:00        Último día de verano     Ride    01:58:06   \n",
       "1  2019-09-21T11:57:23-04:00                   Dos loops     Ride    02:38:51   \n",
       "2  2019-09-20T17:55:47-04:00             Con poco tiempo     Ride    01:07:52   \n",
       "3  2019-09-19T23:45:00-04:00  Complimentary calisthenics  Workout    01:00:00   \n",
       "4  2019-09-18T17:41:48-04:00              Afternoon Ride     Ride    01:26:05   \n",
       "\n",
       "   Distance (km)  Elevation Gain (m) Avg Moving Speed (kph) Avg Pace (/km)  \\\n",
       "0           62.5               589.0                   31.7          01:53   \n",
       "1           80.1               890.0                   28.7          02:05   \n",
       "2           35.2               314.0                   30.9          01:56   \n",
       "3            0.0                 0.0                      -              -   \n",
       "4           45.6               447.0                   31.4          01:54   \n",
       "\n",
       "  Calories Best 20min Speed (kph)  ... HRSS / h Best 20min HR (bpm)  \\\n",
       "0     1772                     34  ...       73                 163   \n",
       "1     2432                   36.7  ...       77                 173   \n",
       "2     1029                   34.5  ...       76                 163   \n",
       "3        -                      -  ...        -                   -   \n",
       "4     1326                     36  ...       81                 173   \n",
       "\n",
       "  Cadence Avg Moving (rpm or spm) Avg Watts (w) Avg Watts / Kilograms (w/kg)  \\\n",
       "0                              95           179                         2.56   \n",
       "1                              89           174                         2.49   \n",
       "2                              91           169                         2.42   \n",
       "3                               -             -                            -   \n",
       "4                              97           181                         2.59   \n",
       "\n",
       "  Best 20min Power (w) Power Stress Score Power Stress Score / h  \\\n",
       "0                  201                  -                      0   \n",
       "1                  225                  -                      0   \n",
       "2                  188                  -                      0   \n",
       "3                    -                  -                      -   \n",
       "4                  221                  -                      0   \n",
       "\n",
       "                           Athlete Settings Delete  \n",
       "0  MaxHr 190bpm. RestHr 65bpm. Weight 70kg.    NaN  \n",
       "1  MaxHr 190bpm. RestHr 65bpm. Weight 70kg.    NaN  \n",
       "2  MaxHr 190bpm. RestHr 65bpm. Weight 70kg.    NaN  \n",
       "3                              Weight 70kg.    NaN  \n",
       "4  MaxHr 190bpm. RestHr 65bpm. Weight 70kg.    NaN  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Distance (km)</th>\n",
       "      <th>Elevation Gain (m)</th>\n",
       "      <th>Delete</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>573.000000</td>\n",
       "      <td>573.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>35.888656</td>\n",
       "      <td>620.718325</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>29.855291</td>\n",
       "      <td>601.756726</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>9.300000</td>\n",
       "      <td>189.400000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>32.900000</td>\n",
       "      <td>526.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>51.900000</td>\n",
       "      <td>884.600000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>170.600000</td>\n",
       "      <td>3617.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Distance (km)  Elevation Gain (m)  Delete\n",
       "count     573.000000          573.000000     0.0\n",
       "mean       35.888656          620.718325     NaN\n",
       "std        29.855291          601.756726     NaN\n",
       "min         0.000000            0.000000     NaN\n",
       "25%         9.300000          189.400000     NaN\n",
       "50%        32.900000          526.000000     NaN\n",
       "75%        51.900000          884.600000     NaN\n",
       "max       170.600000         3617.000000     NaN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before feature pre-processing\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AvgHR</th>\n",
       "      <th>Date</th>\n",
       "      <th>Type</th>\n",
       "      <th>Distance (km)</th>\n",
       "      <th>Avg Pace (/km)</th>\n",
       "      <th>Calories</th>\n",
       "      <th>HRSS</th>\n",
       "      <th>Elevation Gain (m)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>158</td>\n",
       "      <td>2019-09-22T16:27:01-04:00</td>\n",
       "      <td>Ride</td>\n",
       "      <td>62.5</td>\n",
       "      <td>01:53</td>\n",
       "      <td>1772</td>\n",
       "      <td>144</td>\n",
       "      <td>589.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>2019-09-21T11:57:23-04:00</td>\n",
       "      <td>Ride</td>\n",
       "      <td>80.1</td>\n",
       "      <td>02:05</td>\n",
       "      <td>2432</td>\n",
       "      <td>217</td>\n",
       "      <td>890.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>159</td>\n",
       "      <td>2019-09-20T17:55:47-04:00</td>\n",
       "      <td>Ride</td>\n",
       "      <td>35.2</td>\n",
       "      <td>01:56</td>\n",
       "      <td>1029</td>\n",
       "      <td>87</td>\n",
       "      <td>314.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-</td>\n",
       "      <td>2019-09-19T23:45:00-04:00</td>\n",
       "      <td>Workout</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>161</td>\n",
       "      <td>2019-09-18T17:41:48-04:00</td>\n",
       "      <td>Ride</td>\n",
       "      <td>45.6</td>\n",
       "      <td>01:54</td>\n",
       "      <td>1326</td>\n",
       "      <td>119</td>\n",
       "      <td>447.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  AvgHR                       Date     Type  Distance (km) Avg Pace (/km)  \\\n",
       "0   158  2019-09-22T16:27:01-04:00     Ride           62.5          01:53   \n",
       "1   158  2019-09-21T11:57:23-04:00     Ride           80.1          02:05   \n",
       "2   159  2019-09-20T17:55:47-04:00     Ride           35.2          01:56   \n",
       "3     -  2019-09-19T23:45:00-04:00  Workout            0.0              -   \n",
       "4   161  2019-09-18T17:41:48-04:00     Ride           45.6          01:54   \n",
       "\n",
       "  Calories HRSS  Elevation Gain (m)  \n",
       "0     1772  144               589.0  \n",
       "1     2432  217               890.0  \n",
       "2     1029   87               314.0  \n",
       "3        -    -                 0.0  \n",
       "4     1326  119               447.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select features and rename columns\n",
    "df = df[['Avg HR (bpm)','Date','Type','Distance (km)','Avg Pace (/km)','Calories','HRSS','Elevation Gain (m)']]\n",
    "df = df.rename(columns={\"Avg HR (bpm)\": \"AvgHR\"})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AvgHR</th>\n",
       "      <th>Distance (km)</th>\n",
       "      <th>Avg Pace (/km)</th>\n",
       "      <th>Calories</th>\n",
       "      <th>HRSS</th>\n",
       "      <th>Elevation Gain (m)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>309.000000</td>\n",
       "      <td>309.000000</td>\n",
       "      <td>309.000000</td>\n",
       "      <td>309.000000</td>\n",
       "      <td>309.000000</td>\n",
       "      <td>309.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>155.032362</td>\n",
       "      <td>52.237540</td>\n",
       "      <td>150.624595</td>\n",
       "      <td>1573.423948</td>\n",
       "      <td>152.022654</td>\n",
       "      <td>888.765372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>12.505282</td>\n",
       "      <td>27.117491</td>\n",
       "      <td>42.469105</td>\n",
       "      <td>848.025054</td>\n",
       "      <td>82.021593</td>\n",
       "      <td>650.796847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>149.000000</td>\n",
       "      <td>32.900000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>979.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>492.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>50.100000</td>\n",
       "      <td>151.000000</td>\n",
       "      <td>1354.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>715.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>60.400000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>1827.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>1128.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>191.000000</td>\n",
       "      <td>170.600000</td>\n",
       "      <td>477.000000</td>\n",
       "      <td>5415.000000</td>\n",
       "      <td>471.000000</td>\n",
       "      <td>3617.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            AvgHR  Distance (km)  Avg Pace (/km)     Calories        HRSS  \\\n",
       "count  309.000000     309.000000      309.000000   309.000000  309.000000   \n",
       "mean   155.032362      52.237540      150.624595  1573.423948  152.022654   \n",
       "std     12.505282      27.117491       42.469105   848.025054   82.021593   \n",
       "min     84.000000       1.400000       86.000000    48.000000    3.000000   \n",
       "25%    149.000000      32.900000      119.000000   979.000000   98.000000   \n",
       "50%    155.000000      50.100000      151.000000  1354.000000  128.000000   \n",
       "75%    161.000000      60.400000      168.000000  1827.000000  187.000000   \n",
       "max    191.000000     170.600000      477.000000  5415.000000  471.000000   \n",
       "\n",
       "       Elevation Gain (m)  \n",
       "count          309.000000  \n",
       "mean           888.765372  \n",
       "std            650.796847  \n",
       "min              0.000000  \n",
       "25%            492.000000  \n",
       "50%            715.000000  \n",
       "75%           1128.900000  \n",
       "max           3617.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define features and pre-process data for final dataset\n",
    "from dateutil.parser import parse\n",
    "# Filter rows to include only those with AvgHR\n",
    "df = df[df.AvgHR != '-']\n",
    "# Only include Ride and VirtualRide types\n",
    "types = ['Ride', 'VirtualRide']\n",
    "df = df[df.Type.isin(types)]\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "# Convert features to numbers\n",
    "df[\"AvgHR\"] = pd.to_numeric(df[\"AvgHR\"])\n",
    "df[\"Calories\"] = pd.to_numeric(df[\"Calories\"])\n",
    "df[\"HRSS\"] = pd.to_numeric(df[\"HRSS\"])\n",
    "df.head()\n",
    "# Convert Avg Pace to seconds, parse Date and Time as separate columns\n",
    "for i in range(df.shape[0]):\n",
    "    #print('done')\n",
    "    #print(df.loc[i,'Avg Pace (/km)'])\n",
    "    (m, s) = str(df.loc[i,'Avg Pace (/km)']).split(':')\n",
    "    df.loc[i,'Avg Pace (/km)']= (int(m) * 60) + int(s)\n",
    "    dt = parse(df.loc[i,'Date'])\n",
    "    df.loc[i,'Date'] = dt.date()\n",
    "    df.loc[i,'Time'] = dt.time()\n",
    "# Convert Avg Pace to number\n",
    "df['Avg Pace (/km)'] = pd.to_numeric(df['Avg Pace (/km)'])\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AvgHR</th>\n",
       "      <th>Distance (km)</th>\n",
       "      <th>Avg Pace (/km)</th>\n",
       "      <th>Calories</th>\n",
       "      <th>HRSS</th>\n",
       "      <th>Elevation Gain (m)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Ride</td>\n",
       "      <td>154.535593</td>\n",
       "      <td>53.271525</td>\n",
       "      <td>152.298305</td>\n",
       "      <td>1615.294915</td>\n",
       "      <td>155.122034</td>\n",
       "      <td>912.209831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>VirtualRide</td>\n",
       "      <td>165.500000</td>\n",
       "      <td>30.450000</td>\n",
       "      <td>115.357143</td>\n",
       "      <td>691.142857</td>\n",
       "      <td>86.714286</td>\n",
       "      <td>394.757143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  AvgHR  Distance (km)  Avg Pace (/km)     Calories  \\\n",
       "Type                                                                  \n",
       "Ride         154.535593      53.271525      152.298305  1615.294915   \n",
       "VirtualRide  165.500000      30.450000      115.357143   691.142857   \n",
       "\n",
       "                   HRSS  Elevation Gain (m)  \n",
       "Type                                         \n",
       "Ride         155.122034          912.209831  \n",
       "VirtualRide   86.714286          394.757143  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['Type']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">AvgHR</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Distance (km)</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"2\" halign=\"left\">HRSS</th>\n",
       "      <th colspan=\"8\" halign=\"left\">Elevation Gain (m)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>...</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Ride</td>\n",
       "      <td>295.0</td>\n",
       "      <td>154.535593</td>\n",
       "      <td>12.103490</td>\n",
       "      <td>84.0</td>\n",
       "      <td>149.00</td>\n",
       "      <td>155.0</td>\n",
       "      <td>160.50</td>\n",
       "      <td>191.0</td>\n",
       "      <td>295.0</td>\n",
       "      <td>53.271525</td>\n",
       "      <td>...</td>\n",
       "      <td>189.5</td>\n",
       "      <td>471.0</td>\n",
       "      <td>295.0</td>\n",
       "      <td>912.209831</td>\n",
       "      <td>656.472151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>743.0</td>\n",
       "      <td>1131.85</td>\n",
       "      <td>3617.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>VirtualRide</td>\n",
       "      <td>14.0</td>\n",
       "      <td>165.500000</td>\n",
       "      <td>16.383622</td>\n",
       "      <td>135.0</td>\n",
       "      <td>156.75</td>\n",
       "      <td>163.0</td>\n",
       "      <td>175.75</td>\n",
       "      <td>191.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>30.450000</td>\n",
       "      <td>...</td>\n",
       "      <td>94.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>394.757143</td>\n",
       "      <td>114.192663</td>\n",
       "      <td>230.6</td>\n",
       "      <td>301.0</td>\n",
       "      <td>368.5</td>\n",
       "      <td>500.00</td>\n",
       "      <td>558.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             AvgHR                                                       \\\n",
       "             count        mean        std    min     25%    50%     75%   \n",
       "Type                                                                      \n",
       "Ride         295.0  154.535593  12.103490   84.0  149.00  155.0  160.50   \n",
       "VirtualRide   14.0  165.500000  16.383622  135.0  156.75  163.0  175.75   \n",
       "\n",
       "                   Distance (km)             ...   HRSS         \\\n",
       "               max         count       mean  ...    75%    max   \n",
       "Type                                         ...                 \n",
       "Ride         191.0         295.0  53.271525  ...  189.5  471.0   \n",
       "VirtualRide  191.0          14.0  30.450000  ...   94.0  127.0   \n",
       "\n",
       "            Elevation Gain (m)                                               \\\n",
       "                         count        mean         std    min    25%    50%   \n",
       "Type                                                                          \n",
       "Ride                     295.0  912.209831  656.472151    0.0  512.0  743.0   \n",
       "VirtualRide               14.0  394.757143  114.192663  230.6  301.0  368.5   \n",
       "\n",
       "                              \n",
       "                 75%     max  \n",
       "Type                          \n",
       "Ride         1131.85  3617.0  \n",
       "VirtualRide   500.00   558.0  \n",
       "\n",
       "[2 rows x 48 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['Type']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary labels for High 'AvgHR' (1) and Low 'AvgHR' (0) based on threshold of 154 bpm\n",
    "for j in range(df.shape[0]):\n",
    "    if int(df.loc[j,'AvgHR']) > 154:\n",
    "        #print(df.loc[j,'AvgHR'])\n",
    "        df.loc[j,'AvgHR_bin'] = 1\n",
    "    else: \n",
    "        df.loc[j,'AvgHR_bin'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AvgHR</th>\n",
       "      <th>Date</th>\n",
       "      <th>Type</th>\n",
       "      <th>Distance (km)</th>\n",
       "      <th>Avg Pace (/km)</th>\n",
       "      <th>Calories</th>\n",
       "      <th>HRSS</th>\n",
       "      <th>Elevation Gain (m)</th>\n",
       "      <th>Time</th>\n",
       "      <th>AvgHR_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>158</td>\n",
       "      <td>2019-09-22</td>\n",
       "      <td>Ride</td>\n",
       "      <td>62.5</td>\n",
       "      <td>113</td>\n",
       "      <td>1772</td>\n",
       "      <td>144</td>\n",
       "      <td>589.0</td>\n",
       "      <td>16:27:01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>2019-09-21</td>\n",
       "      <td>Ride</td>\n",
       "      <td>80.1</td>\n",
       "      <td>125</td>\n",
       "      <td>2432</td>\n",
       "      <td>217</td>\n",
       "      <td>890.0</td>\n",
       "      <td>11:57:23</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>159</td>\n",
       "      <td>2019-09-20</td>\n",
       "      <td>Ride</td>\n",
       "      <td>35.2</td>\n",
       "      <td>116</td>\n",
       "      <td>1029</td>\n",
       "      <td>87</td>\n",
       "      <td>314.0</td>\n",
       "      <td>17:55:47</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>161</td>\n",
       "      <td>2019-09-18</td>\n",
       "      <td>Ride</td>\n",
       "      <td>45.6</td>\n",
       "      <td>114</td>\n",
       "      <td>1326</td>\n",
       "      <td>119</td>\n",
       "      <td>447.0</td>\n",
       "      <td>17:41:48</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>155</td>\n",
       "      <td>2019-09-17</td>\n",
       "      <td>Ride</td>\n",
       "      <td>41.1</td>\n",
       "      <td>120</td>\n",
       "      <td>1156</td>\n",
       "      <td>96</td>\n",
       "      <td>454.0</td>\n",
       "      <td>16:47:31</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>149</td>\n",
       "      <td>2017-08-18</td>\n",
       "      <td>Ride</td>\n",
       "      <td>70.9</td>\n",
       "      <td>146</td>\n",
       "      <td>1817</td>\n",
       "      <td>175</td>\n",
       "      <td>1202.7</td>\n",
       "      <td>10:09:39</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>148</td>\n",
       "      <td>2017-08-16</td>\n",
       "      <td>Ride</td>\n",
       "      <td>70.1</td>\n",
       "      <td>159</td>\n",
       "      <td>1827</td>\n",
       "      <td>191</td>\n",
       "      <td>1185.3</td>\n",
       "      <td>11:44:59</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>306</td>\n",
       "      <td>150</td>\n",
       "      <td>2017-08-14</td>\n",
       "      <td>Ride</td>\n",
       "      <td>53.7</td>\n",
       "      <td>163</td>\n",
       "      <td>1432</td>\n",
       "      <td>161</td>\n",
       "      <td>1015.8</td>\n",
       "      <td>13:41:10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>307</td>\n",
       "      <td>131</td>\n",
       "      <td>2017-08-13</td>\n",
       "      <td>Ride</td>\n",
       "      <td>40.5</td>\n",
       "      <td>161</td>\n",
       "      <td>941</td>\n",
       "      <td>70</td>\n",
       "      <td>350.4</td>\n",
       "      <td>13:14:20</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>308</td>\n",
       "      <td>151</td>\n",
       "      <td>2017-08-05</td>\n",
       "      <td>Ride</td>\n",
       "      <td>52.0</td>\n",
       "      <td>138</td>\n",
       "      <td>1332</td>\n",
       "      <td>126</td>\n",
       "      <td>811.4</td>\n",
       "      <td>09:32:39</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>309 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     AvgHR        Date  Type  Distance (km)  Avg Pace (/km)  Calories  HRSS  \\\n",
       "0      158  2019-09-22  Ride           62.5             113      1772   144   \n",
       "1      158  2019-09-21  Ride           80.1             125      2432   217   \n",
       "2      159  2019-09-20  Ride           35.2             116      1029    87   \n",
       "3      161  2019-09-18  Ride           45.6             114      1326   119   \n",
       "4      155  2019-09-17  Ride           41.1             120      1156    96   \n",
       "..     ...         ...   ...            ...             ...       ...   ...   \n",
       "304    149  2017-08-18  Ride           70.9             146      1817   175   \n",
       "305    148  2017-08-16  Ride           70.1             159      1827   191   \n",
       "306    150  2017-08-14  Ride           53.7             163      1432   161   \n",
       "307    131  2017-08-13  Ride           40.5             161       941    70   \n",
       "308    151  2017-08-05  Ride           52.0             138      1332   126   \n",
       "\n",
       "     Elevation Gain (m)      Time  AvgHR_bin  \n",
       "0                 589.0  16:27:01        1.0  \n",
       "1                 890.0  11:57:23        1.0  \n",
       "2                 314.0  17:55:47        1.0  \n",
       "3                 447.0  17:41:48        1.0  \n",
       "4                 454.0  16:47:31        1.0  \n",
       "..                  ...       ...        ...  \n",
       "304              1202.7  10:09:39        0.0  \n",
       "305              1185.3  11:44:59        0.0  \n",
       "306              1015.8  13:41:10        0.0  \n",
       "307               350.4  13:14:20        0.0  \n",
       "308               811.4  09:32:39        0.0  \n",
       "\n",
       "[309 rows x 10 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AvgHR</th>\n",
       "      <th>Distance (km)</th>\n",
       "      <th>Avg Pace (/km)</th>\n",
       "      <th>Calories</th>\n",
       "      <th>HRSS</th>\n",
       "      <th>Elevation Gain (m)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AvgHR_bin</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.0</td>\n",
       "      <td>145.794521</td>\n",
       "      <td>55.214384</td>\n",
       "      <td>167.794521</td>\n",
       "      <td>1644.650685</td>\n",
       "      <td>145.445205</td>\n",
       "      <td>1048.490411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1.0</td>\n",
       "      <td>163.306748</td>\n",
       "      <td>49.571166</td>\n",
       "      <td>135.245399</td>\n",
       "      <td>1509.625767</td>\n",
       "      <td>157.914110</td>\n",
       "      <td>745.698773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                AvgHR  Distance (km)  Avg Pace (/km)     Calories        HRSS  \\\n",
       "AvgHR_bin                                                                       \n",
       "0.0        145.794521      55.214384      167.794521  1644.650685  145.445205   \n",
       "1.0        163.306748      49.571166      135.245399  1509.625767  157.914110   \n",
       "\n",
       "           Elevation Gain (m)  \n",
       "AvgHR_bin                      \n",
       "0.0               1048.490411  \n",
       "1.0                745.698773  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('AvgHR_bin').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/pandas/plotting/_matplotlib/converter.py:103: FutureWarning: Using an implicitly registered datetime converter for a matplotlib plotting method. The converter was registered by pandas on import. Future versions of pandas will require you to explicitly register matplotlib converters.\n",
      "\n",
      "To register the converters:\n",
      "\t>>> from pandas.plotting import register_matplotlib_converters\n",
      "\t>>> register_matplotlib_converters()\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAE2CAYAAACOfY6TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO2dfZwdZZXnv6c7DXRA6SCRIQ0xqBBGVBLJiis7K+BLRFEyIm8zjjCjy+roOLAYB2ZdkR3ZzU5U1HHUwRkGnVFexYiiBkdAHTQqIQSMEmR4TYMQTYKSNNDpnP2j6t5U3663e2/dunX7/r6fT3/63qq6Vb966uU8z3POcx5zd4QQQgiAgW4LEEIIUR1kFIQQQtSRURBCCFFHRkEIIUQdGQUhhBB1ZBSEEELUkVEQogTM7HNm9r9KOta7zewxM3vSzJ5TwvFSz83M3Mxe2GkdohhkFEQqZnaLmW01sz0L3OcDZvaahmVnmdm/N2wzHr7YfmVml5vZPhn7PdHMfmJm283sN2b2JTM7qCjdKcedH+qs/Xmoofb9D9z9Xe7+NyVoGQI+DrzO3fdx998UsM/Ua1HWuYlykFEQiZjZAuAPAAfe3AUJb3L3fYBFwGLggqQNzeytwJeBTwD7A0cATwP/bmZzihRlZrOi3939ofAFvE+oF+DIyLIfFHn8DA4A9gI2NPtDC0h6J+S+FqK3kVEQabwdWANcDpxZW2hmR4c1xsHIsj80szvDz8Nm9oWwhfELM/uAmW1qVYS7/wpYTfBCmoaZGfAx4CPu/mV3Hw9/807gSeBcM9vTzLaZ2Ysjv5sb1oCfG34/0czuCLf7oZm9NLLtA2b2V+E5bm80DFmEteuPhJ+PNbNNYbk8bmaPmtkyM3uDmd1jZlvM7K8jvx0ws/PN7D/CFtDVZrZfzDEOAzaGX7eZ2U3h8lea2U/N7Inw/ysjv7nFzC42s1uBHcDz084j7lpEzy38vjw8p0fM7M8aNO5pZh81s4fCLq7PmdlwuG5/M/tGWP5bzOwHKUZKdAgVuEjj7cCXwr+lZnYAgLv/GNgOHB/Z9o8IauoAFwILCF4wrwXe1o6IsAvoBODehE0WAvOBa6IL3X0X8BXgte7+NHAdcEZkk1OB77n742a2GLgM+O/Ac4B/AK5v6DY7A3gjMOLuO9s5J+D3CGr0o8CHgM8TlNNRBK2z/2Vmh4Tb/gWwDHgVMA/YCvx94w7d/R6CFhKhxuND43ED8KnwvD4O3GBTfQ1/ApwNPAt4ME101rUws9cD7ye47ocCr2nYZAVwGIFReWHk/AHOAzYBcwlaPH9N0EoVZeLu+tPftD/gvwATwP7h97uBcyPrPwJcFn5+FoGReF74/T5gaWTbdwKbIt8fIKjBb4v87QD+PWab3xG8GL5L8KJL0urAXjHr3gX8Mvz8GuA/IutuBd4efv4s8DcNv90IvCqi589ylp0DL2xYdjlBSwbgWGAcGIyUnwNHR7ZfCywLP/8CeHVk3YHhtZkVc+wF4b5mhd//BPhJwzY/As4KP98C/O+M80m9Fg3ndhmwIrLusFp5ABbeJy+IrP/PwP3h5/8NfK2x7PRX7p9aCiKJM4Eb3f3X4fcvE+lCCr+/JaxJvwW43d1rtcx5wMORbaOfayxz95HaH/DnCds8i+AlejiBryCOmsYDY9YdGFl/MzA77P5aQFBb/Wq47nnAeWHXxTYz2wYcHJ5L2nm0ym/cfTL8PB7+fyyyfhyo+SeeB3w1ousXwCRBbTqLeUyv/T9IUEOvkee88l6LxmsfPfZcYDawNnIu3w6XA6wkaIHcaGb3mdn5OXSJgpFRENMI+3hPBV4V+g5+BZwLHGlmRwK4+88JHvgTmNp1BPAoEI36ObgdPe7+PYLa6EcTNtlI0O1wSsN5DAAnE9RsCV/CVxN0A50BfMPdfxdu/jBwcdRQuftsd78iKqWd82iDh4ETGrTt5e5jOX77CIFRiTIfiP4293nluBaPMvV6z498/jWBsTsich77euicd/ffuft57v58gsCG/2Fmr86rTRSDjIKIYxlBTfRFBLXpRcDvAz8g8DPU+DLwl8B/ZWp//tXABWY2x8xGgfcWoOkTwGtrRimKB30P7wc+aGZ/ZGZ7mdnvAf8IPBu4pEHzacAfM9WQfR54V9iKMDPb28zeaGbPKkB7u3wOuNjMngd1B/lJOX/7TeCwsFxmmdlpBNf1G23oSbwWBNf+LDN7kZnNJvAvAXUfz+eBSyLO/VEzWxp+PtHMXhgGDjxBcA/uakOnaAEZBRHHmcA/exBq+avaH/Bp4I8jkTdXEDg/b4p0M0HQN7wJuB/4N+BagvDQlnH3zcAX2e2UbFx/FUH/+bnAb4CfA8PAMR6J1ffdTvJ5wLciy28D/lt4jlsJujHOakdzgXwSuJ6gW+V3BBFhR+f5YXjuJxI4cX8DfAA4seF6NUXatXD3bxEYjZsIyvCmhk3+Kly+xsx+S3B/LAzXHRp+f5LA7/EZd7+5VZ2iNSyoZAnROczs3cDp7v6qbmsRQqSjloIoHDM70MyOCePrFxLUUr+a9TshRPdpagCOEDnZgyDO/xCCcNMrgc90VZEQIhfqPhJCCFFH3UdCCCHqyCgIIYSo09M+hf33398XLFjQbRlCCNFTrF279tfuPjduXU8bhQULFnDbbbd1W4YQQvQUZpaY+FDdR0IIIerIKAghhKgjoyCEEKKOjIIQQog6MgpCCCHq9HT0kRCifFatG2Pl6o08sm2ceSPDLF+6kGWLR7N/WLFjiHh6Os3FkiVLXCGpQpTHqnVjXHDdXYxPTNaXGcEsPaMFvbzjjjE8NMj/fctLZBgKwszWuvuSuHXqPhJC5Gbl6o1TXtawe9q2sW3jXHDdXaxal2dCuOaOMT4xycrVG9var8iHjIIQIjePbBtPXV/EyzvpGGPbxjlmxU0ccv4NHLPipraNj4hHRkEIkZt5I8OZ22QZjlaPYQSGwSmuVSKmI6MghMjN8qULGR4aTN0mj+Fo9hg1v0WU8YlJzrt6PQvOv4EXXPBNFqgFUQgyCkKI3CxbPMr/fctLGA1f/NawfnhokOVLF07/YYvHMAIHdlI4zGQYKFP7rxZE+yj6SAjRMmWFjh6z4ibGmuyWKioaaiaSFn2kcQpCiJZZtni0lJfu8qULp4WpZlFrNQAyDE2gloIQoieItkoGzOpdRlkMmrHLfVpLpp8HyKmlIIToeaKtkrgBbkk0+htqRH+vVsVuZBSEED1DtHY/MnuIPWcNsG18gsGcLYfoOIqkAXIyCkII0QM0tg627phgeGiQT5y2CIDl165nYjLbMKSNo2h3jMVMQEZBCFEK7fbhZ6W/yGMQYPc4irhopnbHWMwEZBSEEB2nsZbfSh9+Ui2+mdp9dBxFXNK9dsdYzAQ0eE0I0XGKSHKXVIufNzKcuG7O7KEpg+BqmVbjBsgpC2uAWgpCiI5TRC0/bqxCVs3/wjcdkfiiL2uMRa8hoyCE6DjzRobb7sOvvcDT/BL9Ou6gSDR4TQjRcTRxTrXoyiQ7ZnaZmT1uZj+LLFtkZmvM7A4zu83MXh4uNzP7lJnda2Z3mtnLOqVLCFE+6sPvHTrZfXQ58Gngi5Flfwtc5O7fMrM3hN+PBU4ADg3/jgY+G/4XQswQ1IffG3SspeDu3we2NC4Gnh1+3hd4JPx8EvBFD1gDjJjZgZ3SJoQQIp6yHc3nAKvN7KMEBumV4fJR4OHIdpvCZY+WK08IIfqbsscpvBs4190PBs4F/qnZHZjZ2aE/4rbNmzcXLlAIIfqZslsKZwJ/GX6+BvjH8PMYcHBku4PCZdNw90uBSyGIPuqMTCFEO3QiLXXWPvs5FXaRlG0UHgFeBdwCHA/8Mlx+PfBeM7uSwMH8hLur60iIHqSIlBZ59nnuVXdwzlV3MDoyzHGHz+Ura8eUCrsAOhmSegXwI2ChmW0ys3cA/w34mJmtB/4PcHa4+TeB+4B7gc8Df94pXUKIzlJESos8+6x1E4xtG+dLax4q/Jj9SsdaCu5+RsKqo2K2deA9ndIihCiPrJQWq9aNcdHXN7B1xwQAI8NDfPjNyeko0vZZI6kfWamwm0cJ8YQQhZKWuG7VujGWX7u+bhAAto1PsPya9axaF+tGTN1nq1pEMjIKQohCWb50IcNDg1OWGUE3z3lXx0+EM7HLU7t64vbZiDV8jybLW7VujGNW3MQh59/AMStuSjRAebebySghnhCiUKKJ68a2jWPs7t5JmzIzrasnbZ8QGICTjxrl5rs3T4s+yuv47oSDvBeRUegSCp8TM4Wke3nZ4lGOWXFTbHbUOLK6eqJpMpp5ftIc340ZVjVvs4xCV1CNRMwUsu7lvI7eoQFratazZvIo5Z3LoYg5H2YC8il0gU6E7AnRDbLu5aTa/0DEATAyPMTKU46c1pVTVN9+muO7le1mOjIKXUA1EjFTyLqX4xzEw0ODfPzURTyw4o08sOKN3HHh62L79se2jePsbn20ahiSNDS2TPJuN9ORUegCqpGImULWvdzKPApFt6TzatCcDwHyKXSBrLlmhegV8tzLzc6j0ImWdF4NmvNBRqEr5JlrVsxsZkr0WSfu5aT5nEdmD3HMipt6vsyqjuZoFqJkNF9xOnHlE0dcmUWN7cjsIdzhifEJGZEGujJHsxAiHkWfpRPt24fpI5VrNJZZo4N6644Jto1PFOKs7idkFIQoGUWfZbNs8Si3nn88oyPDicnuYGqZxRnbKDK8+ZBREKJkFH2Wn6zR0NEyy2NUZXizkaNZiA6R5ExuNvpspjilW2HQLDFfUmOZJTmoowyYccj5N2SWYxXKvFsaZBSE6AB5UpnkeeD7PSVKWgK9RidznLFN2l9aOVahzLupQd1HQjRQRIqFopzJveSU7kTa6ZHhodjloyPDiYPPBi3JNT2VpHLMW+adTLPdzesuoyBEhKJSLKQ5k5s5Rq84pYtOTVHb5/Zndk5bnpY8b9niUXY1EWYfV455yrwT59ushk4hoyBmHO3U4IqqoSUngjM+fP2G3MeomlM6qWw7NS9z3IQ8++w1K7ULpZmyiW5bO7ckkxLdttM1+W5edxkFMaNotwZXVA0taaawSXe2jU/E/CL+GFVK0pZWtp2o2Sb9dtuO+PKrkWeWNpg+M1vt3LK2TdNWVE3+uMPnNrW8SGQUxIyimVp4HEXV0Jrt3046RpWStKXVjpPKx6Gl/vZV68YYSCi7PJPxRAe/xTFoNqUc08Y4xJV5p2rytdbKv655KHb9zXdvbmv/eZBREDOGVevGEmvhY9vGc3UpNVszT+uqaqZ/O+0YtYFcl5y2CIBzr7qjfqwy5xROqx2n1WDHto1zzlV3sOiiG3Ppq9Xa4yKP8rSSoqGcc2YPMTQw1bgMDw3ysVN3z9+wat1YYgvBgFvPP36aEe5ECy6rtQLZ4zaKQCGpYsaQ1hqoTRwP6eF9RYeLJsXOz5k9xOw9ZuWOQY871vJr14MHk95nnVcRJJ3LvJHhXDXYbeMTufQl1doba/dxNJbT1h0TDA0aI8NDsTmQatsnkVTz70QiwKwR2UBTLc9WkVEQM4a0/tzGOuf4xCTnXb2ec6+6Y9oDnTd9cp45fZMGql34piNi4+OTXjJxx4pzwhYxp3Arg+7OveqOXPuO6ks6TtJ13OWeeV5J5bRtfILRmBd32os4q+ZfdJrtPP6ItHEbRSGjIGYMeUa0RskzkCmNPM7GvDXKouY6TtOVh1YH3a1cvTF32UfDcuOOk9YiybPvJOLOJW37sn03ee7fND9JUcgoiBlDXE3WmN5KiKOVGnbel1dWjXLVujHOu3r9tFpgVFMzBq8ZZ2djbX3L9qcZn9iVqCPpXPKMJo7qS2tltTMJVVY5NV7npO3jBsd1mqwyLCvqTI5mMWOIi9RpprFdRNhpsw9umlM1qinuWEODFutEzXv8uBDTRoPQqCOJxrKfM3uI2UPTXy81fWmtrHYirvKEo0aPXaWQ32WLRzn5qNHYVOEjw0OltVzUUhAzhrg+6g9fvyExIqkRM1h00Y1THJKwu7ukcdKW4w6fy15DA/Wa3cjwEB9+83RfQRpZzsXobGMjs4fYc9ZAor5mnZ15HJs1BsxYtW4sdd9xrYgkv0FSd9O8keG2EsFFu7iSWgzRllRW914ZSemixxgwi63I7L1n8KouY+a5js28ZmaXAScCj7v7iyPL/wJ4DzAJ3ODuHwiXXwC8I1z+PndfnXUMzbwmasTN1jU0aEzucnY13OIDFkRxTDSuaGBo0KZE92TRyuxph5x/Q2JrJu74Rc7QlnbsOIo8dtLscycfNcpX1o4VMitduzPclTFDXt5Z5mrHLkpLt2Zeuxx4fYOQ44CTgCPd/Qjgo+HyFwGnA0eEv/mMmWUPSawIZcaKt3K8rO3L1t8JkqJO4t7n+w4PsfKUIzPD+yYmPbdBgHyD5BrLet+EhG+DZuy9x6xpxy8jlUISRR47qYvo5rs3F5Y+ot2Bf2UkpcvbWhs0Ky1BXse6j9z9+2a2oGHxu4EV7v50uM3j4fKTgCvD5feb2b3Ay4EfdUpfUZSd4rbZ42VtX4U0wUXQjD9g644Jli0ezR1GWZSOuLKu+QXiWgNJ+opKpdCMc7joY0N8d1PR59xO2GgZSeny7KuxhdApLTXK9ikcBvyBmV0MPAW8391/CowCayLbbQqXVZ48serdPF7W9mXr7xTNROfUWgjNhrDm1RFHUoTRxKQnDmRL6hevtS4a+7uPO3wuN9+9OXefc1x/em0fefrjO0GeiK52z7tILZ06xqAZu9zr55N0L4zMjm9ptkPZRmEWsB/wCuA/AVeb2fOb2YGZnQ2cDTB//vzCBTZL2Slumz1e1vJeSc2cRTO13tqLOes3ST6JJJKiVrIijLbtmGDdh143bfnypQtZfs36aV1I25/ZyQdX3TWl731s2/iUfDl5W3xJNemk/vROR+VkhaPGtbZaOe8itBRB0jHiurmWX7t+2oDFJ5/amRkA0Cxlh6RuAq7zgJ8Au4D9gTHg4Mh2B4XLpuHul7r7EndfMndu5zMGZlF2ittmj5e1vGqpmVulmQR0g2Z8cNVd9VZS7TdzZg8xMjxU739e+dYjefZeyTUxgynbJ/VX540wavTpLFs8yj57Ta+3TUw6V/z44UwDOD4xyTmRPEnN0K1EfFnHzdMHX1RfexllUDvGnEiNf89Z8a/lnTG1k4ldzkVf31CYHuhg9BFA6FP4Ri36yMzeBcxz9w+Z2WHAd4H5wIuALxP4EeaFyw9199SrX4Xoo6Sol733mDUldLGo5m1aRARMD60DEqM8at0EjQO8io6wKJNmI2pq1Pr2d4Rx+rOHBthj1mBmOGtWWa1aN8Y5Kb6LtAgjIPW3zVCla9pOmGfe62vA/SveGHvsi76+ga1h+u1WwoiLDlPNinLKE6H0idMWNaWhK9FHZnYFgaN4oZltMrN3AJcBzzeznwFXAmeGrYYNwNXAz4FvA+/JMghVIW7QDh4k/6oNCPrXNQ8VNkNTUu0FiM11D0zbvhb2V+ujdKgPmOlmauYiaLWFMzHpdYMAsGNiV67xDWm10qxka2kRRhd9fUPmb5uhKlN4tjvfRd7rG7fdqnVjLL92fd0gQPCcLr9mfe7jd2LGtawopzytoyKvbSejj85IWPW2hO0vBi7ulJ5myVsbaNxu+9M7M8MY8yQFSyOuH/iYFTcl3liNqX/jtnWCF02nBsS0Qitl00pETbvE+V+SHMs1aq21pLz50RdX0m8b4/lb0ZlF2jWIWwfJg/1qDtN2AhvyXN+kfv+kmdwmdnnu43ciMKNVv1+efbSCRjTHkDdMM267vIxlJAVr9gZrxmGctO2ke2XCUVstm7iImu1P78w9qrkVGscaZDmWAV42f1++sra12mWtJbfkeft1NHIo7RoA09Y1pvKOGrbab9sNrUyLmMqqPKQ9n3mP34nAjKwopzxRckX6ADvqU+g0nfIpHLPipsQkWbXaTm1IequpbAcMDtw3ORnXrecf31RNOUlz7Vh7zhrgqYlduV6SteN3k6TzGTSbMkFKHpoZNdoKZnDJqbv7dNOuRY3BFu+dPPMwFDUSd9FFN8beJ3NmD/Hb8Z0t6U8676R7roj++9o+sq7JnJiWTVyLPO2ZbYU8PoW4yKMaQwPGylOaeya6NaK5Z0my+rXaTq0/sZ3c5rs8ueYSTS2ct+8yLRHYLofxiV31/Wx/Zue0RGqNx+82Wa2ZZvpwo36YTuDOFE2dyos/NGg8+dTOzHuiiKiZtFnstu6YaPnen3TPnYCuiP77PLOZ1di6Y2KKLzDuWJ1IoJfnek0mdEkPDw00bRCyUEuhgbS+YLPgBZDFgMGz9wpmekprTaTVmiDeaKTVSPLWiGC3YzlOWZVbCjVa1ZhU+y2CmqYiWwrRQUxJLbxOXK9OlVNjazvO75DV4mrmfPNciyy90Vb72LZxBoz62JW80UuttniyfFOtXnu1FHKS1hc8NGi5DAIEN8zTO3dxyWmL+NipRyZul1ZraqXvctni0dw3iBMYuXZSL3eSrBTIrTpNtz+zsx1ZqaSluY4yPDTIGUcfnJnieWgg6Cq7f8UbufX843ki4SVddMsurZXQDrV7q3afXnLaIp4Ko7ziaudF9N+3WzaNrXZgymDGp3fGpxqP0mqLJ49vqhOtehmFCEmhX2aw9x7N+eSjEQkjCUnPAAyPHQDVzqCyvOGKuxz22WtW6QOU8pA1GK2V4f1J0SdF4cALLvgmtz24ZVqYcuM1/siyl0wbtNTIHrMGWLl6Y2byvKJTHXQidDVufuWsUMwiBla264CdNzLMh6/fkOiPyhPq22pivTyhqJ0YZKroowhJVtfDcQet7u/Dbz4i0dG5Y2IXE5POJQ2DT9oZYn/G0Qcnhjo2kpRioQrUyqOo4f1Ztao8aS2GBozTXn5wYjjopHu97PO02p5KmNQGYPszk2x/JtBcS54X7bqoUXSqg6Jrn0lO7qyWQBFpJpYvXZg+eHAgOYX68NAgxx0+N/NZyuqearXFk7XfoQHrSKteLYUIaVY3qcY6Z/ZQogOztr+sWm9tqHo01QFMH3SWtxb/kWUv4W2vmJ+rxVBGOot2UnMvWzwa20qrxZY3Q9q51tJafPzURYm195Ew5Xatlp/muL7ixw9P+R5XBs1McgPBALu4noRWyiKNIu+JtPs2qyVQhMM8raW+9x6D09KINA7ivPnuzZnHyHrOWm3xpO23di/21CQ7ZVC0ozkrJUHSJBcQn0qi8QZecP4NubUUPaFJXG27lVC2Vo7dbnhkWrk9EJPKoJNaWtGWdNwiQ2ST0jq0QlYIb57Bc3nKtYxJbJKOkzaBUlRD3rQaafdhq+dZ1H0fhxzNEdJqrWm1ikEzTj5qNLbWElejOfmo0Sn9wavWjTWVmqCW6qAIli0eZeVbj5xSA+5kTSNKEROVJJVbs6keAPaKzBs8MjwUe52aIY+2pDJI++1wzPzGaRRZu49L3RL1idTyZqUlE8xjEGrlEo11SEoGV/T57EyZQCmPXyNKVqhzKy2etPdFp0Kra/SVTyFuhObya9fz4es31EPiTjzywNga0KQ7X1k7lngxo6knkkaCNhvbvXXHRP0l1e4AnnYmG2mHIiJIksqtmfKMq61tf2YnV/3k4frLoZUR5Un+mzOO3p30N23MRVwywpOPGuWqnz4c+5skiu5bzptSuxZBd+Gb8ieVa9xH9N28bXyiqWuQN9Szdj61Y2fdOWl+jUaKKPukkNdGyogO7KuWQtKUjdGQuK+sHePko0ZjrXTeGm6zNcM0/vq6OwtPwFUmRUSQJNWMmqkxJV37dqe7bPTfDJrxtlfM5yPLXlLfJu1c45IR3nz35tgoqaTbZ87sodIMfhEtvyxfSt79tRLqmdePk9cfmKfss3SmhbxGiYvg6gR91VLIUzsdn5hMjTbIs4+kqIFaraqZvuQdMdEpcQm4ik7nWxRFRZC0u49WYtvzlOmqdWPcfPdmdrnXB2ZNmxwlo7bpTB2ElDQlpXu8X+uNLz2QY1bcVMq1L2vsQJ5t4kJFs5LT5Z3+cvnShVOu/8jsIZ58aue0FOcXvumIKb+Nu2eSDOlFX9+Qe7ApwC73Up7pvmopFNHvmrWPVevGSGoP1GqCRfQJRm/uTqTzLYqiIkja3Uezse15yjRvuedJsxG9nklao/dPXBr0Mq59WWMH8jxnSWHiaS/+rP0mpaLfumMCLH1SpaT7Iemlv3XHRFOjrcua+Kqvoo/aTYyWJ2IgaVi9wZSxCO0Ovwfq3RSdSNI108gbgVK7xkk1uGiZtlLueX7TTLRK2de+iIihrOew8VmJI+35yUoFk0d/UpqPtMR95159R+6sB81SdFSWoo9C4qIQUvLC1Wmmdpo4AI6pjrOsVAh5+Nc1D/HBVXfNmHmWO0lca2PlW49k5SlHxrZA8pRpK+WeJ6FaMy2jsq99kS2/JBqflTiyyjjr2Gn6m22F1EK+O2UQRoaHSs000Fc+BZgeVZGV+KvZlL5Juc8buw7i8sJv2f404zE+hDmzhxInXbnixw8nHrOWdmEypb+7n0iKqEkaWJWW4z7vNknHyvJV5I0Wa0VDuxQRybZs8WhqayyLpPOuOX7T/EHRSKSVqzdy7lV3sHL1xvo2aU7ueSPDfHDVXVzx44eZdGfQjD1mWVPpU0ZTkhvC7mSJ3Xpm+84oNJKUZAySh5GnTT7SjFO08eFKatpe+KYjEgfVTbqnOjJrYZvtTODTj+S5jq06wIsMDy7CCd8t2tGe9NsL33RErgma0rZJa4UseM7wlECUSXfGJ/IbBIN61tUyBu61Ql91H8WRVKMyI3FwV9aUfK02r9N+mzbQKY8jM6qxn8mbciPtWtT2ce5Vd7DX0EBTg7aKpojunG7RqWclT9hs0jbnXb0+MfHgnNlDrLlva9PnGWXAjEPOv4GVqzcmDobtNn3laI6jFYudNPS9yFQDjXxw1V2xobKNMfFZw/I7qbHqdMpJWpUangjI83ymPSdpAQhpaXCapZv3jRzNKbRSW0mqSXSiH7dWK/3SmofYe4/Berhr3CCpPBqS1reTtK5X6NTAK7XAqkWesNm052Ri0tlj1kC9dR4Eo3ji+JHaNnvvsTuAwIBDn7t36qg7i7oAABjASURBVIDVWsukas9a3/sUoLk+3qSJWjqRxraxVrr9mcnM2kWafyFr2sO0PtiZQCcHXvV7pFeVBk+26g+Ksv2Z3ctr09mmMWjwTGTCHQd++fj2TK216WWhOs9aSy0FM9u7aCG9QtJELfvsNavwi9pKrbTRv1CrqaS1gPql9tvJgVdlDSyqIlUbPJmn9Z+VvqJZJnbFZ1zNQ9WetdSWgpmNAgcCd7r7M2b2XOAc4CxgXuflVY+kGuG2hJDRThwrq1babHRLv9R+24l2iSYsi0ti1wvRPp0iK/CiG2Q9A7XrGZeUsBtU6VlLbCmY2TnAHcDfAWvM7J3AL4Bh4Khy5FWPMmuKZR2rX2q/rUa7NCYsi0tiV5WmfzfotUpF3PXsNlV61tJaCmcDC919i5nNB+4BjnH3teVIqyZlxoWXdaxejXVvpR+7lTECcTXhxiR2M41myrYbA+jaodkZ74rCgFe+YD9++B9bphmiLdufTpxStWx/TZpP4Sl33wLg7g8BG/vdIEC5ceFlHasXY93L7MfutZpwuzRbtnlSd1SJZq6bMTWqqB0cuP2hJ2JbJuMTu1h+zfRIpG74axLHKZjZ48CVkUWnR7+7+/s6pionRU/HKXqHMhPB9VvCwVbOt0rRR1nkTUZZO9+k7efMHgrmYimw/2nO7CHWfeh1mVrbvffSximkdR8tb/jeVCvBzC4DTgQed/cXN6w7D/goMNfdf21mBnwSeAOwAzjL3W9v5niivyiz9t6r3Wut0krZFpm6o9PkmU0ten2TznvrjgkGB6zpGRXTqM22WCvLbrRSE42Cu3+hzX1fDnwa+GJ0oZkdDLwOiA7PPQE4NPw7Gvhs+L8v6aVaV7cosx87bxK7mUKv+QiaJe56Hnf4XG6+e3Ps9U0qj0EzJlsMQ00jGrXVjWuRaBTM7OukOObd/c1pO3b375vZgphVlwAfAL4WWXYS8EUP+rLWmNmImR3o7o+mHWMm0i8Dydql7Np7L9WE26UfWkbNXM/jDp8bm2KmyBZClGgroBvXIq376KPhfwM+D7yz3YOZ2UnAmLuvt6mDRkaB6Ezlm8JlfWcUqhjzXUX6rfZeJirbqdx89+bY5bUU10UTbQV041qkdR99r/bZzJ6Mfm8FM5sN/DVB11E7+zmbIFyW+fPnt7OrStJvkS7t0E+197JR2e4m6dmbdGdocPpcCgMWGIxWRjjHtQLKvhZ5cx8VYQ5fABwC1FoJBwG3m9nLgTHg4Mi2B4XLpgtxvxS4FILoowJ0VYqZ3p/bL8gvNJVV68a46Osb6pNFjQwP8eE3H9ETZZI2cdbypQunndeJRx7IDXc+mjgxVhJVmQgrzaewX+TroJnNYfdATmpjGPLi7ncBz43s/wFgSRh9dD3wXjO7ksDB/EQ/+hOgP/pzZzryCwVEU4M0sm18guXXrAeqXyZpz2SeibKyqFrq9bTBa2uB28L/zwZuDz/XlqdiZlcAPwIWmtkmM3tHyubfBO4D7iXwX/x5LvUzkF4cSCam0i8JBtNoTCURx8Qu74kyaeaZbHa0tAEnH1Wtrro0n8Ih7ezY3c/IWL8g8tmB97RzvJmE+nN7G/mF8r8ce6VM8j6TzZ6PM92R3e2ux0yfgpm9LGbxE8CD7j59YgEh+hz5hfK/HGdamSRd+zSiZVWFrsc88yl8BlhD4Nz9fPj5GmCjmbUVSSTETKTMXEBVnTEvz8u+ExNTdZuka/+J0xYlzqEeLasqdD3mMQqPAIvdfYm7HwUsIuj/fy3wt50UJ0QvUpZfqGqT20SJezlGGRkeYuUpR864btK0a5+nslCFrsc8IamHufuG2hd3/7mZHe7u91lBsxYJMdNo1S/UTH9ylQc6ljnoqtt98I0kXfs8ZVKFrsc8RmGDmX2W3RlSTwN+bmZ7AsVPNyZEn9Jsf3IVapVplBEwUYU++GbIKpMqhKTnMQpnEYSInhN+vxV4P4FBOK4zskQnqFqNqpGq6+s0zdb8q1Cr7DZVaC0Ved9WIcVIHqNwAvBpd/9YzLonC9YjOkTVa1RV11cGzdb8q1Cr7Dbdbi114r7tdkh6Hkfzm4B7zOxfzOxEM8ubGkNUiCpENaRRdX1l0Oxc2Rro2P35xZPu24u+vqGSUWF5yHzBu/ufmtkQQYvhDODvzew77t521tQq0C9dFt2uUWVRdX1l0ErNv9u1ym7T7dZS2gQ8tdxHvdbqzdNSwN0ngG8ROJtvB/6wk6LKosohfUXT7RpVFlXXVwaq+TdPt8ss7/3ZS63ePCOaTyCIODoWuIVgENspHVVVEmU7qbrZKul2jSqLqusri36v+bdCN8ssz9SeNXql1ZunpfB24KvAQnc/i8C5/MlOiiqLMrssut0q6XaNKouq6xMijrj7dmR4KHbbXmn1mueYOcjMFhP4E04F7geuc/e/67C2TJYsWeK33ZaZsDWRY1bcFBvSN2f2ELP3mFVojT7pWKMjw9x6/vFt7VsIUR3i0mdXLT22ma119yVx6xJbCmZ2mJldaGZ3A39HMF2muftxVTAIRRA37Hxo0HjyqZ2F1+jlSBWiP+j1Vm+aT+Fu4AfAie5+L4CZnVuKqpKIGyiy/emdbBufOlC7CD+DBhoJ0T/0sm8ozSi8BTgduNnMvk0QeTTjkh01XrxDzr8hdrt2a/RypAoheoHE7iN3X+XupwOHAzcTpLl4rpl9dianzO5UaGSvNymFEP1BLkdzfeNgnuZTgNPc/dUdU5WTdh3NcfSCk0gIIdohzdHcVMoKd99KME7h0iKEVYnoGIKR2UPsOWuAJ8Ynckcf9cvIaCFE6/TCe0J5jJjeOti6Y4LhoUEuOW1RrgumZG5CiCx65T2RK83FTKfdZGxK5iaEyKJX3hMyCrQ/hkBjEIQQWfTKe0JGgfYjjpTMTQiRRa+8J2QUiB/Z3MwYgnZ/L4SY+fTKe0KOZtqfAq8KU+gJIapNr7wnmhqnUDU6MU5BCCFmOi0lxBNCCNF/yCgIIYSo0zGjYGaXmdnjZvazyLKVZna3md1pZl81s5HIugvM7F4z22hmSzulSwghRDKdbClcDry+Ydl3gBe7+0uBe4ALAMzsRQQZWY8If/MZMxtECCFEqXTMKLj794EtDctudPed4dc1wEHh55OAK939aXe/H7gXeHmntAkhhIinmz6FPwO+FX4eJZjZrcamcJkQQogS6co4BTP7n8BO4Est/PZs4GyA+fPnF6xMiGR6IcOlEO1SekvBzM4CTgT+2HcPkhgDDo5sdlC4bBrufqm7L3H3JXPnzu2oViFq1DJcFj13txBVo1SjYGavBz4AvNndd0RWXQ+cbmZ7mtkhwKHAT8rUJkQavZLhUoh26Vj3kZldARwL7G9mm4ALCaKN9gS+Y2YAa9z9Xe6+wcyuBn5O0K30HnefjN+zEOXTKxkuhWiXjhkFdz8jZvE/pWx/MXBxp/QI0Q7zRoYZizEAVctwKUS7aESzEDnolQyXQrSLsqQKkYNeyXApRLvIKAiRk2WLR2UExIxH3UdCCCHqyCgIIYSoI6MghBCijoyCEEKIOjIKQggh6sgoCCGEqCOjIIQQoo6MghBCiDoyCkIIIerIKAghhKgjoyCEEKKOjIIQQog6MgpCCCHqyCgIIYSoI6MghBCijoyCEEKIOjIKQggh6sgoCCGEqCOjIIQQoo6MghBCiDoyCkIIIerIKAghhKgjoyCEEKKOjIIQQog6MgpCCCHqdMwomNllZva4mf0ssmw/M/uOmf0y/D8nXG5m9ikzu9fM7jSzl3VKlxBCiGQ62VK4HHh9w7Lzge+6+6HAd8PvACcAh4Z/ZwOf7aAuIYQQCXTMKLj794EtDYtPAr4Qfv4CsCyy/IsesAYYMbMDO6VNCCFEPGX7FA5w90fDz78CDgg/jwIPR7bbFC4TQghRIl1zNLu7A97s78zsbDO7zcxu27x5cweUCSFE/1K2UXis1i0U/n88XD4GHBzZ7qBw2TTc/VJ3X+LuS+bOndtRsUII0W+UbRSuB84MP58JfC2y/O1hFNIrgCci3UxCCCFKYlandmxmVwDHAvub2SbgQmAFcLWZvQN4EDg13PybwBuAe4EdwJ92SpcQQohkOmYU3P2MhFWvjtnWgfd0SosQQoh8aESzEEKIOjIKQggh6sgoCCGEqCOjIIQQoo6MghBCiDoyCkIIIerIKAghhKgjoyCEEKKOjIIQQog6MgpCCCHqyCgIIYSoI6MghBCijoyCEEKIOjIKQggh6sgoCCGEqCOjIIQQoo6MghBCiDoyCkIIIerIKAghhKgjoyCEEKKOjIIQQog6s7otQAghqsiqdWOsXL2RR7aNM29kmOVLF7Js8Wi3ZXUcGQUhhGhg1boxLrjuLsYnJgEY2zbOBdfdBTDjDYO6j4QQooGVqzfWDUKN8YlJVq7e2CVF5SGjIIQQDTyybbyp5TMJGQUhhGhg3shwU8tnEjIKQgjRwPKlCxkeGpyybHhokOVLF3ZJUXnI0SyEEA3UnMmKPioJMzsXeCfgwF3AnwIHAlcCzwHWAn/i7s90Q58QQixbPNoXRqCR0ruPzGwUeB+wxN1fDAwCpwP/D7jE3V8IbAXeUbY2IYTod7rlU5gFDJvZLGA28ChwPHBtuP4LwLIuaRNCiL6ldKPg7mPAR4GHCIzBEwTdRdvcfWe42Sag/9ptQgjRZbrRfTQHOAk4BJgH7A28vonfn21mt5nZbZs3b+6QSiGE6E+60X30GuB+d9/s7hPAdcAxwEjYnQRwEDAW92N3v9Tdl7j7krlz55ajWAgh+oRuRB89BLzCzGYD48CrgduAm4G3EkQgnQl8LWtHa9eu/bWZPQjsD/y6Y4qbR3qSqZIWkJ4sqqRHWpJpVs/zklaYu7cvp0nM7CLgNGAnsI4gPHWUwCDsFy57m7s/nXN/t7n7kg7JbRrpSaZKWkB6sqiSHmlJpkg9XRmn4O4XAhc2LL4PeHkX5AghhAhRmgshhBB1ZopRuLTbAhqQnmSqpAWkJ4sq6ZGWZArT0xWfghBCiGoyU1oKQgghCkBGQQghRB0ZBSGEEHVkFIQQQtTpKaNgZteZ2dvMbJ9uawEws33NbIWZ3W1mW8zsN2b2i3DZSLf1RTGzD3VbA4CZ3dSl437czI7pxrHjMLNZZvbfzezbZnZn+PctM3uXmQ11W18UM+t6pI2Z3dPFY/fVe6enoo/MbAz4EUGa7X8DrgBu6NZkPGa2GrgJ+IK7/ypc9nsEaTpe7e6v64auOMzsIXefX/Ix72xcBBwGbARw95eWqGUz8CAwF7gKuMLd15V1/Bg9VwDbCNLEbwoXH0Rw7+zn7qeVrGe/pFXAenc/qEQtvyOYgKt2fAhS7O8A3N2fXZaWUE9fvXd6zSisc/fFZvZsgkyrZwD/CfgGwUN+Y8l6Nrp77KStaes6qOe3SauAYXcvdQS7mV0P/Bb4CEGeKwN+APwXAHd/sEQttXvnMIIUK6cTTPB0BcG9U2pN1MzucffDml3XQT2TBEbTIos9/D7q7nuUqOVTwAiw3N0fC5fd7+6HlKWhQU9fvXd6qvuIsPbg7r91939x9zcAhwM/Bs7vgp4HzewDZnZAbYGZHWBmfwU83AU924BD3f3ZDX/PIpi7olTc/c3AVwgG1hzp7g8AE+7+YJkGoSYn1HSPu/+Nux8BnArsBXyzZC0AW8zsFDOrP4NmNmBmpxHMPFg29wHHuvshkb/nhy/ix8oU4u7vAz4JXGFm7wvLqJu117567/SaUXiycYG7/8bdP+fux3dBz2kEc0p/L+zb2wLcQpDU79Qu6PkiydkPv1ymkBru/lXgBOBYM/saUFqNswFrXODud7r7BeEUsGVzOkFW4MfM7J6wz/xXwFvCdWXzCWBOwrq/LVMIgLuvJUizD/A9AuPdLfrqvdNT3UeitzGzI4H/7O6f68Kx93H3aQ93FTCz50Dwoum2lipiZgcCi929Gy26vqMrWVLbwcwOJ+jXq03XOQZc7+6/6J6q6ZjZn7r7P3fhuPsSzGQXLZ/V7r6tbC1JesxspGw97v5k1cqmRqMxMLPXuvt3ytZRpWcrTkvoV+jKc16lskmjiPdOT3UfhX1mVxJ0Bfwk/DOCvsdu9O2lcVHZBzSztwO3A8cSRGvMBo4D1obr+lZPlbTk4J/KPmCVnq0qaamingzafu/0VPdR2O96RDiNZ3T5HsAGdz+0ZD2NIZf1VcBh7r5nyXo2Akc31nwtmBf7x12IaKmMnippCY97fdIq4Hh337tkPZV5tqqkpaJ6Ovre6bXuo13APILQuSgHhuvK5gBgKdOjRQz4YflyMOKjNHYR42gtgSrpqZIWgD8A3sZ0J6bRncmmqvRsVUkLVE9PR987vWYUzgG+a2a/ZHfo1XzghcB7u6DnG8A+7n5H4wozu6V8OVwM3G5mNzK1fF4L/E2f66mSFoA1wA53/17jirBVUzZVeraqpKWKejr63ump7iMIYrkJalJRh89P3X2ye6qqQ9gdspTpztRuxL5XSk+VtFSRKj1bVdJSRT2dpOeMQiNmdra7dz03S40K6jnR3b/RbR01qqSnSlqgknoqcy9XSQvMbD0zwSjc7u4v67aOGtKTTpX0VEkLSE8aVdICM1tPT4WkJtANJ2Ea0pNOlfRUSQtITxpV0gIzWM9MaCkc5O6bsrcshwrqebm7/6TbOmpUSU+VtEAl9VTmXq6SFpjZenrKKJjZ+4Cvuns3ks1No4J69iDIm/OIu/+bmf0R8ErgF8CljXHW/aSnSlqqqCfU9HyC3EsHA5PAPcCX3T0p+25faOk3Pb1mFJ4AtgP/QZDy+Bp33yw9dT1fIggznk2QMXUf4Drg1QTX+sx+1VMlLRXV8z7gROD7wBuAdaGuPwT+3N1v6UctfanH3XvmLzz5AeB1BKkANgPfJphc4lnSw53h/1kE6Y4Hw+9WW9eveqqkpaJ67opomA3cEn6eD6zrVy39qKfXHM3u7rvc/UZ3fwfBKMPPECQ5u096GAi7JZ5FcLPsGy7fE+jGFI9V0lMlLVXUA7sHs+5J0HLB3R/qkp4qaekrPb02onmKh92DftfrgevNbLb08E/A3QQziv1P4Bozuw94BUFCr37WUyUtVdTzj8BPzezHBCk4/h+Amc0FtvSxlr7T02s+hcO85GkT06iaHgAzmwfg7o9YMIn3a4CHvEtRLVXSUyUtFdVzBPD7wM/c/e5uaKiiln7T01NGIQ2r2CQq0pNOlfRUSQtITxpV0gIzU0+v+RTS+Hm3BTQgPelUSU+VtID0pFElLTAD9fSUT8HM/kfSKkJnS5lITzpV0lMlLSA9vaIF+k9Pr7UU/g/B5OLPavjbh+6ci/T0jp4qaZGe3tHSf3rKjrFtMz73h8BRCeselh7p6QUt0tM7WvpRT085ms1sIbDFY0YNm9kB7v6Y9EhP1bVIT+9o6Uc9PWUUhBBCdJae8imY2b5mtsLM7jazLWb2GzP7RbhsRHqkpxe0SE/vaOlHPT1lFICrCSarPtbd93P35wDHhcuulh7p6REt0tM7WvpOT091H5nZRndf2Ow66ZGeKmmRnt7R0o96eq2l8KCZfcDMDqgtMLMDzOyvgG7MaSA9vaOnSlqkp3e09J2eXjMKpwHPAb5nZlvNbAtwC7AfcKr0SE+PaJGe3tHSd3p6qvsIwMwOBw4C1ngkx4eZvd7dvy090tMLWqSnd7T0nZ5ODrIo+g94H7ARWAU8AJwUWXe79EhPL2iRnt7R0o96Sj2ZAgrjLmCf8PMC4DbgL8Pv3ZoBSXp6QE+VtEhP72jpRz09lRAPGPCwqeTuD5jZscC1ZvY8mDrhjfRIT4W1SE/vaOk7Pb3maH7MzBbVvoQFcyKwP/AS6ZGeHtEiPb2jpe/09JSj2cwOAna6+69i1h3j7rdKj/RUXYv09I6WftTTU0ZBCCFEZ+m17iMhhBAdREZBCCFEHRkFIZrAzCbN7A4z22Bm683sPDNLfY7MbIGZ/VFZGoVoBxkFIZpj3N0XufsRwGuBE4ALM36zAJBRED2BHM1CNIGZPenu+0S+Px/4KUE44POAfwH2Dle/191/aGZrgN8H7ge+AHwKWAEcC+wJ/L27/0NpJyFECjIKQjRBo1EIl20DFgK/A3a5+1NmdihwhbsvCQcXvd/dTwy3Pxt4rrt/xMz2BG4FTnH3+0s9GSFi6LURzUJUmSHg0+HAokngsITtXge81MzeGn7fFziUoCUhRFeRURCiDcLuo0ngcQLfwmPAkQT+uqeSfgb8hbuvLkWkEE0gR7MQLWJmc4HPAZ/2oB92X+BRd98F/AkwGG76O+BZkZ+uBt5tZkPhfg4zs70RogKopSBEcwyb2R0EXUU7CRzLHw/XfQb4ipm9Hfg2sD1cficwaWbrgcuBTxJEJN1uZgZsBpaVdQJCpCFHsxBCiDrqPhJCCFFHRkEIIUQdGQUhhBB1ZBSEEELUkVEQQghRR0ZBCCFEHRkFIYQQdWQUhBBC1Pn/t/EVwuHKdtAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot AvgHR over time for all virtual and outdoor bike rides\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot_date(df['Date'],df['AvgHR'])\n",
    "plt.title('AvgHR Over Time for Rides')\n",
    "plt.xlabel('Date')\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('AvgHR')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of High AvgHR is 52.75\n",
      "percentage of Low AvgHR is 47.25\n"
     ]
    }
   ],
   "source": [
    "# Calculate sample size of each class\n",
    "count_no_sub = len(df[df['AvgHR_bin']==1])\n",
    "count_sub = len(df[df['AvgHR_bin']==0])\n",
    "pct_of_no_sub = count_no_sub/(count_no_sub+count_sub)\n",
    "print(\"percentage of High AvgHR is\", '%.2f' %(pct_of_no_sub*100))\n",
    "pct_of_sub = count_sub/(count_no_sub+count_sub)\n",
    "print(\"percentage of Low AvgHR is\", '%.2f' %(pct_of_sub*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Frequency of Average HR')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot distribution of AvgHR for Rides and VirtualRides\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.crosstab(df.Type,df.AvgHR_bin).plot(kind='bar')\n",
    "plt.title('Average HR for Type')\n",
    "plt.xlabel('Type')\n",
    "plt.ylabel('Frequency of Average HR')\n",
    "#plt.savefig('purchase_avghr_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Distance (km)  Avg Pace (/km)  HRSS  Elevation Gain (m)\n",
      "0             62.5             113   144               589.0\n",
      "1             80.1             125   217               890.0\n",
      "2             35.2             116    87               314.0\n",
      "3             45.6             114   119               447.0\n",
      "4             41.1             120    96               454.0\n",
      "..             ...             ...   ...                 ...\n",
      "289           40.6             233    86               716.4\n",
      "290          124.3             159   391              2602.0\n",
      "291           53.4             151   187              1012.3\n",
      "292           53.4             152   169              1006.7\n",
      "293           40.3             170   115               794.6\n",
      "\n",
      "[294 rows x 4 columns]      AvgHR_bin\n",
      "0          1.0\n",
      "1          1.0\n",
      "2          1.0\n",
      "3          1.0\n",
      "4          1.0\n",
      "..         ...\n",
      "289        0.0\n",
      "290        0.0\n",
      "291        1.0\n",
      "292        1.0\n",
      "293        0.0\n",
      "\n",
      "[294 rows x 1 columns]\n",
      "[ True  True  True  True]\n",
      "[1 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Define train and test set, perform RFE\n",
    "window_size = 294\n",
    "# removed Calories as feature since p-value was 0.07 > 0.05 (from below Logit function), as recommended\n",
    "df_vars = ['Distance (km)', 'Avg Pace (/km)', 'HRSS', 'Elevation Gain (m)','AvgHR_bin']\n",
    "df_final = df[df_vars]\n",
    "df_final_vars=df_final.columns.values.tolist()\n",
    "y=df_final.AvgHR_bin\n",
    "X=[i for i in df_final_vars if i not in y]\n",
    "\n",
    "X = df_final.loc[:, df_final.columns != 'AvgHR_bin']\n",
    "y = df_final.loc[:, df_final.columns == 'AvgHR_bin']\n",
    "\n",
    "# Configure train and test sets\n",
    "X_train = X.iloc[window_size:]\n",
    "y_train = y.iloc[window_size:]\n",
    "\n",
    "X_test = X.iloc[:window_size]\n",
    "y_test = y.iloc[:window_size]\n",
    "print(X_test, y_test)\n",
    "\n",
    "# Perform RFE (recursive feature elimination) to determine ranking of features\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "rfe = RFE(logreg, 20)\n",
    "rfe = rfe.fit(X_train, y_train.values.ravel())\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.307462\n",
      "         Iterations 8\n",
      "                          Results: Logit\n",
      "==================================================================\n",
      "Model:               Logit            Pseudo R-squared: 0.555     \n",
      "Dependent Variable:  AvgHR_bin        AIC:              198.0117  \n",
      "Date:                2019-11-17 22:53 BIC:              212.9451  \n",
      "No. Observations:    309              Log-Likelihood:   -95.006   \n",
      "Df Model:            3                LL-Null:          -213.71   \n",
      "Df Residuals:        305              LLR p-value:      3.4432e-51\n",
      "Converged:           1.0000           Scale:            1.0000    \n",
      "No. Iterations:      8.0000                                       \n",
      "------------------------------------------------------------------\n",
      "                    Coef.  Std.Err.    z    P>|z|   [0.025  0.975]\n",
      "------------------------------------------------------------------\n",
      "Distance (km)      -0.1857   0.0271 -6.8416 0.0000 -0.2389 -0.1325\n",
      "Avg Pace (/km)     -0.0120   0.0029 -4.1900 0.0000 -0.0175 -0.0064\n",
      "HRSS                0.1395   0.0176  7.9317 0.0000  0.1050  0.1740\n",
      "Elevation Gain (m) -0.0107   0.0015 -7.2999 0.0000 -0.0136 -0.0078\n",
      "==================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Implement Logit model to determine p-values and coefficients for each feature\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "logit_model=sm.Logit(y,X)\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate Over All Window Sizes for LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Predictions:  1\n",
      "accuracy for window size 308: 1.0\n",
      "# Predictions:  2\n",
      "accuracy for window size 307: 1.0\n",
      "# Predictions:  3\n",
      "accuracy for window size 306: 1.0\n",
      "# Predictions:  4\n",
      "accuracy for window size 305: 1.0\n",
      "# Predictions:  5\n",
      "accuracy for window size 304: 0.8\n",
      "# Predictions:  6\n",
      "accuracy for window size 303: 0.6666666666666666\n",
      "# Predictions:  7\n",
      "accuracy for window size 302: 0.7142857142857143\n",
      "# Predictions:  8\n",
      "accuracy for window size 301: 0.5\n",
      "# Predictions:  9\n",
      "accuracy for window size 300: 0.3333333333333333\n",
      "# Predictions:  10\n",
      "accuracy for window size 299: 0.2\n",
      "# Predictions:  11\n",
      "accuracy for window size 298: 0.2727272727272727\n",
      "# Predictions:  12\n",
      "accuracy for window size 297: 0.5\n",
      "# Predictions:  13\n",
      "accuracy for window size 296: 0.6923076923076923\n",
      "# Predictions:  14\n",
      "accuracy for window size 295: 0.7142857142857143\n",
      "# Predictions:  15\n",
      "accuracy for window size 294: 0.7333333333333333\n",
      "# Predictions:  16\n",
      "accuracy for window size 293: 0.625\n",
      "# Predictions:  17\n",
      "accuracy for window size 292: 0.5294117647058824\n",
      "# Predictions:  18\n",
      "accuracy for window size 291: 0.5555555555555556\n",
      "# Predictions:  19\n",
      "accuracy for window size 290: 0.3684210526315789\n",
      "# Predictions:  20\n",
      "accuracy for window size 289: 0.35\n",
      "# Predictions:  21\n",
      "accuracy for window size 288: 0.5238095238095238\n",
      "# Predictions:  22\n",
      "accuracy for window size 287: 0.36363636363636365\n",
      "# Predictions:  23\n",
      "accuracy for window size 286: 0.43478260869565216\n",
      "# Predictions:  24\n",
      "accuracy for window size 285: 0.5416666666666666\n",
      "# Predictions:  25\n",
      "accuracy for window size 284: 0.32\n",
      "# Predictions:  26\n",
      "accuracy for window size 283: 0.34615384615384615\n",
      "# Predictions:  27\n",
      "accuracy for window size 282: 0.37037037037037035\n",
      "# Predictions:  28\n",
      "accuracy for window size 281: 0.25\n",
      "# Predictions:  29\n",
      "accuracy for window size 280: 0.3103448275862069\n",
      "# Predictions:  30\n",
      "accuracy for window size 279: 0.3333333333333333\n",
      "# Predictions:  31\n",
      "accuracy for window size 278: 0.45161290322580644\n",
      "# Predictions:  32\n",
      "accuracy for window size 277: 0.53125\n",
      "# Predictions:  33\n",
      "accuracy for window size 276: 0.45454545454545453\n",
      "# Predictions:  34\n",
      "accuracy for window size 275: 0.4117647058823529\n",
      "# Predictions:  35\n",
      "accuracy for window size 274: 0.4857142857142857\n",
      "# Predictions:  36\n",
      "accuracy for window size 273: 0.3055555555555556\n",
      "# Predictions:  37\n",
      "accuracy for window size 272: 0.3783783783783784\n",
      "# Predictions:  38\n",
      "accuracy for window size 271: 0.39473684210526316\n",
      "# Predictions:  39\n",
      "accuracy for window size 270: 0.3076923076923077\n",
      "# Predictions:  40\n",
      "accuracy for window size 269: 0.325\n",
      "# Predictions:  41\n",
      "accuracy for window size 268: 0.3902439024390244\n",
      "# Predictions:  42\n",
      "accuracy for window size 267: 0.35714285714285715\n",
      "# Predictions:  43\n",
      "accuracy for window size 266: 0.4186046511627907\n",
      "# Predictions:  44\n",
      "accuracy for window size 265: 0.38636363636363635\n",
      "# Predictions:  45\n",
      "accuracy for window size 264: 0.4444444444444444\n",
      "# Predictions:  46\n",
      "accuracy for window size 263: 0.3695652173913043\n",
      "# Predictions:  47\n",
      "accuracy for window size 262: 0.40425531914893614\n",
      "# Predictions:  48\n",
      "accuracy for window size 261: 0.4166666666666667\n",
      "# Predictions:  49\n",
      "accuracy for window size 260: 0.40816326530612246\n",
      "# Predictions:  50\n",
      "accuracy for window size 259: 0.52\n",
      "# Predictions:  51\n",
      "accuracy for window size 258: 0.49019607843137253\n",
      "# Predictions:  52\n",
      "accuracy for window size 257: 0.4807692307692308\n",
      "# Predictions:  53\n",
      "accuracy for window size 256: 0.5471698113207547\n",
      "# Predictions:  54\n",
      "accuracy for window size 255: 0.5370370370370371\n",
      "# Predictions:  55\n",
      "accuracy for window size 254: 0.5272727272727272\n",
      "# Predictions:  56\n",
      "accuracy for window size 253: 0.5357142857142857\n",
      "# Predictions:  57\n",
      "accuracy for window size 252: 0.49122807017543857\n",
      "# Predictions:  58\n",
      "accuracy for window size 251: 0.5172413793103449\n",
      "# Predictions:  59\n",
      "accuracy for window size 250: 0.5423728813559322\n",
      "# Predictions:  60\n",
      "accuracy for window size 249: 0.48333333333333334\n",
      "# Predictions:  61\n",
      "accuracy for window size 248: 0.45901639344262296\n",
      "# Predictions:  62\n",
      "accuracy for window size 247: 0.5\n",
      "# Predictions:  63\n",
      "accuracy for window size 246: 0.5714285714285714\n",
      "# Predictions:  64\n",
      "accuracy for window size 245: 0.578125\n",
      "# Predictions:  65\n",
      "accuracy for window size 244: 0.5846153846153846\n",
      "# Predictions:  66\n",
      "accuracy for window size 243: 0.5757575757575758\n",
      "# Predictions:  67\n",
      "accuracy for window size 242: 0.6268656716417911\n",
      "# Predictions:  68\n",
      "accuracy for window size 241: 0.5735294117647058\n",
      "# Predictions:  69\n",
      "accuracy for window size 240: 0.5797101449275363\n",
      "# Predictions:  70\n",
      "accuracy for window size 239: 0.6\n",
      "# Predictions:  71\n",
      "accuracy for window size 238: 0.6056338028169014\n",
      "# Predictions:  72\n",
      "accuracy for window size 237: 0.6666666666666666\n",
      "# Predictions:  73\n",
      "accuracy for window size 236: 0.6301369863013698\n",
      "# Predictions:  74\n",
      "accuracy for window size 235: 0.6351351351351351\n",
      "# Predictions:  75\n",
      "accuracy for window size 234: 0.64\n",
      "# Predictions:  76\n",
      "accuracy for window size 233: 0.5921052631578947\n",
      "# Predictions:  77\n",
      "accuracy for window size 232: 0.6233766233766234\n",
      "# Predictions:  78\n",
      "accuracy for window size 231: 0.6538461538461539\n",
      "# Predictions:  79\n",
      "accuracy for window size 230: 0.6582278481012658\n",
      "# Predictions:  80\n",
      "accuracy for window size 229: 0.6625\n",
      "# Predictions:  81\n",
      "accuracy for window size 228: 0.5925925925925926\n",
      "# Predictions:  82\n",
      "accuracy for window size 227: 0.6463414634146342\n",
      "# Predictions:  83\n",
      "accuracy for window size 226: 0.7469879518072289\n",
      "# Predictions:  84\n",
      "accuracy for window size 225: 0.6666666666666666\n",
      "# Predictions:  85\n",
      "accuracy for window size 224: 0.6823529411764706\n",
      "# Predictions:  86\n",
      "accuracy for window size 223: 0.8023255813953488\n",
      "# Predictions:  87\n",
      "accuracy for window size 222: 0.6666666666666666\n",
      "# Predictions:  88\n",
      "accuracy for window size 221: 0.6931818181818182\n",
      "# Predictions:  89\n",
      "accuracy for window size 220: 0.7415730337078652\n",
      "# Predictions:  90\n",
      "accuracy for window size 219: 0.6777777777777778\n",
      "# Predictions:  91\n",
      "accuracy for window size 218: 0.7692307692307693\n",
      "# Predictions:  92\n",
      "accuracy for window size 217: 0.6521739130434783\n",
      "# Predictions:  93\n",
      "accuracy for window size 216: 0.6559139784946236\n",
      "# Predictions:  94\n",
      "accuracy for window size 215: 0.6808510638297872\n",
      "# Predictions:  95\n",
      "accuracy for window size 214: 0.6210526315789474\n",
      "# Predictions:  96\n",
      "accuracy for window size 213: 0.6770833333333334\n",
      "# Predictions:  97\n",
      "accuracy for window size 212: 0.6907216494845361\n",
      "# Predictions:  98\n",
      "accuracy for window size 211: 0.7346938775510204\n",
      "# Predictions:  99\n",
      "accuracy for window size 210: 0.7272727272727273\n",
      "# Predictions:  100\n",
      "accuracy for window size 209: 0.63\n",
      "# Predictions:  101\n",
      "accuracy for window size 208: 0.6732673267326733\n",
      "# Predictions:  102\n",
      "accuracy for window size 207: 0.6372549019607843\n",
      "# Predictions:  103\n",
      "accuracy for window size 206: 0.6407766990291263\n",
      "# Predictions:  104\n",
      "accuracy for window size 205: 0.6442307692307693\n",
      "# Predictions:  105\n",
      "accuracy for window size 204: 0.7238095238095238\n",
      "# Predictions:  106\n",
      "accuracy for window size 203: 0.6886792452830188\n",
      "# Predictions:  107\n",
      "accuracy for window size 202: 0.616822429906542\n",
      "# Predictions:  108\n",
      "accuracy for window size 201: 0.5648148148148148\n",
      "# Predictions:  109\n",
      "accuracy for window size 200: 0.5688073394495413\n",
      "# Predictions:  110\n",
      "accuracy for window size 199: 0.5\n",
      "# Predictions:  111\n",
      "accuracy for window size 198: 0.5945945945945946\n",
      "# Predictions:  112\n",
      "accuracy for window size 197: 0.5446428571428571\n",
      "# Predictions:  113\n",
      "accuracy for window size 196: 0.584070796460177\n",
      "# Predictions:  114\n",
      "accuracy for window size 195: 0.5964912280701754\n",
      "# Predictions:  115\n",
      "accuracy for window size 194: 0.5652173913043478\n",
      "# Predictions:  116\n",
      "accuracy for window size 193: 0.5862068965517241\n",
      "# Predictions:  117\n",
      "accuracy for window size 192: 0.5213675213675214\n",
      "# Predictions:  118\n",
      "accuracy for window size 191: 0.5254237288135594\n",
      "# Predictions:  119\n",
      "accuracy for window size 190: 0.4957983193277311\n",
      "# Predictions:  120\n",
      "accuracy for window size 189: 0.5166666666666667\n",
      "# Predictions:  121\n",
      "accuracy for window size 188: 0.5537190082644629\n",
      "# Predictions:  122\n",
      "accuracy for window size 187: 0.47540983606557374\n",
      "# Predictions:  123\n",
      "accuracy for window size 186: 0.5365853658536586\n",
      "# Predictions:  124\n",
      "accuracy for window size 185: 0.49193548387096775\n",
      "# Predictions:  125\n",
      "accuracy for window size 184: 0.448\n",
      "# Predictions:  126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for window size 183: 0.4603174603174603\n",
      "# Predictions:  127\n",
      "accuracy for window size 182: 0.4015748031496063\n",
      "# Predictions:  128\n",
      "accuracy for window size 181: 0.390625\n",
      "# Predictions:  129\n",
      "accuracy for window size 180: 0.4496124031007752\n",
      "# Predictions:  130\n",
      "accuracy for window size 179: 0.4076923076923077\n",
      "# Predictions:  131\n",
      "accuracy for window size 178: 0.4580152671755725\n",
      "# Predictions:  132\n",
      "accuracy for window size 177: 0.44696969696969696\n",
      "# Predictions:  133\n",
      "accuracy for window size 176: 0.46616541353383456\n",
      "# Predictions:  134\n",
      "accuracy for window size 175: 0.4253731343283582\n",
      "# Predictions:  135\n",
      "accuracy for window size 174: 0.35555555555555557\n",
      "# Predictions:  136\n",
      "accuracy for window size 173: 0.36764705882352944\n",
      "# Predictions:  137\n",
      "accuracy for window size 172: 0.3722627737226277\n",
      "# Predictions:  138\n",
      "accuracy for window size 171: 0.34782608695652173\n",
      "# Predictions:  139\n",
      "accuracy for window size 170: 0.381294964028777\n",
      "# Predictions:  140\n",
      "accuracy for window size 169: 0.4\n",
      "# Predictions:  141\n",
      "accuracy for window size 168: 0.4326241134751773\n",
      "# Predictions:  142\n",
      "accuracy for window size 167: 0.38028169014084506\n",
      "# Predictions:  143\n",
      "accuracy for window size 166: 0.32867132867132864\n",
      "# Predictions:  144\n",
      "accuracy for window size 165: 0.3611111111111111\n",
      "# Predictions:  145\n",
      "accuracy for window size 164: 0.35172413793103446\n",
      "# Predictions:  146\n",
      "accuracy for window size 163: 0.3493150684931507\n",
      "# Predictions:  147\n",
      "accuracy for window size 162: 0.38095238095238093\n",
      "# Predictions:  148\n",
      "accuracy for window size 161: 0.3310810810810811\n",
      "# Predictions:  149\n",
      "accuracy for window size 160: 0.37583892617449666\n",
      "# Predictions:  150\n",
      "accuracy for window size 159: 0.44666666666666666\n",
      "# Predictions:  151\n",
      "accuracy for window size 158: 0.3576158940397351\n",
      "# Predictions:  152\n",
      "accuracy for window size 157: 0.42105263157894735\n",
      "# Predictions:  153\n",
      "accuracy for window size 156: 0.39869281045751637\n",
      "# Predictions:  154\n",
      "accuracy for window size 155: 0.33766233766233766\n",
      "# Predictions:  155\n",
      "accuracy for window size 154: 0.36774193548387096\n",
      "# Predictions:  156\n",
      "accuracy for window size 153: 0.33974358974358976\n",
      "# Predictions:  157\n",
      "accuracy for window size 152: 0.3503184713375796\n",
      "# Predictions:  158\n",
      "accuracy for window size 151: 0.34177215189873417\n",
      "# Predictions:  159\n",
      "accuracy for window size 150: 0.39622641509433965\n",
      "# Predictions:  160\n",
      "accuracy for window size 149: 0.325\n",
      "# Predictions:  161\n",
      "accuracy for window size 148: 0.391304347826087\n",
      "# Predictions:  162\n",
      "accuracy for window size 147: 0.3950617283950617\n",
      "# Predictions:  163\n",
      "accuracy for window size 146: 0.3374233128834356\n",
      "# Predictions:  164\n",
      "accuracy for window size 145: 0.42073170731707316\n",
      "# Predictions:  165\n",
      "accuracy for window size 144: 0.4\n",
      "# Predictions:  166\n",
      "accuracy for window size 143: 0.35542168674698793\n",
      "# Predictions:  167\n",
      "accuracy for window size 142: 0.41916167664670656\n",
      "# Predictions:  168\n",
      "accuracy for window size 141: 0.44047619047619047\n",
      "# Predictions:  169\n",
      "accuracy for window size 140: 0.41420118343195267\n",
      "# Predictions:  170\n",
      "accuracy for window size 139: 0.35294117647058826\n",
      "# Predictions:  171\n",
      "accuracy for window size 138: 0.4502923976608187\n",
      "# Predictions:  172\n",
      "accuracy for window size 137: 0.4069767441860465\n",
      "# Predictions:  173\n",
      "accuracy for window size 136: 0.3872832369942196\n",
      "# Predictions:  174\n",
      "accuracy for window size 135: 0.4540229885057471\n",
      "# Predictions:  175\n",
      "accuracy for window size 134: 0.45714285714285713\n",
      "# Predictions:  176\n",
      "accuracy for window size 133: 0.3465909090909091\n",
      "# Predictions:  177\n",
      "accuracy for window size 132: 0.4180790960451977\n",
      "# Predictions:  178\n",
      "accuracy for window size 131: 0.37640449438202245\n",
      "# Predictions:  179\n",
      "accuracy for window size 130: 0.4581005586592179\n",
      "# Predictions:  180\n",
      "accuracy for window size 129: 0.4388888888888889\n",
      "# Predictions:  181\n",
      "accuracy for window size 128: 0.40331491712707185\n",
      "# Predictions:  182\n",
      "accuracy for window size 127: 0.4725274725274725\n",
      "# Predictions:  183\n",
      "accuracy for window size 126: 0.40437158469945356\n",
      "# Predictions:  184\n",
      "accuracy for window size 125: 0.3858695652173913\n",
      "# Predictions:  185\n",
      "accuracy for window size 124: 0.43243243243243246\n",
      "# Predictions:  186\n",
      "accuracy for window size 123: 0.4838709677419355\n",
      "# Predictions:  187\n",
      "accuracy for window size 122: 0.5133689839572193\n",
      "# Predictions:  188\n",
      "accuracy for window size 121: 0.5053191489361702\n",
      "# Predictions:  189\n",
      "accuracy for window size 120: 0.4656084656084656\n",
      "# Predictions:  190\n",
      "accuracy for window size 119: 0.4473684210526316\n",
      "# Predictions:  191\n",
      "accuracy for window size 118: 0.41361256544502617\n",
      "# Predictions:  192\n",
      "accuracy for window size 117: 0.4479166666666667\n",
      "# Predictions:  193\n",
      "accuracy for window size 116: 0.45077720207253885\n",
      "# Predictions:  194\n",
      "accuracy for window size 115: 0.5154639175257731\n",
      "# Predictions:  195\n",
      "accuracy for window size 114: 0.46153846153846156\n",
      "# Predictions:  196\n",
      "accuracy for window size 113: 0.44387755102040816\n",
      "# Predictions:  197\n",
      "accuracy for window size 112: 0.45685279187817257\n",
      "# Predictions:  198\n",
      "accuracy for window size 111: 0.4444444444444444\n",
      "# Predictions:  199\n",
      "accuracy for window size 110: 0.4623115577889447\n",
      "# Predictions:  200\n",
      "accuracy for window size 109: 0.445\n",
      "# Predictions:  201\n",
      "accuracy for window size 108: 0.4925373134328358\n",
      "# Predictions:  202\n",
      "accuracy for window size 107: 0.504950495049505\n",
      "# Predictions:  203\n",
      "accuracy for window size 106: 0.47783251231527096\n",
      "# Predictions:  204\n",
      "accuracy for window size 105: 0.49019607843137253\n",
      "# Predictions:  205\n",
      "accuracy for window size 104: 0.5121951219512195\n",
      "# Predictions:  206\n",
      "accuracy for window size 103: 0.48058252427184467\n",
      "# Predictions:  207\n",
      "accuracy for window size 102: 0.5314009661835749\n",
      "# Predictions:  208\n",
      "accuracy for window size 101: 0.4855769230769231\n",
      "# Predictions:  209\n",
      "accuracy for window size 100: 0.507177033492823\n",
      "# Predictions:  210\n",
      "accuracy for window size 99: 0.42857142857142855\n",
      "# Predictions:  211\n",
      "accuracy for window size 98: 0.4881516587677725\n",
      "# Predictions:  212\n",
      "accuracy for window size 97: 0.4528301886792453\n",
      "# Predictions:  213\n",
      "accuracy for window size 96: 0.5070422535211268\n",
      "# Predictions:  214\n",
      "accuracy for window size 95: 0.5280373831775701\n",
      "# Predictions:  215\n",
      "accuracy for window size 94: 0.5162790697674419\n",
      "# Predictions:  216\n",
      "accuracy for window size 93: 0.5\n",
      "# Predictions:  217\n",
      "accuracy for window size 92: 0.48847926267281105\n",
      "# Predictions:  218\n",
      "accuracy for window size 91: 0.47706422018348627\n",
      "# Predictions:  219\n",
      "accuracy for window size 90: 0.502283105022831\n",
      "# Predictions:  220\n",
      "accuracy for window size 89: 0.4636363636363636\n",
      "# Predictions:  221\n",
      "accuracy for window size 88: 0.5203619909502263\n",
      "# Predictions:  222\n",
      "accuracy for window size 87: 0.4864864864864865\n",
      "# Predictions:  223\n",
      "accuracy for window size 86: 0.515695067264574\n",
      "# Predictions:  224\n",
      "accuracy for window size 85: 0.4732142857142857\n",
      "# Predictions:  225\n",
      "accuracy for window size 84: 0.52\n",
      "# Predictions:  226\n",
      "accuracy for window size 83: 0.5176991150442478\n",
      "# Predictions:  227\n",
      "accuracy for window size 82: 0.4977973568281938\n",
      "# Predictions:  228\n",
      "accuracy for window size 81: 0.4868421052631579\n",
      "# Predictions:  229\n",
      "accuracy for window size 80: 0.5021834061135371\n",
      "# Predictions:  230\n",
      "accuracy for window size 79: 0.508695652173913\n",
      "# Predictions:  231\n",
      "accuracy for window size 78: 0.5064935064935064\n",
      "# Predictions:  232\n",
      "accuracy for window size 77: 0.5043103448275862\n",
      "# Predictions:  233\n",
      "accuracy for window size 76: 0.4721030042918455\n",
      "# Predictions:  234\n",
      "accuracy for window size 75: 0.5769230769230769\n",
      "# Predictions:  235\n",
      "accuracy for window size 74: 0.46808510638297873\n",
      "# Predictions:  236\n",
      "accuracy for window size 73: 0.4872881355932203\n",
      "# Predictions:  237\n",
      "accuracy for window size 72: 0.5021097046413502\n",
      "# Predictions:  238\n",
      "accuracy for window size 71: 0.49159663865546216\n",
      "# Predictions:  239\n",
      "accuracy for window size 70: 0.4895397489539749\n",
      "# Predictions:  240\n",
      "accuracy for window size 69: 0.5208333333333334\n",
      "# Predictions:  241\n",
      "accuracy for window size 68: 0.5684647302904564\n",
      "# Predictions:  242\n",
      "accuracy for window size 67: 0.4834710743801653\n",
      "# Predictions:  243\n",
      "accuracy for window size 66: 0.4732510288065844\n",
      "# Predictions:  244\n",
      "accuracy for window size 65: 0.4672131147540984\n",
      "# Predictions:  245\n",
      "accuracy for window size 64: 0.45714285714285713\n",
      "# Predictions:  246\n",
      "accuracy for window size 63: 0.5040650406504065\n",
      "# Predictions:  247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for window size 62: 0.46963562753036436\n",
      "# Predictions:  248\n",
      "accuracy for window size 61: 0.5040322580645161\n",
      "# Predictions:  249\n",
      "accuracy for window size 60: 0.5461847389558233\n",
      "# Predictions:  250\n",
      "accuracy for window size 59: 0.524\n",
      "# Predictions:  251\n",
      "accuracy for window size 58: 0.42231075697211157\n",
      "# Predictions:  252\n",
      "accuracy for window size 57: 0.44841269841269843\n",
      "# Predictions:  253\n",
      "accuracy for window size 56: 0.5019762845849802\n",
      "# Predictions:  254\n",
      "accuracy for window size 55: 0.468503937007874\n",
      "# Predictions:  255\n",
      "accuracy for window size 54: 0.4666666666666667\n",
      "# Predictions:  256\n",
      "accuracy for window size 53: 0.51953125\n",
      "# Predictions:  257\n",
      "accuracy for window size 52: 0.5058365758754864\n",
      "# Predictions:  258\n",
      "accuracy for window size 51: 0.4689922480620155\n",
      "# Predictions:  259\n",
      "accuracy for window size 50: 0.5096525096525096\n",
      "# Predictions:  260\n",
      "accuracy for window size 49: 0.5230769230769231\n",
      "# Predictions:  261\n",
      "accuracy for window size 48: 0.5210727969348659\n",
      "# Predictions:  262\n",
      "accuracy for window size 47: 0.4541984732824427\n",
      "# Predictions:  263\n",
      "accuracy for window size 46: 0.4524714828897338\n",
      "# Predictions:  264\n",
      "accuracy for window size 45: 0.5227272727272727\n",
      "# Predictions:  265\n",
      "accuracy for window size 44: 0.47924528301886793\n",
      "# Predictions:  266\n",
      "accuracy for window size 43: 0.4473684210526316\n",
      "# Predictions:  267\n",
      "accuracy for window size 42: 0.5131086142322098\n",
      "# Predictions:  268\n",
      "accuracy for window size 41: 0.4925373134328358\n",
      "# Predictions:  269\n",
      "accuracy for window size 40: 0.45353159851301117\n",
      "# Predictions:  270\n",
      "accuracy for window size 39: 0.4888888888888889\n",
      "# Predictions:  271\n",
      "accuracy for window size 38: 0.47601476014760147\n",
      "# Predictions:  272\n",
      "accuracy for window size 37: 0.4852941176470588\n",
      "# Predictions:  273\n",
      "accuracy for window size 36: 0.4835164835164835\n",
      "# Predictions:  274\n",
      "accuracy for window size 35: 0.4343065693430657\n",
      "# Predictions:  275\n",
      "accuracy for window size 34: 0.43636363636363634\n",
      "# Predictions:  276\n",
      "accuracy for window size 33: 0.4927536231884058\n",
      "Accuracies:  [1.         1.         1.         1.         0.8        0.66666667\n",
      " 0.71428571 0.5        0.33333333 0.2        0.27272727 0.5\n",
      " 0.69230769 0.71428571 0.73333333 0.625      0.52941176 0.55555556\n",
      " 0.36842105 0.35       0.52380952 0.36363636 0.43478261 0.54166667\n",
      " 0.32       0.34615385 0.37037037 0.25       0.31034483 0.33333333\n",
      " 0.4516129  0.53125    0.45454545 0.41176471 0.48571429 0.30555556\n",
      " 0.37837838 0.39473684 0.30769231 0.325      0.3902439  0.35714286\n",
      " 0.41860465 0.38636364 0.44444444 0.36956522 0.40425532 0.41666667\n",
      " 0.40816327 0.52       0.49019608 0.48076923 0.54716981 0.53703704\n",
      " 0.52727273 0.53571429 0.49122807 0.51724138 0.54237288 0.48333333\n",
      " 0.45901639 0.5        0.57142857 0.578125   0.58461538 0.57575758\n",
      " 0.62686567 0.57352941 0.57971014 0.6        0.6056338  0.66666667\n",
      " 0.63013699 0.63513514 0.64       0.59210526 0.62337662 0.65384615\n",
      " 0.65822785 0.6625     0.59259259 0.64634146 0.74698795 0.66666667\n",
      " 0.68235294 0.80232558 0.66666667 0.69318182 0.74157303 0.67777778\n",
      " 0.76923077 0.65217391 0.65591398 0.68085106 0.62105263 0.67708333\n",
      " 0.69072165 0.73469388 0.72727273 0.63       0.67326733 0.6372549\n",
      " 0.6407767  0.64423077 0.72380952 0.68867925 0.61682243 0.56481481\n",
      " 0.56880734 0.5        0.59459459 0.54464286 0.5840708  0.59649123\n",
      " 0.56521739 0.5862069  0.52136752 0.52542373 0.49579832 0.51666667\n",
      " 0.55371901 0.47540984 0.53658537 0.49193548 0.448      0.46031746\n",
      " 0.4015748  0.390625   0.4496124  0.40769231 0.45801527 0.4469697\n",
      " 0.46616541 0.42537313 0.35555556 0.36764706 0.37226277 0.34782609\n",
      " 0.38129496 0.4        0.43262411 0.38028169 0.32867133 0.36111111\n",
      " 0.35172414 0.34931507 0.38095238 0.33108108 0.37583893 0.44666667\n",
      " 0.35761589 0.42105263 0.39869281 0.33766234 0.36774194 0.33974359\n",
      " 0.35031847 0.34177215 0.39622642 0.325      0.39130435 0.39506173\n",
      " 0.33742331 0.42073171 0.4        0.35542169 0.41916168 0.44047619\n",
      " 0.41420118 0.35294118 0.4502924  0.40697674 0.38728324 0.45402299\n",
      " 0.45714286 0.34659091 0.4180791  0.37640449 0.45810056 0.43888889\n",
      " 0.40331492 0.47252747 0.40437158 0.38586957 0.43243243 0.48387097\n",
      " 0.51336898 0.50531915 0.46560847 0.44736842 0.41361257 0.44791667\n",
      " 0.4507772  0.51546392 0.46153846 0.44387755 0.45685279 0.44444444\n",
      " 0.46231156 0.445      0.49253731 0.5049505  0.47783251 0.49019608\n",
      " 0.51219512 0.48058252 0.53140097 0.48557692 0.50717703 0.42857143\n",
      " 0.48815166 0.45283019 0.50704225 0.52803738 0.51627907 0.5\n",
      " 0.48847926 0.47706422 0.50228311 0.46363636 0.52036199 0.48648649\n",
      " 0.51569507 0.47321429 0.52       0.51769912 0.49779736 0.48684211\n",
      " 0.50218341 0.50869565 0.50649351 0.50431034 0.472103   0.57692308\n",
      " 0.46808511 0.48728814 0.5021097  0.49159664 0.48953975 0.52083333\n",
      " 0.56846473 0.48347107 0.47325103 0.46721311 0.45714286 0.50406504\n",
      " 0.46963563 0.50403226 0.54618474 0.524      0.42231076 0.4484127\n",
      " 0.50197628 0.46850394 0.46666667 0.51953125 0.50583658 0.46899225\n",
      " 0.50965251 0.52307692 0.5210728  0.45419847 0.45247148 0.52272727\n",
      " 0.47924528 0.44736842 0.51310861 0.49253731 0.4535316  0.48888889\n",
      " 0.47601476 0.48529412 0.48351648 0.43430657 0.43636364 0.49275362]\n"
     ]
    }
   ],
   "source": [
    "# Iterate over all window sizes to determine optimal size\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Change window_size_optimization flag to perform iterations\n",
    "window_size_optimization = True\n",
    "\n",
    "if window_size_optimization == True:\n",
    "\n",
    "    logreg = LogisticRegression()\n",
    "    accuracies = np.zeros(276)\n",
    "    #298 accuracy indices\n",
    "\n",
    "    # Iterate over all window sizes and compute accuracy for each LR model\n",
    "    for window_size in range(1,277):\n",
    "    #window_size = 299\n",
    "        print('# Predictions: ', window_size)\n",
    "        X_train = X.iloc[window_size:]\n",
    "        y_train = y.iloc[window_size:]\n",
    "        # print('train labels: ', y_train)\n",
    "        X_test = X.iloc[:window_size]\n",
    "        y_test = y.iloc[:window_size]\n",
    "        # print('test labels: ', y_test)\n",
    "\n",
    "        actuals = pd.DataFrame(y_test)\n",
    "        actuals = actuals.rename(columns={'AvgHR_bin':'Actuals'})\n",
    "        # print('actuals: \\n', actuals)\n",
    "        preds = np.zeros(X_test.shape[0])\n",
    "        # print('X_test: \\n', X_test)\n",
    "        for i in range(0,y_test.shape[0]):\n",
    "            # print('X train shape: ', X_train.shape[0])\n",
    "            # print('X train: ', X_train.head())\n",
    "            logreg.fit(X_train, y_train.values.ravel())\n",
    "            # Predict test set\n",
    "            # print('X test: ', X_test.iloc[0])\n",
    "            y_pred = logreg.predict(np.array(X_test.iloc[-1]).reshape(1,-1))\n",
    "            # print('actual: ',y_test.loc[0, 'AvgHR_bin'], '\\n pred: ',y_pred, '\\n')\n",
    "            preds[i] = y_pred\n",
    "            #print('Accuracy of logistic regression classifier on test set {}: {:.2f}'.format(i, logreg.score(X_test, y_test)))\n",
    "            #print(\"X test -1: \", X_test.iloc[-1])\n",
    "            \n",
    "            X_test_inst = pd.DataFrame(data= [X_test.iloc[-1]],columns=[\"Distance (km)\",\"Avg Pace (/km)\", \"HRSS\", \"Elevation Gain (m)\"])\n",
    "            y_test_inst = pd.DataFrame(data = [y_test.iloc[-1]],columns=[\"AvgHR_bin\"])\n",
    "            #print(\"X test inst: \", X_test_inst)\n",
    "            X_train = X_train.drop(X_train.index[-1]).reset_index(drop=True)\n",
    "            X_train = pd.concat([X_test_inst,X_train]).reset_index(drop = True)\n",
    "            #print(\"X train: \\n\", X_train)\n",
    "\n",
    "            y_train = y_train.drop(y_train.index[-1])\n",
    "            y_train = pd.concat([y_test_inst,y_train])\n",
    "            y_train = y_train.reset_index(drop=True)\n",
    "            #print(\"y train \\n\", y_train)\n",
    "\n",
    "            X_test = X_test.drop(X_test.index[-1])\n",
    "            X_test = X_test.reset_index(drop=True)\n",
    "            #print(\"X test: \\n\", X_test)\n",
    "            y_test = y_test.drop(y_test.index[-1])\n",
    "            y_test = y_test.reset_index(drop=True)\n",
    "            \n",
    "        preds_act_df = pd.DataFrame(preds, columns=['Predictions'])\n",
    "        preds_act_df = preds_act_df.join(actuals)\n",
    "        # print('actuals and preds: \\n', preds_act_df)\n",
    "        accuracy = metrics.accuracy_score(preds_act_df.Actuals.ravel(),preds_act_df.Predictions.ravel())\n",
    "        print('accuracy for window size {}: {}'.format(309-window_size, accuracy))\n",
    "        accuracies[window_size-1] = accuracy\n",
    "    print('Accuracies: ', accuracies)\n",
    "    # Output accuracies for each window size to Accuracies_for_Window_Size_Variations.csv file \n",
    "    with open('Accuracies_for_Window_Size_Variations.csv', 'w') as f:\n",
    "        for i in range(0,len(accuracies)):\n",
    "            f.write(str(308-i) + ': ' + str(accuracies[i]))\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Window              Accuracy\n",
      "0      308                   1.0\n",
      "2      306                   1.0\n",
      "3      305                   1.0\n",
      "1      307                   1.0\n",
      "85     223    0.8023255813953488\n",
      "4      304                   0.8\n",
      "90     218    0.7692307692307693\n",
      "82     226    0.7469879518072289\n",
      "88     220    0.7415730337078652\n",
      "97     211    0.7346938775510204\n",
      "14     294    0.7333333333333333\n",
      "98     210    0.7272727272727273\n",
      "104    204    0.7238095238095238\n",
      "13     295    0.7142857142857143\n",
      "6      302    0.7142857142857143\n",
      "87     221    0.6931818181818182\n",
      "12     296    0.6923076923076923\n",
      "96     212    0.6907216494845361\n",
      "105    203    0.6886792452830188\n",
      "84     224    0.6823529411764706\n",
      "93     215    0.6808510638297872\n",
      "89     219    0.6777777777777778\n",
      "95     213    0.6770833333333334\n",
      "100    208    0.6732673267326733\n",
      "83     225    0.6666666666666666\n",
      "71     237    0.6666666666666666\n",
      "5      303    0.6666666666666666\n",
      "86     222    0.6666666666666666\n",
      "79     229                0.6625\n",
      "78     230    0.6582278481012658\n",
      "92     216    0.6559139784946236\n",
      "77     231    0.6538461538461539\n",
      "91     217    0.6521739130434783\n",
      "81     227    0.6463414634146342\n",
      "103    205    0.6442307692307693\n",
      "102    206    0.6407766990291263\n",
      "74     234                  0.64\n",
      "101    207    0.6372549019607843\n",
      "73     235    0.6351351351351351\n",
      "72     236    0.6301369863013698\n",
      "99     209                  0.63\n",
      "66     242    0.6268656716417911\n",
      "15     293                 0.625\n",
      "76     232    0.6233766233766234\n",
      "94     214    0.6210526315789474\n",
      "106    202     0.616822429906542\n",
      "70     238    0.6056338028169014\n",
      "69     239                   0.6\n",
      "113    195    0.5964912280701754\n",
      "110    198    0.5945945945945946\n",
      "80     228    0.5925925925925926\n",
      "75     233    0.5921052631578947\n",
      "115    193    0.5862068965517241\n",
      "64     244    0.5846153846153846\n",
      "112    196     0.584070796460177\n",
      "68     240    0.5797101449275363\n",
      "63     245              0.578125\n",
      "233     75    0.5769230769230769\n",
      "65     243    0.5757575757575758\n",
      "67     241    0.5735294117647058\n",
      "62     246    0.5714285714285714\n",
      "108    200    0.5688073394495413\n",
      "240     68    0.5684647302904564\n",
      "114    194    0.5652173913043478\n",
      "107    201    0.5648148148148148\n",
      "17     291    0.5555555555555556\n",
      "120    188    0.5537190082644629\n",
      "52     256    0.5471698113207547\n",
      "248     60    0.5461847389558233\n",
      "111    197    0.5446428571428571\n",
      "58     250    0.5423728813559322\n",
      "23     285    0.5416666666666666\n",
      "53     255    0.5370370370370371\n",
      "122    186    0.5365853658536586\n",
      "55     253    0.5357142857142857\n",
      "206    102    0.5314009661835749\n",
      "31     277               0.53125\n",
      "16     292    0.5294117647058824\n",
      "213     95    0.5280373831775701\n",
      "54     254    0.5272727272727272\n",
      "117    191    0.5254237288135594\n",
      "249     59                 0.524\n",
      "20     288    0.5238095238095238\n",
      "259     49    0.5230769230769231\n",
      "263     45    0.5227272727272727\n",
      "116    192    0.5213675213675214\n",
      "260     48    0.5210727969348659\n",
      "239     69    0.5208333333333334\n",
      "220     88    0.5203619909502263\n",
      "49     259                  0.52\n",
      "224     84                  0.52\n",
      "255     53            0.51953125\n",
      "225     83    0.5176991150442478\n",
      "57     251    0.5172413793103449\n",
      "119    189    0.5166666666666667\n",
      "214     94    0.5162790697674419\n",
      "222     86     0.515695067264574\n",
      "193    115    0.5154639175257731\n",
      "186    122    0.5133689839572193\n",
      "266     42    0.5131086142322098\n",
      "204    104    0.5121951219512195\n",
      "258     50    0.5096525096525096\n",
      "229     79     0.508695652173913\n",
      "208    100     0.507177033492823\n",
      "212     96    0.5070422535211268\n",
      "230     78    0.5064935064935064\n",
      "256     52    0.5058365758754864\n",
      "187    121    0.5053191489361702\n",
      "201    107     0.504950495049505\n",
      "231     77    0.5043103448275862\n",
      "245     63    0.5040650406504065\n",
      "247     61    0.5040322580645161\n",
      "218     90     0.502283105022831\n",
      "228     80    0.5021834061135371\n",
      "236     72    0.5021097046413502\n",
      "252     56    0.5019762845849802\n",
      "11     297                   0.5\n",
      "109    199                   0.5\n",
      "7      301                   0.5\n",
      "215     93                   0.5\n",
      "61     247                   0.5\n",
      "226     82    0.4977973568281938\n",
      "118    190    0.4957983193277311\n",
      "275     33    0.4927536231884058\n",
      "200    108    0.4925373134328358\n",
      "267     41    0.4925373134328358\n",
      "123    185   0.49193548387096775\n",
      "237     71   0.49159663865546216\n",
      "56     252   0.49122807017543857\n",
      "50     258   0.49019607843137253\n",
      "203    105   0.49019607843137253\n",
      "238     70    0.4895397489539749\n",
      "269     39    0.4888888888888889\n",
      "216     92   0.48847926267281105\n",
      "210     98    0.4881516587677725\n",
      "235     73    0.4872881355932203\n",
      "227     81    0.4868421052631579\n",
      "221     87    0.4864864864864865\n",
      "34     274    0.4857142857142857\n",
      "207    101    0.4855769230769231\n",
      "271     37    0.4852941176470588\n",
      "185    123    0.4838709677419355\n",
      "272     36    0.4835164835164835\n",
      "241     67    0.4834710743801653\n",
      "59     249   0.48333333333333334\n",
      "51     257    0.4807692307692308\n",
      "205    103   0.48058252427184467\n",
      "264     44   0.47924528301886793\n",
      "202    106   0.47783251231527096\n",
      "217     91   0.47706422018348627\n",
      "270     38   0.47601476014760147\n",
      "121    187   0.47540983606557374\n",
      "242     66    0.4732510288065844\n",
      "223     85    0.4732142857142857\n",
      "181    127    0.4725274725274725\n",
      "232     76    0.4721030042918455\n",
      "246     62   0.46963562753036436\n",
      "257     51    0.4689922480620155\n",
      "253     55     0.468503937007874\n",
      "234     74   0.46808510638297873\n",
      "243     65    0.4672131147540984\n",
      "254     54    0.4666666666666667\n",
      "132    176   0.46616541353383456\n",
      "188    120    0.4656084656084656\n",
      "219     89    0.4636363636363636\n",
      "198    110    0.4623115577889447\n",
      "194    114   0.46153846153846156\n",
      "125    183    0.4603174603174603\n",
      "60     248   0.45901639344262296\n",
      "178    130    0.4581005586592179\n",
      "130    178    0.4580152671755725\n",
      "244     64   0.45714285714285713\n",
      "174    134   0.45714285714285713\n",
      "196    112   0.45685279187817257\n",
      "32     276   0.45454545454545453\n",
      "261     47    0.4541984732824427\n",
      "173    135    0.4540229885057471\n",
      "268     40   0.45353159851301117\n",
      "211     97    0.4528301886792453\n",
      "262     46    0.4524714828897338\n",
      "30     278   0.45161290322580644\n",
      "192    116   0.45077720207253885\n",
      "170    138    0.4502923976608187\n",
      "128    180    0.4496124031007752\n",
      "251     57   0.44841269841269843\n",
      "124    184                 0.448\n",
      "191    117    0.4479166666666667\n",
      "189    119    0.4473684210526316\n",
      "265     43    0.4473684210526316\n",
      "131    177   0.44696969696969696\n",
      "149    159   0.44666666666666666\n",
      "199    109                 0.445\n",
      "197    111    0.4444444444444444\n",
      "44     264    0.4444444444444444\n",
      "195    113   0.44387755102040816\n",
      "167    141   0.44047619047619047\n",
      "179    129    0.4388888888888889\n",
      "274     34   0.43636363636363634\n",
      "22     286   0.43478260869565216\n",
      "273     35    0.4343065693430657\n",
      "140    168    0.4326241134751773\n",
      "184    124   0.43243243243243246\n",
      "209     99   0.42857142857142855\n",
      "133    175    0.4253731343283582\n",
      "250     58   0.42231075697211157\n",
      "151    157   0.42105263157894735\n",
      "163    145   0.42073170731707316\n",
      "166    142   0.41916167664670656\n",
      "42     266    0.4186046511627907\n",
      "176    132    0.4180790960451977\n",
      "47     261    0.4166666666666667\n",
      "168    140   0.41420118343195267\n",
      "190    118   0.41361256544502617\n",
      "33     275    0.4117647058823529\n",
      "48     260   0.40816326530612246\n",
      "129    179    0.4076923076923077\n",
      "171    137    0.4069767441860465\n",
      "182    126   0.40437158469945356\n",
      "46     262   0.40425531914893614\n",
      "180    128   0.40331491712707185\n",
      "126    182    0.4015748031496063\n",
      "139    169                   0.4\n",
      "164    144                   0.4\n",
      "152    156   0.39869281045751637\n",
      "158    150   0.39622641509433965\n",
      "161    147    0.3950617283950617\n",
      "37     271   0.39473684210526316\n",
      "160    148     0.391304347826087\n",
      "127    181              0.390625\n",
      "40     268    0.3902439024390244\n",
      "172    136    0.3872832369942196\n",
      "43     265   0.38636363636363635\n",
      "183    125    0.3858695652173913\n",
      "138    170     0.381294964028777\n",
      "146    162   0.38095238095238093\n",
      "141    167   0.38028169014084506\n",
      "36     272    0.3783783783783784\n",
      "177    131   0.37640449438202245\n",
      "148    160   0.37583892617449666\n",
      "136    172    0.3722627737226277\n",
      "26     282   0.37037037037037035\n",
      "45     263    0.3695652173913043\n",
      "18     290    0.3684210526315789\n",
      "154    154   0.36774193548387096\n",
      "135    173   0.36764705882352944\n",
      "21     287   0.36363636363636365\n",
      "143    165    0.3611111111111111\n",
      "150    158    0.3576158940397351\n",
      "41     267   0.35714285714285715\n",
      "134    174   0.35555555555555557\n",
      "165    143   0.35542168674698793\n",
      "169    139   0.35294117647058826\n",
      "144    164   0.35172413793103446\n",
      "156    152    0.3503184713375796\n",
      "19     289                  0.35\n",
      "145    163    0.3493150684931507\n",
      "137    171   0.34782608695652173\n",
      "175    133    0.3465909090909091\n",
      "25     283   0.34615384615384615\n",
      "157    151   0.34177215189873417\n",
      "155    153   0.33974358974358976\n",
      "153    155   0.33766233766233766\n",
      "162    146    0.3374233128834356\n",
      "8      300    0.3333333333333333\n",
      "29     279    0.3333333333333333\n",
      "147    161    0.3310810810810811\n",
      "142    166   0.32867132867132864\n",
      "159    149                 0.325\n",
      "39     269                 0.325\n",
      "24     284                  0.32\n",
      "28     280    0.3103448275862069\n",
      "38     270    0.3076923076923077\n",
      "35     273    0.3055555555555556\n",
      "10     298    0.2727272727272727\n",
      "27     281                  0.25\n",
      "9      299                   0.2\n"
     ]
    }
   ],
   "source": [
    "# Read in accuracies for each window size from Accuracies_for_Window_Size_Variations.csv file and sort by accuracy to determine best window size\n",
    "import csv\n",
    "pd.set_option('display.max_columns', None)  # or 1000\n",
    "pd.set_option('display.max_rows', None)  # or 1000\n",
    "mylist = pd.DataFrame(columns=['Window', 'Accuracy'])\n",
    "with open('Accuracies_for_Window_Size_Variations.csv', 'r') as csvfile:\n",
    "    for i,row in enumerate(csv.reader(csvfile, delimiter='\\n')):\n",
    "        mylist.loc[i,'Window'] = row[0].split(':')[0]\n",
    "        mylist.loc[i,'Accuracy'] = row[0].split(':')[1]\n",
    "print(mylist.sort_values('Accuracy', 0,ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Predictions:  86\n",
      "Window Size:  223\n",
      "train labels:       AvgHR_bin\n",
      "86         1.0\n",
      "87         1.0\n",
      "88         1.0\n",
      "89         0.0\n",
      "90         1.0\n",
      "91         1.0\n",
      "92         1.0\n",
      "93         1.0\n",
      "94         1.0\n",
      "95         1.0\n",
      "96         1.0\n",
      "97         0.0\n",
      "98         1.0\n",
      "99         1.0\n",
      "100        1.0\n",
      "101        1.0\n",
      "102        1.0\n",
      "103        1.0\n",
      "104        0.0\n",
      "105        1.0\n",
      "106        0.0\n",
      "107        0.0\n",
      "108        1.0\n",
      "109        1.0\n",
      "110        1.0\n",
      "111        1.0\n",
      "112        1.0\n",
      "113        1.0\n",
      "114        1.0\n",
      "115        1.0\n",
      "116        1.0\n",
      "117        1.0\n",
      "118        1.0\n",
      "119        1.0\n",
      "120        1.0\n",
      "121        1.0\n",
      "122        1.0\n",
      "123        0.0\n",
      "124        0.0\n",
      "125        0.0\n",
      "126        1.0\n",
      "127        0.0\n",
      "128        1.0\n",
      "129        1.0\n",
      "130        1.0\n",
      "131        0.0\n",
      "132        1.0\n",
      "133        0.0\n",
      "134        1.0\n",
      "135        1.0\n",
      "136        0.0\n",
      "137        1.0\n",
      "138        1.0\n",
      "139        1.0\n",
      "140        1.0\n",
      "141        1.0\n",
      "142        0.0\n",
      "143        1.0\n",
      "144        1.0\n",
      "145        0.0\n",
      "146        1.0\n",
      "147        1.0\n",
      "148        1.0\n",
      "149        1.0\n",
      "150        1.0\n",
      "151        0.0\n",
      "152        0.0\n",
      "153        1.0\n",
      "154        0.0\n",
      "155        1.0\n",
      "156        1.0\n",
      "157        1.0\n",
      "158        0.0\n",
      "159        0.0\n",
      "160        1.0\n",
      "161        1.0\n",
      "162        1.0\n",
      "163        0.0\n",
      "164        1.0\n",
      "165        1.0\n",
      "166        0.0\n",
      "167        1.0\n",
      "168        1.0\n",
      "169        0.0\n",
      "170        0.0\n",
      "171        1.0\n",
      "172        1.0\n",
      "173        1.0\n",
      "174        1.0\n",
      "175        1.0\n",
      "176        1.0\n",
      "177        0.0\n",
      "178        0.0\n",
      "179        0.0\n",
      "180        1.0\n",
      "181        1.0\n",
      "182        0.0\n",
      "183        1.0\n",
      "184        1.0\n",
      "185        0.0\n",
      "186        1.0\n",
      "187        1.0\n",
      "188        1.0\n",
      "189        0.0\n",
      "190        1.0\n",
      "191        1.0\n",
      "192        0.0\n",
      "193        1.0\n",
      "194        1.0\n",
      "195        1.0\n",
      "196        1.0\n",
      "197        0.0\n",
      "198        1.0\n",
      "199        0.0\n",
      "200        0.0\n",
      "201        1.0\n",
      "202        0.0\n",
      "203        1.0\n",
      "204        1.0\n",
      "205        0.0\n",
      "206        1.0\n",
      "207        1.0\n",
      "208        1.0\n",
      "209        0.0\n",
      "210        0.0\n",
      "211        1.0\n",
      "212        1.0\n",
      "213        1.0\n",
      "214        1.0\n",
      "215        0.0\n",
      "216        1.0\n",
      "217        1.0\n",
      "218        1.0\n",
      "219        1.0\n",
      "220        1.0\n",
      "221        1.0\n",
      "222        1.0\n",
      "223        1.0\n",
      "224        1.0\n",
      "225        1.0\n",
      "226        0.0\n",
      "227        1.0\n",
      "228        1.0\n",
      "229        1.0\n",
      "230        1.0\n",
      "231        0.0\n",
      "232        0.0\n",
      "233        0.0\n",
      "234        1.0\n",
      "235        1.0\n",
      "236        1.0\n",
      "237        1.0\n",
      "238        1.0\n",
      "239        1.0\n",
      "240        1.0\n",
      "241        1.0\n",
      "242        0.0\n",
      "243        1.0\n",
      "244        0.0\n",
      "245        0.0\n",
      "246        1.0\n",
      "247        1.0\n",
      "248        0.0\n",
      "249        0.0\n",
      "250        0.0\n",
      "251        1.0\n",
      "252        1.0\n",
      "253        1.0\n",
      "254        0.0\n",
      "255        0.0\n",
      "256        0.0\n",
      "257        1.0\n",
      "258        1.0\n",
      "259        0.0\n",
      "260        1.0\n",
      "261        1.0\n",
      "262        0.0\n",
      "263        1.0\n",
      "264        0.0\n",
      "265        1.0\n",
      "266        0.0\n",
      "267        0.0\n",
      "268        0.0\n",
      "269        0.0\n",
      "270        1.0\n",
      "271        0.0\n",
      "272        0.0\n",
      "273        0.0\n",
      "274        0.0\n",
      "275        0.0\n",
      "276        1.0\n",
      "277        0.0\n",
      "278        1.0\n",
      "279        0.0\n",
      "280        0.0\n",
      "281        0.0\n",
      "282        0.0\n",
      "283        1.0\n",
      "284        1.0\n",
      "285        1.0\n",
      "286        0.0\n",
      "287        1.0\n",
      "288        0.0\n",
      "289        0.0\n",
      "290        0.0\n",
      "291        1.0\n",
      "292        1.0\n",
      "293        0.0\n",
      "294        0.0\n",
      "295        1.0\n",
      "296        0.0\n",
      "297        0.0\n",
      "298        1.0\n",
      "299        0.0\n",
      "300        0.0\n",
      "301        0.0\n",
      "302        0.0\n",
      "303        0.0\n",
      "304        0.0\n",
      "305        0.0\n",
      "306        0.0\n",
      "307        0.0\n",
      "308        0.0\n",
      "test labels:      AvgHR_bin\n",
      "0         1.0\n",
      "1         1.0\n",
      "2         1.0\n",
      "3         1.0\n",
      "4         1.0\n",
      "5         1.0\n",
      "6         1.0\n",
      "7         0.0\n",
      "8         0.0\n",
      "9         0.0\n",
      "10        1.0\n",
      "11        1.0\n",
      "12        1.0\n",
      "13        0.0\n",
      "14        1.0\n",
      "15        0.0\n",
      "16        0.0\n",
      "17        1.0\n",
      "18        0.0\n",
      "19        1.0\n",
      "20        1.0\n",
      "21        0.0\n",
      "22        0.0\n",
      "23        0.0\n",
      "24        0.0\n",
      "25        0.0\n",
      "26        0.0\n",
      "27        0.0\n",
      "28        0.0\n",
      "29        0.0\n",
      "30        0.0\n",
      "31        0.0\n",
      "32        0.0\n",
      "33        0.0\n",
      "34        0.0\n",
      "35        0.0\n",
      "36        0.0\n",
      "37        0.0\n",
      "38        0.0\n",
      "39        0.0\n",
      "40        0.0\n",
      "41        0.0\n",
      "42        0.0\n",
      "43        0.0\n",
      "44        0.0\n",
      "45        0.0\n",
      "46        0.0\n",
      "47        0.0\n",
      "48        0.0\n",
      "49        0.0\n",
      "50        0.0\n",
      "51        0.0\n",
      "52        1.0\n",
      "53        1.0\n",
      "54        0.0\n",
      "55        0.0\n",
      "56        0.0\n",
      "57        0.0\n",
      "58        0.0\n",
      "59        0.0\n",
      "60        0.0\n",
      "61        0.0\n",
      "62        0.0\n",
      "63        0.0\n",
      "64        0.0\n",
      "65        1.0\n",
      "66        0.0\n",
      "67        0.0\n",
      "68        1.0\n",
      "69        0.0\n",
      "70        0.0\n",
      "71        1.0\n",
      "72        1.0\n",
      "73        1.0\n",
      "74        0.0\n",
      "75        0.0\n",
      "76        1.0\n",
      "77        0.0\n",
      "78        1.0\n",
      "79        0.0\n",
      "80        1.0\n",
      "81        0.0\n",
      "82        0.0\n",
      "83        0.0\n",
      "84        1.0\n",
      "85        1.0\n",
      "Iteration:  0\n",
      "X train shape:  223\n",
      "X test shape:  86\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 85, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  1\n",
      "X train shape:  223\n",
      "X test shape:  85\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 84, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  2\n",
      "X train shape:  223\n",
      "X test shape:  84\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 83, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  3\n",
      "X train shape:  223\n",
      "X test shape:  83\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 82, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  4\n",
      "X train shape:  223\n",
      "X test shape:  82\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 81, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  5\n",
      "X train shape:  223\n",
      "X test shape:  81\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 80, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  6\n",
      "X train shape:  223\n",
      "X test shape:  80\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 79, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  7\n",
      "X train shape:  223\n",
      "X test shape:  79\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 78, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  8\n",
      "X train shape:  223\n",
      "X test shape:  78\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 77, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  9\n",
      "X train shape:  223\n",
      "X test shape:  77\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 76, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  10\n",
      "X train shape:  223\n",
      "X test shape:  76\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 75, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  11\n",
      "X train shape:  223\n",
      "X test shape:  75\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 74, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  12\n",
      "X train shape:  223\n",
      "X test shape:  74\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 73, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  13\n",
      "X train shape:  223\n",
      "X test shape:  73\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 72, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  14\n",
      "X train shape:  223\n",
      "X test shape:  72\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 71, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  15\n",
      "X train shape:  223\n",
      "X test shape:  71\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 70, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  16\n",
      "X train shape:  223\n",
      "X test shape:  70\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 69, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  17\n",
      "X train shape:  223\n",
      "X test shape:  69\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 68, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  18\n",
      "X train shape:  223\n",
      "X test shape:  68\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 67, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  19\n",
      "X train shape:  223\n",
      "X test shape:  67\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 66, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  20\n",
      "X train shape:  223\n",
      "X test shape:  66\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 65, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  21\n",
      "X train shape:  223\n",
      "X test shape:  65\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 64, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  22\n",
      "X train shape:  223\n",
      "X test shape:  64\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 63, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  23\n",
      "X train shape:  223\n",
      "X test shape:  63\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 62, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  24\n",
      "X train shape:  223\n",
      "X test shape:  62\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 61, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  25\n",
      "X train shape:  223\n",
      "X test shape:  61\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 60, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  26\n",
      "X train shape:  223\n",
      "X test shape:  60\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 59, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  27\n",
      "X train shape:  223\n",
      "X test shape:  59\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 58, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  28\n",
      "X train shape:  223\n",
      "X test shape:  58\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 57, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  29\n",
      "X train shape:  223\n",
      "X test shape:  57\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 56, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  30\n",
      "X train shape:  223\n",
      "X test shape:  56\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 55, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  31\n",
      "X train shape:  223\n",
      "X test shape:  55\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 54, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  32\n",
      "X train shape:  223\n",
      "X test shape:  54\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 53, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  33\n",
      "X train shape:  223\n",
      "X test shape:  53\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 52, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  34\n",
      "X train shape:  223\n",
      "X test shape:  52\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 51, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  35\n",
      "X train shape:  223\n",
      "X test shape:  51\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 50, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  36\n",
      "X train shape:  223\n",
      "X test shape:  50\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 49, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  37\n",
      "X train shape:  223\n",
      "X test shape:  49\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 48, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  38\n",
      "X train shape:  223\n",
      "X test shape:  48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual:  AvgHR_bin    0.0\n",
      "Name: 47, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  39\n",
      "X train shape:  223\n",
      "X test shape:  47\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 46, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  40\n",
      "X train shape:  223\n",
      "X test shape:  46\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 45, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  41\n",
      "X train shape:  223\n",
      "X test shape:  45\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 44, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  42\n",
      "X train shape:  223\n",
      "X test shape:  44\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 43, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  43\n",
      "X train shape:  223\n",
      "X test shape:  43\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 42, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  44\n",
      "X train shape:  223\n",
      "X test shape:  42\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 41, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  45\n",
      "X train shape:  223\n",
      "X test shape:  41\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 40, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  46\n",
      "X train shape:  223\n",
      "X test shape:  40\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 39, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  47\n",
      "X train shape:  223\n",
      "X test shape:  39\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 38, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  48\n",
      "X train shape:  223\n",
      "X test shape:  38\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 37, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  49\n",
      "X train shape:  223\n",
      "X test shape:  37\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 36, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  50\n",
      "X train shape:  223\n",
      "X test shape:  36\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 35, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  51\n",
      "X train shape:  223\n",
      "X test shape:  35\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 34, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  52\n",
      "X train shape:  223\n",
      "X test shape:  34\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 33, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  53\n",
      "X train shape:  223\n",
      "X test shape:  33\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 32, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  54\n",
      "X train shape:  223\n",
      "X test shape:  32\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 31, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  55\n",
      "X train shape:  223\n",
      "X test shape:  31\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 30, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  56\n",
      "X train shape:  223\n",
      "X test shape:  30\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 29, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  57\n",
      "X train shape:  223\n",
      "X test shape:  29\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 28, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  58\n",
      "X train shape:  223\n",
      "X test shape:  28\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 27, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  59\n",
      "X train shape:  223\n",
      "X test shape:  27\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 26, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  60\n",
      "X train shape:  223\n",
      "X test shape:  26\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 25, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  61\n",
      "X train shape:  223\n",
      "X test shape:  25\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 24, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  62\n",
      "X train shape:  223\n",
      "X test shape:  24\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 23, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  63\n",
      "X train shape:  223\n",
      "X test shape:  23\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 22, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  64\n",
      "X train shape:  223\n",
      "X test shape:  22\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 21, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  65\n",
      "X train shape:  223\n",
      "X test shape:  21\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 20, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  66\n",
      "X train shape:  223\n",
      "X test shape:  20\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 19, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  67\n",
      "X train shape:  223\n",
      "X test shape:  19\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 18, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  68\n",
      "X train shape:  223\n",
      "X test shape:  18\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 17, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  69\n",
      "X train shape:  223\n",
      "X test shape:  17\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 16, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  70\n",
      "X train shape:  223\n",
      "X test shape:  16\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 15, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  71\n",
      "X train shape:  223\n",
      "X test shape:  15\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 14, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  72\n",
      "X train shape:  223\n",
      "X test shape:  14\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 13, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  73\n",
      "X train shape:  223\n",
      "X test shape:  13\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 12, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  74\n",
      "X train shape:  223\n",
      "X test shape:  12\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 11, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  75\n",
      "X train shape:  223\n",
      "X test shape:  11\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 10, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  76\n",
      "X train shape:  223\n",
      "X test shape:  10\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 9, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  77\n",
      "X train shape:  223\n",
      "X test shape:  9\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 8, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  78\n",
      "X train shape:  223\n",
      "X test shape:  8\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 7, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  79\n",
      "X train shape:  223\n",
      "X test shape:  7\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 6, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  80\n",
      "X train shape:  223\n",
      "X test shape:  6\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 5, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  81\n",
      "X train shape:  223\n",
      "X test shape:  5\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 4, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "Iteration:  82\n",
      "X train shape:  223\n",
      "X test shape:  4\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 3, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  83\n",
      "X train shape:  223\n",
      "X test shape:  3\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 2, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  84\n",
      "X train shape:  223\n",
      "X test shape:  2\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 1, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "Iteration:  85\n",
      "X train shape:  223\n",
      "X test shape:  1\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 0, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actuals and preds: \n",
      "     Predictions  Actuals\n",
      "0           1.0      1.0\n",
      "1           1.0      1.0\n",
      "2           1.0      1.0\n",
      "3           0.0      1.0\n",
      "4           0.0      1.0\n",
      "5           1.0      1.0\n",
      "6           0.0      1.0\n",
      "7           1.0      0.0\n",
      "8           0.0      0.0\n",
      "9           1.0      0.0\n",
      "10          0.0      1.0\n",
      "11          0.0      1.0\n",
      "12          1.0      1.0\n",
      "13          0.0      0.0\n",
      "14          1.0      1.0\n",
      "15          0.0      0.0\n",
      "16          0.0      0.0\n",
      "17          1.0      1.0\n",
      "18          0.0      0.0\n",
      "19          0.0      1.0\n",
      "20          1.0      1.0\n",
      "21          0.0      0.0\n",
      "22          0.0      0.0\n",
      "23          0.0      0.0\n",
      "24          0.0      0.0\n",
      "25          0.0      0.0\n",
      "26          0.0      0.0\n",
      "27          0.0      0.0\n",
      "28          0.0      0.0\n",
      "29          0.0      0.0\n",
      "30          0.0      0.0\n",
      "31          0.0      0.0\n",
      "32          0.0      0.0\n",
      "33          1.0      0.0\n",
      "34          0.0      0.0\n",
      "35          0.0      0.0\n",
      "36          0.0      0.0\n",
      "37          0.0      0.0\n",
      "38          0.0      0.0\n",
      "39          0.0      0.0\n",
      "40          0.0      0.0\n",
      "41          0.0      0.0\n",
      "42          0.0      0.0\n",
      "43          0.0      0.0\n",
      "44          0.0      0.0\n",
      "45          0.0      0.0\n",
      "46          0.0      0.0\n",
      "47          0.0      0.0\n",
      "48          0.0      0.0\n",
      "49          0.0      0.0\n",
      "50          0.0      0.0\n",
      "51          0.0      0.0\n",
      "52          0.0      1.0\n",
      "53          1.0      1.0\n",
      "54          0.0      0.0\n",
      "55          1.0      0.0\n",
      "56          0.0      0.0\n",
      "57          0.0      0.0\n",
      "58          0.0      0.0\n",
      "59          0.0      0.0\n",
      "60          0.0      0.0\n",
      "61          0.0      0.0\n",
      "62          0.0      0.0\n",
      "63          0.0      0.0\n",
      "64          0.0      0.0\n",
      "65          1.0      1.0\n",
      "66          0.0      0.0\n",
      "67          0.0      0.0\n",
      "68          1.0      1.0\n",
      "69          0.0      0.0\n",
      "70          0.0      0.0\n",
      "71          1.0      1.0\n",
      "72          1.0      1.0\n",
      "73          1.0      1.0\n",
      "74          1.0      0.0\n",
      "75          1.0      0.0\n",
      "76          1.0      1.0\n",
      "77          0.0      0.0\n",
      "78          0.0      1.0\n",
      "79          1.0      0.0\n",
      "80          1.0      1.0\n",
      "81          0.0      0.0\n",
      "82          1.0      0.0\n",
      "83          1.0      0.0\n",
      "84          1.0      1.0\n",
      "85          1.0      1.0\n",
      "accuracy for window size 86: 0.8023255813953488\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Run LR with optimal window size of 15 (# predictions = 294)\n",
    "window_size = 86\n",
    "print('# Predictions: ', window_size)\n",
    "print('Window Size: ', 309-window_size)\n",
    "# train set\n",
    "X_train = X.iloc[window_size:]\n",
    "y_train = y.iloc[window_size:]\n",
    "print('train labels: ', y_train)\n",
    "# test set\n",
    "X_test = X.iloc[:window_size]\n",
    "y_test = y.iloc[:window_size]\n",
    "print('test labels: ', y_test)\n",
    "\n",
    "actuals = pd.DataFrame(y_test)\n",
    "actuals = actuals.rename(columns={'AvgHR_bin':'Actuals'})\n",
    "# print('actuals: \\n', actuals)\n",
    "preds = np.zeros(X_test.shape[0])\n",
    "# print('X_test: \\n', X_test)\n",
    "for i in range(0,y_test.shape[0]):\n",
    "    print('Iteration: ', i)\n",
    "    print('X train shape: ', X_train.shape[0])\n",
    "    print('X test shape: ', y_test.shape[0])\n",
    "    logreg.fit(X_train, y_train.values.ravel())\n",
    "    # Predict test set\n",
    "    y_pred = logreg.predict(np.array(X_test.iloc[-1]).reshape(1,-1))\n",
    "    print('actual: ',y_test.iloc[-1], '\\n pred: ',y_pred, '\\n')\n",
    "    preds[i] = y_pred\n",
    "    # print('Accuracy of logistic regression classifier on test set {}: {:.2f}'.format(i, logreg.score(X_test, y_test)))\n",
    "    #print('test instance: ', X_test.iloc[-1])\n",
    "    # X_train = pd.concat([X_test.iloc[0], X_train]).reset_index(drop = True)\n",
    "    # print('new X train: ', X_train.head())\n",
    "    \n",
    "    #print(\"X test -1: \", X_test.iloc[-1])\n",
    "    X_test_inst = pd.DataFrame(data= [X_test.iloc[-1]],columns=[\"Distance (km)\",\"Avg Pace (/km)\", \"HRSS\", \"Elevation Gain (m)\"])\n",
    "    y_test_inst = pd.DataFrame(data = [y_test.iloc[-1]],columns=[\"AvgHR_bin\"])\n",
    "    #print(\"X test inst: \", X_test_inst)\n",
    "    X_train = X_train.drop(X_train.index[-1]).reset_index(drop=True)\n",
    "    X_train = pd.concat([X_test_inst,X_train]).reset_index(drop = True)\n",
    "    #print(\"X train: \\n\", X_train)\n",
    "\n",
    "    y_train = y_train.drop(y_train.index[-1])\n",
    "    y_train = pd.concat([y_test_inst,y_train])\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    #print(\"y train \\n\", y_train)\n",
    "    \n",
    "    X_test = X_test.drop(X_test.index[-1])\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    #print(\"X test: \\n\", X_test)\n",
    "\n",
    "    y_test = y_test.drop(y_test.index[-1])\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "preds_act_df = pd.DataFrame(preds, columns=['Predictions'])\n",
    "preds_act_df = preds_act_df.join(actuals)\n",
    "print('actuals and preds: \\n', preds_act_df)\n",
    "accuracy = metrics.accuracy_score(preds_act_df.Actuals.ravel(),preds_act_df.Predictions.ravel())\n",
    "print('accuracy for window size {}: {}'.format(window_size, accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[51  9]\n",
      " [ 8 18]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix1 = confusion_matrix(actuals, preds)\n",
    "print(confusion_matrix1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true neg:  51 \n",
      "false pos:  9 \n",
      "false neg:  8 \n",
      "true pos:  18\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(actuals, preds).ravel()\n",
    "print('true neg: ', tn, '\\nfalse pos: ', fp, '\\nfalse neg: ', fn, '\\ntrue pos: ',tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.85      0.86        60\n",
      "         1.0       0.67      0.69      0.68        26\n",
      "\n",
      "    accuracy                           0.80        86\n",
      "   macro avg       0.77      0.77      0.77        86\n",
      "weighted avg       0.80      0.80      0.80        86\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Precision, recall, f1-score, support (# of test instances per class)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(actuals, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.metrics import roc_auc_score\\nfrom sklearn.metrics import roc_curve\\nlogit_roc_auc = roc_auc_score(actuals, logreg.predict(np.array(X_test.iloc[0].reshape(1,-1))\\nfpr, tpr, thresholds = roc_curve(y_test.iloc[0], logreg.predict_proba(np.array(X_test.iloc[0].reshape(1,-1))[:,1])\\nplt.figure()\\nplt.plot(fpr, tpr, label=\\'Logistic Regression (area = %0.2f)\\' % logit_roc_auc)\\nplt.plot([0, 1], [0, 1],\\'r--\\')\\nplt.xlim([0.0, 1.0])\\nplt.ylim([0.0, 1.05])\\nplt.xlabel(\\'False Positive Rate\\')\\nplt.ylabel(\\'True Positive Rate\\')\\nplt.title(\\'Receiver operating characteristic\\')\\nplt.legend(loc=\"lower right\")\\nplt.savefig(\\'Log_ROC\\')\\nplt.show()\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ROC curve\n",
    "'''\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "logit_roc_auc = roc_auc_score(actuals, logreg.predict(np.array(X_test.iloc[0].reshape(1,-1))\n",
    "fpr, tpr, thresholds = roc_curve(y_test.iloc[0], logreg.predict_proba(np.array(X_test.iloc[0].reshape(1,-1))[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look Ahead Implementation with LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Predictions:  86\n",
      "Window Size:  223\n",
      "train labels:       AvgHR_bin\n",
      "86         1.0\n",
      "87         1.0\n",
      "88         1.0\n",
      "89         0.0\n",
      "90         1.0\n",
      "91         1.0\n",
      "92         1.0\n",
      "93         1.0\n",
      "94         1.0\n",
      "95         1.0\n",
      "96         1.0\n",
      "97         0.0\n",
      "98         1.0\n",
      "99         1.0\n",
      "100        1.0\n",
      "101        1.0\n",
      "102        1.0\n",
      "103        1.0\n",
      "104        0.0\n",
      "105        1.0\n",
      "106        0.0\n",
      "107        0.0\n",
      "108        1.0\n",
      "109        1.0\n",
      "110        1.0\n",
      "111        1.0\n",
      "112        1.0\n",
      "113        1.0\n",
      "114        1.0\n",
      "115        1.0\n",
      "116        1.0\n",
      "117        1.0\n",
      "118        1.0\n",
      "119        1.0\n",
      "120        1.0\n",
      "121        1.0\n",
      "122        1.0\n",
      "123        0.0\n",
      "124        0.0\n",
      "125        0.0\n",
      "126        1.0\n",
      "127        0.0\n",
      "128        1.0\n",
      "129        1.0\n",
      "130        1.0\n",
      "131        0.0\n",
      "132        1.0\n",
      "133        0.0\n",
      "134        1.0\n",
      "135        1.0\n",
      "136        0.0\n",
      "137        1.0\n",
      "138        1.0\n",
      "139        1.0\n",
      "140        1.0\n",
      "141        1.0\n",
      "142        0.0\n",
      "143        1.0\n",
      "144        1.0\n",
      "145        0.0\n",
      "146        1.0\n",
      "147        1.0\n",
      "148        1.0\n",
      "149        1.0\n",
      "150        1.0\n",
      "151        0.0\n",
      "152        0.0\n",
      "153        1.0\n",
      "154        0.0\n",
      "155        1.0\n",
      "156        1.0\n",
      "157        1.0\n",
      "158        0.0\n",
      "159        0.0\n",
      "160        1.0\n",
      "161        1.0\n",
      "162        1.0\n",
      "163        0.0\n",
      "164        1.0\n",
      "165        1.0\n",
      "166        0.0\n",
      "167        1.0\n",
      "168        1.0\n",
      "169        0.0\n",
      "170        0.0\n",
      "171        1.0\n",
      "172        1.0\n",
      "173        1.0\n",
      "174        1.0\n",
      "175        1.0\n",
      "176        1.0\n",
      "177        0.0\n",
      "178        0.0\n",
      "179        0.0\n",
      "180        1.0\n",
      "181        1.0\n",
      "182        0.0\n",
      "183        1.0\n",
      "184        1.0\n",
      "185        0.0\n",
      "186        1.0\n",
      "187        1.0\n",
      "188        1.0\n",
      "189        0.0\n",
      "190        1.0\n",
      "191        1.0\n",
      "192        0.0\n",
      "193        1.0\n",
      "194        1.0\n",
      "195        1.0\n",
      "196        1.0\n",
      "197        0.0\n",
      "198        1.0\n",
      "199        0.0\n",
      "200        0.0\n",
      "201        1.0\n",
      "202        0.0\n",
      "203        1.0\n",
      "204        1.0\n",
      "205        0.0\n",
      "206        1.0\n",
      "207        1.0\n",
      "208        1.0\n",
      "209        0.0\n",
      "210        0.0\n",
      "211        1.0\n",
      "212        1.0\n",
      "213        1.0\n",
      "214        1.0\n",
      "215        0.0\n",
      "216        1.0\n",
      "217        1.0\n",
      "218        1.0\n",
      "219        1.0\n",
      "220        1.0\n",
      "221        1.0\n",
      "222        1.0\n",
      "223        1.0\n",
      "224        1.0\n",
      "225        1.0\n",
      "226        0.0\n",
      "227        1.0\n",
      "228        1.0\n",
      "229        1.0\n",
      "230        1.0\n",
      "231        0.0\n",
      "232        0.0\n",
      "233        0.0\n",
      "234        1.0\n",
      "235        1.0\n",
      "236        1.0\n",
      "237        1.0\n",
      "238        1.0\n",
      "239        1.0\n",
      "240        1.0\n",
      "241        1.0\n",
      "242        0.0\n",
      "243        1.0\n",
      "244        0.0\n",
      "245        0.0\n",
      "246        1.0\n",
      "247        1.0\n",
      "248        0.0\n",
      "249        0.0\n",
      "250        0.0\n",
      "251        1.0\n",
      "252        1.0\n",
      "253        1.0\n",
      "254        0.0\n",
      "255        0.0\n",
      "256        0.0\n",
      "257        1.0\n",
      "258        1.0\n",
      "259        0.0\n",
      "260        1.0\n",
      "261        1.0\n",
      "262        0.0\n",
      "263        1.0\n",
      "264        0.0\n",
      "265        1.0\n",
      "266        0.0\n",
      "267        0.0\n",
      "268        0.0\n",
      "269        0.0\n",
      "270        1.0\n",
      "271        0.0\n",
      "272        0.0\n",
      "273        0.0\n",
      "274        0.0\n",
      "275        0.0\n",
      "276        1.0\n",
      "277        0.0\n",
      "278        1.0\n",
      "279        0.0\n",
      "280        0.0\n",
      "281        0.0\n",
      "282        0.0\n",
      "283        1.0\n",
      "284        1.0\n",
      "285        1.0\n",
      "286        0.0\n",
      "287        1.0\n",
      "288        0.0\n",
      "289        0.0\n",
      "290        0.0\n",
      "291        1.0\n",
      "292        1.0\n",
      "293        0.0\n",
      "294        0.0\n",
      "295        1.0\n",
      "296        0.0\n",
      "297        0.0\n",
      "298        1.0\n",
      "299        0.0\n",
      "300        0.0\n",
      "301        0.0\n",
      "302        0.0\n",
      "303        0.0\n",
      "304        0.0\n",
      "305        0.0\n",
      "306        0.0\n",
      "307        0.0\n",
      "308        0.0\n",
      "test labels:      AvgHR_bin\n",
      "0         1.0\n",
      "1         1.0\n",
      "2         1.0\n",
      "3         1.0\n",
      "4         1.0\n",
      "5         1.0\n",
      "6         1.0\n",
      "7         0.0\n",
      "8         0.0\n",
      "9         0.0\n",
      "10        1.0\n",
      "11        1.0\n",
      "12        1.0\n",
      "13        0.0\n",
      "14        1.0\n",
      "15        0.0\n",
      "16        0.0\n",
      "17        1.0\n",
      "18        0.0\n",
      "19        1.0\n",
      "20        1.0\n",
      "21        0.0\n",
      "22        0.0\n",
      "23        0.0\n",
      "24        0.0\n",
      "25        0.0\n",
      "26        0.0\n",
      "27        0.0\n",
      "28        0.0\n",
      "29        0.0\n",
      "30        0.0\n",
      "31        0.0\n",
      "32        0.0\n",
      "33        0.0\n",
      "34        0.0\n",
      "35        0.0\n",
      "36        0.0\n",
      "37        0.0\n",
      "38        0.0\n",
      "39        0.0\n",
      "40        0.0\n",
      "41        0.0\n",
      "42        0.0\n",
      "43        0.0\n",
      "44        0.0\n",
      "45        0.0\n",
      "46        0.0\n",
      "47        0.0\n",
      "48        0.0\n",
      "49        0.0\n",
      "50        0.0\n",
      "51        0.0\n",
      "52        1.0\n",
      "53        1.0\n",
      "54        0.0\n",
      "55        0.0\n",
      "56        0.0\n",
      "57        0.0\n",
      "58        0.0\n",
      "59        0.0\n",
      "60        0.0\n",
      "61        0.0\n",
      "62        0.0\n",
      "63        0.0\n",
      "64        0.0\n",
      "65        1.0\n",
      "66        0.0\n",
      "67        0.0\n",
      "68        1.0\n",
      "69        0.0\n",
      "70        0.0\n",
      "71        1.0\n",
      "72        1.0\n",
      "73        1.0\n",
      "74        0.0\n",
      "75        0.0\n",
      "76        1.0\n",
      "77        0.0\n",
      "78        1.0\n",
      "79        0.0\n",
      "80        1.0\n",
      "81        0.0\n",
      "82        0.0\n",
      "83        0.0\n",
      "84        1.0\n",
      "85        1.0\n",
      "Iteration:  0\n",
      "X train shape:  223\n",
      "X test shape:  86\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 85, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 84, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 83, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 82, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 81, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 80, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 79, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 78, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 77, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 76, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 75, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 74, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 73, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 72, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 71, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 70, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 69, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 68, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 67, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 66, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 65, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 64, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 63, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 62, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 61, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 60, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 59, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 58, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 57, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 56, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 55, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 54, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 53, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 52, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 51, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 50, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 49, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 48, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 47, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 46, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 45, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 44, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 43, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 42, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 41, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 40, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 39, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 38, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 37, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 36, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 35, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 34, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 33, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 32, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 31, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 30, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 29, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 28, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 27, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 26, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 25, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 24, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 23, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 22, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 21, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 20, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 19, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 18, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 17, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 16, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 15, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 14, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 13, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 12, dtype: float64 \n",
      " pred:  [1.] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 11, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 10, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 9, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 8, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 7, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 6, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 5, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 4, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 3, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 2, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 1, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 0, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "X test inst:      Distance (km)  Avg Pace (/km)   HRSS  Elevation Gain (m)\n",
      "85           56.3           121.0  142.0               668.0\n",
      "84           40.2           113.0   91.0               379.0\n",
      "83           55.8           126.0  125.0               496.0\n",
      "82           80.3           137.0  164.0              1019.0\n",
      "81           56.1           134.0  123.0               665.0\n",
      "80           30.0            91.0  111.0                13.0\n",
      "79           40.0           118.0   87.0               371.0\n",
      "78           60.6           116.0  137.0               546.0\n",
      "77           50.8           116.0  107.0               510.0\n",
      "76           32.5           109.0   86.0               285.0\n",
      "75           80.3           133.0  171.0              1077.0\n",
      "74           40.1           118.0   69.0               388.0\n",
      "73           73.9           148.0  292.0              1986.0\n",
      "72           20.1           177.0   73.0               488.0\n",
      "71           29.1           112.0  102.0               412.0\n",
      "70           50.5           163.0  135.0              1306.0\n",
      "69           33.1           170.0   98.0               852.0\n",
      "68           90.2            87.0  213.0               461.0\n",
      "67           30.1           158.0    6.0               678.0\n",
      "66           51.5           161.0  148.0              1258.0\n",
      "65           33.1           155.0  123.0               878.0\n",
      "64          107.6           160.0  335.0              2925.0\n",
      "63           33.2           174.0   93.0               866.0\n",
      "62           41.8           188.0  117.0              1094.0\n",
      "61           34.3           160.0  111.0               894.0\n",
      "60          151.2           165.0  344.0              3142.0\n",
      "59           50.8           150.0  137.0               783.0\n",
      "58           43.8           140.0   90.0               387.0\n",
      "57           33.3           158.0  104.0               882.0\n",
      "56           55.2           146.0  157.0              1126.0\n",
      "55           50.4           162.0  137.0              1293.0\n",
      "54           40.1           156.0  103.0               859.0\n",
      "53           73.2           146.0  227.0              2031.0\n",
      "52          144.1           124.0  467.0              3120.0\n",
      "51           50.4           165.0  126.0              1239.0\n",
      "50           73.6           177.0  181.0              1922.0\n",
      "49           64.3           180.0  164.0              1053.0\n",
      "48           60.4           157.0  134.0               679.0\n",
      "47           40.6           207.0   96.0               833.0\n",
      "46           52.9           176.0  135.0              1313.0\n",
      "45          105.5           169.0  309.0              3067.0\n",
      "44           53.0           164.0  137.0              1400.0\n",
      "43           53.0           155.0  138.0              1387.0\n",
      "42          146.9           168.0  445.0              3617.0\n",
      "41           93.8           153.0  205.0              1446.0\n",
      "40           50.4           165.0  108.0              1327.0\n",
      "39          101.6           146.0  213.0              1790.0\n",
      "38           76.0           172.0  197.0              2003.0\n",
      "37           50.4           180.0  100.0              1310.0\n",
      "36           30.4           117.0   52.0               247.0\n",
      "35           93.4           173.0  300.0              2624.0\n",
      "34           76.3           173.0  189.0              2067.0\n",
      "33           76.3           168.0  172.0              2074.0\n",
      "32           25.6           208.0  109.0               685.0\n",
      "31          170.6           149.0  471.0              3388.0\n",
      "30           26.0           307.0  128.0               715.0\n",
      "29           27.2           221.0   82.0               636.0\n",
      "28          141.0           167.0  354.0              3366.0\n",
      "27           15.1           477.0   71.0               492.0\n",
      "26           52.9           193.0  137.0              1150.0\n",
      "25           64.2           195.0  195.0              2194.0\n",
      "24           40.3           266.0  166.0              1326.0\n",
      "23          139.8           151.0  390.0              2768.3\n",
      "22           55.2           149.0  116.0               884.0\n",
      "21           50.6           187.0  133.0               981.0\n",
      "20           52.8           116.0  138.0               605.0\n",
      "19           55.4           108.0  123.0               541.0\n",
      "18           63.0           119.0  128.0               744.0\n",
      "17           47.5           110.0  132.0               406.0\n",
      "16           32.5           119.0   66.0               282.0\n",
      "15           45.9           148.0   79.0               447.0\n",
      "14           30.3           127.0   92.0               550.0\n",
      "13           13.2           301.0   74.0               184.0\n",
      "12           52.0           116.0  152.0               560.0\n",
      "11          100.9           130.0  337.0              1395.0\n",
      "10           52.6           122.0  172.0               535.0\n",
      "9            47.2           116.0  106.0               447.0\n",
      "8            55.4           114.0  116.0               502.0\n",
      "7            51.4           128.0   90.0               576.0\n",
      "6            81.0           129.0  225.0              1096.0\n",
      "5            61.4           122.0  146.0               692.0\n",
      "4            41.1           120.0   96.0               454.0\n",
      "3            45.6           114.0  119.0               447.0\n",
      "2            35.2           116.0   87.0               314.0\n",
      "1            80.1           125.0  217.0               890.0\n",
      "0            62.5           113.0  144.0               589.0\n",
      "predictions:  [1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1.\n",
      " 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
      "actuals:  [1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1.\n",
      " 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "accuracy for window size 86: 0.8953488372093024\n",
      "accuracies:  [0.89534884]\n",
      "avg accuracy:  0.8953488372093024\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import statistics as stats\n",
    "# Run LR with optimal window size of 15 (# predictions = 294)\n",
    "\n",
    "window_size = 86 # num of preds, window_size = 309-window_size param\n",
    "print('# Predictions: ', window_size)\n",
    "print('Window Size: ', 309-window_size)\n",
    "# train set\n",
    "X_train = X.iloc[window_size:]\n",
    "y_train = y.iloc[window_size:]\n",
    "print('train labels: ', y_train)\n",
    "# test set\n",
    "X_test = X.iloc[:window_size]\n",
    "y_test = y.iloc[:window_size]\n",
    "print('test labels: ', y_test)\n",
    "\n",
    "look_ahead = 86\n",
    "\n",
    "\n",
    "accuracies = np.zeros(math.ceil(y_test.shape[0]/look_ahead))\n",
    "# print('X_test: \\n', X_test)\n",
    "\n",
    "for i in range(0,math.ceil(y_test.shape[0]/look_ahead)):\n",
    "    preds = np.zeros(min(look_ahead,y_test.shape[0]))\n",
    "    actuals_look_ahead = np.zeros(min(look_ahead,y_test.shape[0]))\n",
    "    print('Iteration: ', i)\n",
    "    print('X train shape: ', X_train.shape[0])\n",
    "    print('X test shape: ', y_test.shape[0])\n",
    "    logreg.fit(X_train, y_train.values.ravel())\n",
    "    # Predict test set\n",
    "    for j in range(1,min(look_ahead+1,y_test.shape[0]+1)):\n",
    "        y_pred = logreg.predict(np.array(X_test.iloc[-j]).reshape(1,-1))\n",
    "        print('actual: ',y_test.iloc[-j], '\\n pred: ',y_pred, '\\n')\n",
    "        preds[j-1] = y_pred\n",
    "        actuals_look_ahead[j-1] = y_test.iloc[-j]\n",
    "    # print('Accuracy of logistic regression classifier on test set {}: {:.2f}'.format(i, logreg.score(X_test, y_test)))\n",
    "    #print('test instance: ', X_test.iloc[-1])\n",
    "    # X_train = pd.concat([X_test.iloc[0], X_train]).reset_index(drop = True)\n",
    "    # print('new X train: ', X_train.head())\n",
    "    \n",
    "    #print(\"X test -1: \", X_test.iloc[-1])\n",
    "    X_test_inst = pd.DataFrame(columns=[\"Distance (km)\",\"Avg Pace (/km)\", \"HRSS\", \"Elevation Gain (m)\"])\n",
    "    y_test_inst = pd.DataFrame(columns=[\"AvgHR_bin\"])\n",
    "    for k in range(1,min(look_ahead+1,y_test.shape[0]+1)):\n",
    "        X_test_inst = X_test_inst.append(X_test.iloc[-k])\n",
    "        y_test_inst = y_test_inst.append(y_test.iloc[-k])        \n",
    "    print(\"X test inst: \", X_test_inst)\n",
    "\n",
    "    #X_train = X_train.drop(X_train.index[-1]).reset_index(drop=True)\n",
    "    X_train = pd.concat([X_test_inst,X_train]).reset_index(drop = True)\n",
    "    #print(\"X train: \\n\", X_train)\n",
    "\n",
    "    #y_train = y_train.drop(y_train.index[-1])\n",
    "    y_train = pd.concat([y_test_inst,y_train]).reset_index(drop=True)\n",
    "    #print(\"y train \\n\", y_train)\n",
    "    \n",
    "    X_test = X_test.drop(X_test_inst.index.values)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    #print(\"X test: \\n\", X_test)\n",
    "\n",
    "    y_test = y_test.drop(y_test_inst.index.values)\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "    \n",
    "    preds_act_df = pd.DataFrame(preds)\n",
    "    act_df = pd.DataFrame(actuals_look_ahead)\n",
    "    print(\"predictions: \", preds)\n",
    "    print(\"actuals: \", actuals_look_ahead)\n",
    "    #preds_act_df = preds_act_df.join(act_df)\n",
    "    #print('actuals and preds: \\n', preds_act_df)\n",
    "    accuracy = metrics.accuracy_score(preds,actuals_look_ahead)\n",
    "    accuracies[i] = accuracy\n",
    "    print('accuracy for window size {} with look ahead value {}: {}'.format(309-window_size, look_ahead, accuracy))\n",
    "#print('accuracies: ', accuracies)\n",
    "#print('avg accuracy: ', stats.mean(accuracies))\n",
    "# preds_act_df = pd.DataFrame(preds)\n",
    "# preds_act_df = preds_act_df.join(actuals)\n",
    "# print('actuals and preds: \\n', preds_act_df)\n",
    "# accuracy = metrics.accuracy_score(preds_act_df.Actuals.ravel(),preds_act_df.Predictions.ravel())\n",
    "# print('accuracy for window size {}: {}'.format(window_size, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate Over All Look-Ahead Values for Given Window Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Predictions:  86\n",
      "Window Size:  223\n",
      "\n",
      "\n",
      "look ahead:  1\n",
      "# iterations:  86\n",
      "accuracies:  [1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1.]\n",
      "avg accuracy:  0.8953488372093024\n",
      "\n",
      "\n",
      "look ahead:  2\n",
      "# iterations:  43\n",
      "accuracies:  [1.  0.5 1.  1.  1.  1.  0.5 1.  1.  1.  1.  1.  1.  1.  1.  1.  0.5 1.\n",
      " 1.  1.  1.  1.  1.  1.  1.  1.  0.5 0.5 1.  1.  1.  1.  1.  0.5 1.  1.\n",
      " 0.5 1.  1.  1.  0.  1.  1. ]\n",
      "avg accuracy:  0.8953488372093024\n",
      "\n",
      "\n",
      "look ahead:  3\n",
      "# iterations:  29\n",
      "accuracies:  [0.66666667 1.         1.         1.         0.66666667 1.\n",
      " 1.         1.         1.         1.         0.66666667 1.\n",
      " 1.         1.         1.         1.         1.         0.66666667\n",
      " 0.66666667 1.         1.         1.         0.66666667 1.\n",
      " 0.66666667 1.         0.66666667 0.66666667 1.        ]\n",
      "avg accuracy:  0.896551724137931\n",
      "\n",
      "\n",
      "look ahead:  4\n",
      "# iterations:  22\n",
      "accuracies:  [0.75 1.   1.   0.75 1.   1.   1.   1.   0.75 1.   1.   1.   1.   0.5\n",
      " 1.   1.   0.75 1.   0.75 1.   0.5  1.  ]\n",
      "avg accuracy:  0.8977272727272727\n",
      "\n",
      "\n",
      "look ahead:  5\n",
      "# iterations:  18\n",
      "accuracies:  [0.8 1.  0.8 1.  1.  1.  0.8 1.  1.  1.  0.8 0.8 1.  0.8 0.8 1.  0.6 1. ]\n",
      "avg accuracy:  0.9\n",
      "\n",
      "\n",
      "look ahead:  6\n",
      "# iterations:  15\n",
      "accuracies:  [0.83333333 1.         0.83333333 1.         1.         0.83333333\n",
      " 1.         1.         0.83333333 0.83333333 1.         0.83333333\n",
      " 0.66666667 0.66666667 1.        ]\n",
      "avg accuracy:  0.888888888888889\n",
      "\n",
      "\n",
      "look ahead:  7\n",
      "# iterations:  13\n",
      "accuracies:  [0.85714286 0.85714286 1.         1.         0.85714286 1.\n",
      " 1.         0.71428571 1.         0.85714286 0.85714286 0.71428571\n",
      " 1.        ]\n",
      "avg accuracy:  0.9010989010989011\n",
      "\n",
      "\n",
      "look ahead:  8\n",
      "# iterations:  11\n",
      "accuracies:  [0.875      0.875      1.         1.         0.875      1.\n",
      " 0.75       1.         0.875      0.75       0.66666667]\n",
      "avg accuracy:  0.8787878787878788\n",
      "\n",
      "\n",
      "look ahead:  9\n",
      "# iterations:  10\n",
      "accuracies:  [0.88888889 0.88888889 1.         0.88888889 1.         0.88888889\n",
      " 0.88888889 0.88888889 0.66666667 0.8       ]\n",
      "avg accuracy:  0.88\n",
      "\n",
      "\n",
      "look ahead:  10\n",
      "# iterations:  9\n",
      "accuracies:  [0.9        0.9        1.         0.9        1.         0.8\n",
      " 0.9        0.9        0.66666667]\n",
      "avg accuracy:  0.8851851851851852\n",
      "\n",
      "\n",
      "look ahead:  11\n",
      "# iterations:  8\n",
      "accuracies:  [0.90909091 0.90909091 0.90909091 1.         0.90909091 0.90909091\n",
      " 0.81818182 0.77777778]\n",
      "avg accuracy:  0.8926767676767676\n",
      "\n",
      "\n",
      "look ahead:  12\n",
      "# iterations:  8\n",
      "accuracies:  [0.91666667 0.91666667 0.91666667 1.         0.83333333 0.91666667\n",
      " 0.66666667 1.        ]\n",
      "avg accuracy:  0.8958333333333333\n",
      "\n",
      "\n",
      "look ahead:  13\n",
      "# iterations:  7\n",
      "accuracies:  [0.92307692 0.92307692 0.92307692 1.         0.84615385 0.84615385\n",
      " 0.75      ]\n",
      "avg accuracy:  0.8873626373626374\n",
      "\n",
      "\n",
      "look ahead:  14\n",
      "# iterations:  7\n",
      "accuracies:  [0.85714286 1.         0.92857143 0.85714286 0.92857143 0.78571429\n",
      " 1.        ]\n",
      "avg accuracy:  0.9081632653061225\n",
      "\n",
      "\n",
      "look ahead:  15\n",
      "# iterations:  6\n",
      "accuracies:  [0.86666667 1.         0.93333333 0.86666667 0.86666667 0.81818182]\n",
      "avg accuracy:  0.891919191919192\n",
      "\n",
      "\n",
      "look ahead:  16\n",
      "# iterations:  6\n",
      "accuracies:  [0.875      1.         0.9375     0.875      0.875      0.66666667]\n",
      "avg accuracy:  0.8715277777777778\n",
      "\n",
      "\n",
      "look ahead:  17\n",
      "# iterations:  6\n",
      "accuracies:  [0.88235294 0.94117647 1.         0.82352941 0.76470588 1.        ]\n",
      "avg accuracy:  0.9019607843137255\n",
      "\n",
      "\n",
      "look ahead:  18\n",
      "# iterations:  5\n",
      "accuracies:  [0.88888889 0.94444444 0.94444444 0.88888889 0.71428571]\n",
      "avg accuracy:  0.8761904761904762\n",
      "\n",
      "\n",
      "look ahead:  19\n",
      "# iterations:  5\n",
      "accuracies:  [0.89473684 0.94736842 0.89473684 0.89473684 0.8       ]\n",
      "avg accuracy:  0.8863157894736842\n",
      "\n",
      "\n",
      "look ahead:  20\n",
      "# iterations:  5\n",
      "accuracies:  [0.9        0.95       0.9        0.9        0.66666667]\n",
      "avg accuracy:  0.8633333333333333\n",
      "\n",
      "\n",
      "look ahead:  21\n",
      "# iterations:  5\n",
      "accuracies:  [0.9047619  0.95238095 0.9047619  0.80952381 1.        ]\n",
      "avg accuracy:  0.9142857142857143\n",
      "\n",
      "\n",
      "look ahead:  22\n",
      "# iterations:  4\n",
      "accuracies:  [0.90909091 0.95454545 0.90909091 0.8       ]\n",
      "avg accuracy:  0.8931818181818182\n",
      "\n",
      "\n",
      "look ahead:  23\n",
      "# iterations:  4\n",
      "accuracies:  [0.91304348 0.95652174 0.86956522 0.76470588]\n",
      "avg accuracy:  0.8759590792838874\n",
      "\n",
      "\n",
      "look ahead:  24\n",
      "# iterations:  4\n",
      "accuracies:  [0.91666667 0.95833333 0.875      0.71428571]\n",
      "avg accuracy:  0.8660714285714286\n",
      "\n",
      "\n",
      "look ahead:  25\n",
      "# iterations:  4\n",
      "accuracies:  [0.92       0.96       0.84       0.81818182]\n",
      "avg accuracy:  0.8845454545454545\n",
      "\n",
      "\n",
      "look ahead:  26\n",
      "# iterations:  4\n",
      "accuracies:  [0.92307692 0.96153846 0.84615385 0.75      ]\n",
      "avg accuracy:  0.8701923076923077\n",
      "\n",
      "\n",
      "look ahead:  27\n",
      "# iterations:  4\n",
      "accuracies:  [0.92592593 0.92592593 0.85185185 0.8       ]\n",
      "avg accuracy:  0.875925925925926\n",
      "\n",
      "\n",
      "look ahead:  28\n",
      "# iterations:  4\n",
      "accuracies:  [0.92857143 0.89285714 0.85714286 1.        ]\n",
      "avg accuracy:  0.9196428571428572\n",
      "\n",
      "\n",
      "look ahead:  29\n",
      "# iterations:  3\n",
      "accuracies:  [0.93103448 0.89655172 0.85714286]\n",
      "avg accuracy:  0.8949096880131363\n",
      "\n",
      "\n",
      "look ahead:  30\n",
      "# iterations:  3\n",
      "accuracies:  [0.93333333 0.9        0.84615385]\n",
      "avg accuracy:  0.8931623931623932\n",
      "\n",
      "\n",
      "look ahead:  31\n",
      "# iterations:  3\n",
      "accuracies:  [0.93548387 0.90322581 0.83333333]\n",
      "avg accuracy:  0.8906810035842294\n",
      "\n",
      "\n",
      "look ahead:  32\n",
      "# iterations:  3\n",
      "accuracies:  [0.9375     0.90625    0.81818182]\n",
      "avg accuracy:  0.8873106060606061\n",
      "\n",
      "\n",
      "look ahead:  33\n",
      "# iterations:  3\n",
      "accuracies:  [0.90909091 0.93939394 0.8       ]\n",
      "avg accuracy:  0.8828282828282829\n",
      "\n",
      "\n",
      "look ahead:  34\n",
      "# iterations:  3\n",
      "accuracies:  [0.91176471 0.91176471 0.77777778]\n",
      "avg accuracy:  0.8671023965141612\n",
      "\n",
      "\n",
      "look ahead:  35\n",
      "# iterations:  3\n",
      "accuracies:  [0.91428571 0.91428571 0.8125    ]\n",
      "avg accuracy:  0.8803571428571428\n",
      "\n",
      "\n",
      "look ahead:  36\n",
      "# iterations:  3\n",
      "accuracies:  [0.91666667 0.91666667 0.71428571]\n",
      "avg accuracy:  0.8492063492063492\n",
      "\n",
      "\n",
      "look ahead:  37\n",
      "# iterations:  3\n",
      "accuracies:  [0.91891892 0.89189189 0.83333333]\n",
      "avg accuracy:  0.8813813813813814\n",
      "\n",
      "\n",
      "look ahead:  38\n",
      "# iterations:  3\n",
      "accuracies:  [0.92105263 0.89473684 0.8       ]\n",
      "avg accuracy:  0.8719298245614036\n",
      "\n",
      "\n",
      "look ahead:  39\n",
      "# iterations:  3\n",
      "accuracies:  [0.92307692 0.8974359  0.75      ]\n",
      "avg accuracy:  0.8568376068376069\n",
      "\n",
      "\n",
      "look ahead:  40\n",
      "# iterations:  3\n",
      "accuracies:  [0.925      0.9        0.66666667]\n",
      "avg accuracy:  0.8305555555555556\n",
      "\n",
      "\n",
      "look ahead:  41\n",
      "# iterations:  3\n",
      "accuracies:  [0.92682927 0.85365854 1.        ]\n",
      "avg accuracy:  0.926829268292683\n",
      "\n",
      "\n",
      "look ahead:  42\n",
      "# iterations:  3\n",
      "accuracies:  [0.92857143 0.85714286 1.        ]\n",
      "avg accuracy:  0.9285714285714286\n",
      "\n",
      "\n",
      "look ahead:  43\n",
      "# iterations:  2\n",
      "accuracies:  [0.93023256 0.86046512]\n",
      "avg accuracy:  0.8953488372093024\n",
      "\n",
      "\n",
      "look ahead:  44\n",
      "# iterations:  2\n",
      "accuracies:  [0.93181818 0.85714286]\n",
      "avg accuracy:  0.8944805194805194\n",
      "\n",
      "\n",
      "look ahead:  45\n",
      "# iterations:  2\n",
      "accuracies:  [0.93333333 0.85365854]\n",
      "avg accuracy:  0.8934959349593496\n",
      "\n",
      "\n",
      "look ahead:  46\n",
      "# iterations:  2\n",
      "accuracies:  [0.93478261 0.85      ]\n",
      "avg accuracy:  0.8923913043478261\n",
      "\n",
      "\n",
      "look ahead:  47\n",
      "# iterations:  2\n",
      "accuracies:  [0.93617021 0.84615385]\n",
      "avg accuracy:  0.8911620294599019\n",
      "\n",
      "\n",
      "look ahead:  48\n",
      "# iterations:  2\n",
      "accuracies:  [0.9375     0.84210526]\n",
      "avg accuracy:  0.8898026315789473\n",
      "\n",
      "\n",
      "look ahead:  49\n",
      "# iterations:  2\n",
      "accuracies:  [0.93877551 0.83783784]\n",
      "avg accuracy:  0.8883066740209598\n",
      "\n",
      "\n",
      "look ahead:  50\n",
      "# iterations:  2\n",
      "accuracies:  [0.94       0.83333333]\n",
      "avg accuracy:  0.8866666666666667\n",
      "\n",
      "\n",
      "look ahead:  51\n",
      "# iterations:  2\n",
      "accuracies:  [0.94117647 0.82857143]\n",
      "avg accuracy:  0.8848739495798319\n",
      "\n",
      "\n",
      "look ahead:  52\n",
      "# iterations:  2\n",
      "accuracies:  [0.94230769 0.82352941]\n",
      "avg accuracy:  0.8829185520361991\n",
      "\n",
      "\n",
      "look ahead:  53\n",
      "# iterations:  2\n",
      "accuracies:  [0.94339623 0.81818182]\n",
      "avg accuracy:  0.8807890222984562\n",
      "\n",
      "\n",
      "look ahead:  54\n",
      "# iterations:  2\n",
      "accuracies:  [0.92592593 0.84375   ]\n",
      "avg accuracy:  0.884837962962963\n",
      "\n",
      "\n",
      "look ahead:  55\n",
      "# iterations:  2\n",
      "accuracies:  [0.92727273 0.83870968]\n",
      "avg accuracy:  0.8829912023460411\n",
      "\n",
      "\n",
      "look ahead:  56\n",
      "# iterations:  2\n",
      "accuracies:  [0.91071429 0.86666667]\n",
      "avg accuracy:  0.8886904761904761\n",
      "\n",
      "\n",
      "look ahead:  57\n",
      "# iterations:  2\n",
      "accuracies:  [0.9122807  0.86206897]\n",
      "avg accuracy:  0.8871748336358136\n",
      "\n",
      "\n",
      "look ahead:  58\n",
      "# iterations:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracies:  [0.9137931  0.85714286]\n",
      "avg accuracy:  0.8854679802955665\n",
      "\n",
      "\n",
      "look ahead:  59\n",
      "# iterations:  2\n",
      "accuracies:  [0.91525424 0.85185185]\n",
      "avg accuracy:  0.8835530445699937\n",
      "\n",
      "\n",
      "look ahead:  60\n",
      "# iterations:  2\n",
      "accuracies:  [0.91666667 0.84615385]\n",
      "avg accuracy:  0.8814102564102564\n",
      "\n",
      "\n",
      "look ahead:  61\n",
      "# iterations:  2\n",
      "accuracies:  [0.91803279 0.84      ]\n",
      "avg accuracy:  0.879016393442623\n",
      "\n",
      "\n",
      "look ahead:  62\n",
      "# iterations:  2\n",
      "accuracies:  [0.91935484 0.83333333]\n",
      "avg accuracy:  0.8763440860215054\n",
      "\n",
      "\n",
      "look ahead:  63\n",
      "# iterations:  2\n",
      "accuracies:  [0.92063492 0.82608696]\n",
      "avg accuracy:  0.8733609385783299\n",
      "\n",
      "\n",
      "look ahead:  64\n",
      "# iterations:  2\n",
      "accuracies:  [0.921875   0.81818182]\n",
      "avg accuracy:  0.8700284090909092\n",
      "\n",
      "\n",
      "look ahead:  65\n",
      "# iterations:  2\n",
      "accuracies:  [0.92307692 0.80952381]\n",
      "avg accuracy:  0.8663003663003663\n",
      "\n",
      "\n",
      "look ahead:  66\n",
      "# iterations:  2\n",
      "accuracies:  [0.92424242 0.8       ]\n",
      "avg accuracy:  0.8621212121212121\n",
      "\n",
      "\n",
      "look ahead:  67\n",
      "# iterations:  2\n",
      "accuracies:  [0.91044776 0.78947368]\n",
      "avg accuracy:  0.8499607227022781\n",
      "\n",
      "\n",
      "look ahead:  68\n",
      "# iterations:  2\n",
      "accuracies:  [0.91176471 0.77777778]\n",
      "avg accuracy:  0.8447712418300654\n",
      "\n",
      "\n",
      "look ahead:  69\n",
      "# iterations:  2\n",
      "accuracies:  [0.91304348 0.76470588]\n",
      "avg accuracy:  0.8388746803069054\n",
      "\n",
      "\n",
      "look ahead:  70\n",
      "# iterations:  2\n",
      "accuracies:  [0.91428571 0.8125    ]\n",
      "avg accuracy:  0.8633928571428571\n",
      "\n",
      "\n",
      "look ahead:  71\n",
      "# iterations:  2\n",
      "accuracies:  [0.91549296 0.8       ]\n",
      "avg accuracy:  0.8577464788732394\n",
      "\n",
      "\n",
      "look ahead:  72\n",
      "# iterations:  2\n",
      "accuracies:  [0.91666667 0.71428571]\n",
      "avg accuracy:  0.8154761904761905\n",
      "\n",
      "\n",
      "look ahead:  73\n",
      "# iterations:  2\n",
      "accuracies:  [0.90410959 0.84615385]\n",
      "avg accuracy:  0.875131717597471\n",
      "\n",
      "\n",
      "look ahead:  74\n",
      "# iterations:  2\n",
      "accuracies:  [0.90540541 0.83333333]\n",
      "avg accuracy:  0.8693693693693694\n",
      "\n",
      "\n",
      "look ahead:  75\n",
      "# iterations:  2\n",
      "accuracies:  [0.90666667 0.81818182]\n",
      "avg accuracy:  0.8624242424242424\n",
      "\n",
      "\n",
      "look ahead:  76\n",
      "# iterations:  2\n",
      "accuracies:  [0.90789474 0.8       ]\n",
      "avg accuracy:  0.8539473684210527\n",
      "\n",
      "\n",
      "look ahead:  77\n",
      "# iterations:  2\n",
      "accuracies:  [0.8961039  0.77777778]\n",
      "avg accuracy:  0.8369408369408369\n",
      "\n",
      "\n",
      "look ahead:  78\n",
      "# iterations:  2\n",
      "accuracies:  [0.8974359 0.75     ]\n",
      "avg accuracy:  0.8237179487179487\n",
      "\n",
      "\n",
      "look ahead:  79\n",
      "# iterations:  2\n",
      "accuracies:  [0.89873418 0.71428571]\n",
      "avg accuracy:  0.8065099457504521\n",
      "\n",
      "\n",
      "look ahead:  80\n",
      "# iterations:  2\n",
      "accuracies:  [0.9        0.66666667]\n",
      "avg accuracy:  0.7833333333333333\n",
      "\n",
      "\n",
      "look ahead:  81\n",
      "# iterations:  2\n",
      "accuracies:  [0.90123457 0.8       ]\n",
      "avg accuracy:  0.8506172839506173\n",
      "\n",
      "\n",
      "look ahead:  82\n",
      "# iterations:  2\n",
      "accuracies:  [0.8902439 1.       ]\n",
      "avg accuracy:  0.9451219512195121\n",
      "\n",
      "\n",
      "look ahead:  83\n",
      "# iterations:  2\n",
      "accuracies:  [0.89156627 1.        ]\n",
      "avg accuracy:  0.9457831325301205\n",
      "\n",
      "\n",
      "look ahead:  84\n",
      "# iterations:  2\n",
      "accuracies:  [0.89285714 1.        ]\n",
      "avg accuracy:  0.9464285714285714\n",
      "\n",
      "\n",
      "look ahead:  85\n",
      "# iterations:  2\n",
      "accuracies:  [0.89411765 1.        ]\n",
      "avg accuracy:  0.9470588235294117\n",
      "\n",
      "\n",
      "look ahead:  86\n",
      "# iterations:  1\n",
      "accuracies:  [0.89534884]\n",
      "avg accuracy:  0.8953488372093024\n",
      "Averages:      Avg Accuracy\n",
      "0       0.895349\n",
      "1       0.895349\n",
      "2       0.896552\n",
      "3       0.897727\n",
      "4       0.900000\n",
      "5       0.888889\n",
      "6       0.901099\n",
      "7       0.878788\n",
      "8       0.880000\n",
      "9       0.885185\n",
      "10      0.892677\n",
      "11      0.895833\n",
      "12      0.887363\n",
      "13      0.908163\n",
      "14      0.891919\n",
      "15      0.871528\n",
      "16      0.901961\n",
      "17      0.876190\n",
      "18      0.886316\n",
      "19      0.863333\n",
      "20      0.914286\n",
      "21      0.893182\n",
      "22      0.875959\n",
      "23      0.866071\n",
      "24      0.884545\n",
      "25      0.870192\n",
      "26      0.875926\n",
      "27      0.919643\n",
      "28      0.894910\n",
      "29      0.893162\n",
      "30      0.890681\n",
      "31      0.887311\n",
      "32      0.882828\n",
      "33      0.867102\n",
      "34      0.880357\n",
      "35      0.849206\n",
      "36      0.881381\n",
      "37      0.871930\n",
      "38      0.856838\n",
      "39      0.830556\n",
      "40      0.926829\n",
      "41      0.928571\n",
      "42      0.895349\n",
      "43      0.894481\n",
      "44      0.893496\n",
      "45      0.892391\n",
      "46      0.891162\n",
      "47      0.889803\n",
      "48      0.888307\n",
      "49      0.886667\n",
      "50      0.884874\n",
      "51      0.882919\n",
      "52      0.880789\n",
      "53      0.884838\n",
      "54      0.882991\n",
      "55      0.888690\n",
      "56      0.887175\n",
      "57      0.885468\n",
      "58      0.883553\n",
      "59      0.881410\n",
      "60      0.879016\n",
      "61      0.876344\n",
      "62      0.873361\n",
      "63      0.870028\n",
      "64      0.866300\n",
      "65      0.862121\n",
      "66      0.849961\n",
      "67      0.844771\n",
      "68      0.838875\n",
      "69      0.863393\n",
      "70      0.857746\n",
      "71      0.815476\n",
      "72      0.875132\n",
      "73      0.869369\n",
      "74      0.862424\n",
      "75      0.853947\n",
      "76      0.836941\n",
      "77      0.823718\n",
      "78      0.806510\n",
      "79      0.783333\n",
      "80      0.850617\n",
      "81      0.945122\n",
      "82      0.945783\n",
      "83      0.946429\n",
      "84      0.947059\n",
      "85      0.895349\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import statistics as stats\n",
    "# Run LR with optimal window size of 15 (# predictions = 294)\n",
    "\n",
    "window_size = 86 # num of preds, window_size = 309-window_size param\n",
    "print('# Predictions: ', window_size)\n",
    "print('Window Size: ', 309-window_size)\n",
    "average_accuracies = np.zeros(window_size)\n",
    "\n",
    "for m in range(1,window_size+1):\n",
    "    # train set\n",
    "    X_train = X.iloc[window_size:]\n",
    "    y_train = y.iloc[window_size:]\n",
    "    #print('train labels: ', y_train)\n",
    "    # test set\n",
    "    X_test = X.iloc[:window_size]\n",
    "    y_test = y.iloc[:window_size]\n",
    "    #print('test labels: ', y_test)\n",
    "    \n",
    "    #look_ahead = 7\n",
    "    look_ahead=m\n",
    "    print(\"\\n\\nlook ahead: \", m)\n",
    "    print(\"# iterations: \", math.ceil(y_test.shape[0]/look_ahead))\n",
    "    \n",
    "    #actuals = pd.DataFrame(y_test)\n",
    "    #actuals = actuals.rename(columns={'AvgHR_bin':'Actuals'})\n",
    "    accuracies = np.zeros(math.ceil(y_test.shape[0]/look_ahead))\n",
    "    # print('actuals: \\n', actuals)\n",
    "    # preds = np.zeros((math.ceil(y_test.shape[0]/look_ahead),look_ahead))\n",
    "    # actuals_look_ahead = np.zeros((math.ceil(y_test.shape[0]/look_ahead),look_ahead))\n",
    "    # print('X_test: \\n', X_test)\n",
    "    for i in range(0,math.ceil(y_test.shape[0]/look_ahead)):\n",
    "        preds = np.zeros(min(look_ahead,y_test.shape[0]))\n",
    "        actuals_look_ahead = np.zeros(min(look_ahead,y_test.shape[0]))\n",
    "#         print('Iteration: ', i)\n",
    "#         print('X train shape: ', X_train.shape[0])\n",
    "#         print('X test shape: ', y_test.shape[0])\n",
    "        logreg.fit(X_train, y_train.values.ravel())\n",
    "        # Predict test set\n",
    "        for j in range(1,min(look_ahead+1,y_test.shape[0]+1)):\n",
    "            y_pred = logreg.predict(np.array(X_test.iloc[-j]).reshape(1,-1))\n",
    "#             print('actual: ',y_test.iloc[-j], '\\n pred: ',y_pred, '\\n')\n",
    "            preds[j-1] = y_pred\n",
    "            actuals_look_ahead[j-1] = y_test.iloc[-j]\n",
    "        # print('Accuracy of logistic regression classifier on test set {}: {:.2f}'.format(i, logreg.score(X_test, y_test)))\n",
    "        #print('test instance: ', X_test.iloc[-1])\n",
    "        # X_train = pd.concat([X_test.iloc[0], X_train]).reset_index(drop = True)\n",
    "        # print('new X train: ', X_train.head())\n",
    "\n",
    "        #print(\"X test -1: \", X_test.iloc[-1])\n",
    "        X_test_inst = pd.DataFrame(columns=[\"Distance (km)\",\"Avg Pace (/km)\", \"HRSS\", \"Elevation Gain (m)\"])\n",
    "        y_test_inst = pd.DataFrame(columns=[\"AvgHR_bin\"])\n",
    "        for k in range(1,min(look_ahead+1,y_test.shape[0]+1)):\n",
    "            X_test_inst = X_test_inst.append(X_test.iloc[-k])\n",
    "            y_test_inst = y_test_inst.append(y_test.iloc[-k])        \n",
    "        #print(\"X test inst: \", X_test_inst)\n",
    "\n",
    "        #X_train = X_train.drop(X_train.index[-1]).reset_index(drop=True)\n",
    "        X_train = pd.concat([X_test_inst,X_train]).reset_index(drop = True)\n",
    "        #print(\"X train: \\n\", X_train)\n",
    "\n",
    "        #y_train = y_train.drop(y_train.index[-1])\n",
    "        y_train = pd.concat([y_test_inst,y_train]).reset_index(drop=True)\n",
    "        #print(\"y train \\n\", y_train)\n",
    "\n",
    "        X_test = X_test.drop(X_test_inst.index.values)\n",
    "        X_test = X_test.reset_index(drop=True)\n",
    "        #print(\"X test: \\n\", X_test)\n",
    "\n",
    "        y_test = y_test.drop(y_test_inst.index.values)\n",
    "        y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "        preds_act_df = pd.DataFrame(preds)\n",
    "        act_df = pd.DataFrame(actuals_look_ahead)\n",
    "#         print(\"predictions: \", preds)\n",
    "#         print(\"actuals: \", actuals_look_ahead)\n",
    "        #preds_act_df = preds_act_df.join(act_df)\n",
    "        #print('actuals and preds: \\n', preds_act_df)\n",
    "        accuracy = metrics.accuracy_score(preds,actuals_look_ahead)\n",
    "    \n",
    "        accuracies[i] = accuracy\n",
    "        # print('accuracy for window size {}: {}'.format(window_size, accuracy))\n",
    "    \n",
    "    print('accuracies: ', accuracies)\n",
    "    avg_accuracy = stats.mean(accuracies)\n",
    "    print('avg accuracy: ', avg_accuracy)\n",
    "    average_accuracies[m-1] = avg_accuracy\n",
    "\n",
    "avg_acc_df = pd.DataFrame(data=average_accuracies, columns=['Avg Accuracy'])\n",
    "print('Averages: ', avg_acc_df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output accuracies for each window size to Accuracies_for_Look_Ahead_Variations.csv file \n",
    "with open('Accuracies_for_Look_Ahead_Variations.csv', 'w') as f:\n",
    "    f.write(\"Look Ahead : Accuracy \\n\")\n",
    "    for i in range(0,len(average_accuracies)):\n",
    "        f.write(str(i+1) + ': ' + str(average_accuracies[i]))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Look Ahead             Accuracy\n",
      "0   Look Ahead             Accuracy \n",
      "85           85   0.9470588235294117\n",
      "84           84   0.9464285714285714\n",
      "83           83   0.9457831325301205\n",
      "82           82   0.9451219512195121\n",
      "42           42   0.9285714285714286\n",
      "41           41    0.926829268292683\n",
      "28           28   0.9196428571428572\n",
      "21           21   0.9142857142857143\n",
      "14           14   0.9081632653061225\n",
      "17           17   0.9019607843137255\n",
      "7             7   0.9010989010989011\n",
      "5             5                  0.9\n",
      "4             4   0.8977272727272727\n",
      "3             3    0.896551724137931\n",
      "12           12   0.8958333333333333\n",
      "1             1   0.8953488372093024\n",
      "43           43   0.8953488372093024\n",
      "86           86   0.8953488372093024\n",
      "2             2   0.8953488372093024\n",
      "29           29   0.8949096880131363\n",
      "44           44   0.8944805194805194\n",
      "45           45   0.8934959349593496\n",
      "22           22   0.8931818181818182\n",
      "30           30   0.8931623931623932\n",
      "11           11   0.8926767676767676\n",
      "46           46   0.8923913043478261\n",
      "15           15    0.891919191919192\n",
      "47           47   0.8911620294599019\n",
      "31           31   0.8906810035842294\n",
      "48           48   0.8898026315789473\n",
      "6             6    0.888888888888889\n",
      "56           56   0.8886904761904761\n",
      "49           49   0.8883066740209598\n",
      "13           13   0.8873626373626374\n",
      "32           32   0.8873106060606061\n",
      "57           57   0.8871748336358136\n",
      "50           50   0.8866666666666667\n",
      "19           19   0.8863157894736842\n",
      "58           58   0.8854679802955665\n",
      "10           10   0.8851851851851852\n",
      "51           51   0.8848739495798319\n",
      "54           54    0.884837962962963\n",
      "25           25   0.8845454545454545\n",
      "59           59   0.8835530445699937\n",
      "55           55   0.8829912023460411\n",
      "52           52   0.8829185520361991\n",
      "33           33   0.8828282828282829\n",
      "60           60   0.8814102564102564\n",
      "37           37   0.8813813813813814\n",
      "53           53   0.8807890222984562\n",
      "35           35   0.8803571428571428\n",
      "9             9                 0.88\n",
      "61           61    0.879016393442623\n",
      "8             8   0.8787878787878788\n",
      "62           62   0.8763440860215054\n",
      "18           18   0.8761904761904762\n",
      "23           23   0.8759590792838874\n",
      "27           27    0.875925925925926\n",
      "73           73    0.875131717597471\n",
      "63           63   0.8733609385783299\n",
      "38           38   0.8719298245614036\n",
      "16           16   0.8715277777777778\n",
      "26           26   0.8701923076923077\n",
      "64           64   0.8700284090909092\n",
      "74           74   0.8693693693693694\n",
      "34           34   0.8671023965141612\n",
      "65           65   0.8663003663003663\n",
      "24           24   0.8660714285714286\n",
      "70           70   0.8633928571428571\n",
      "20           20   0.8633333333333333\n",
      "75           75   0.8624242424242424\n",
      "66           66   0.8621212121212121\n",
      "71           71   0.8577464788732394\n",
      "39           39   0.8568376068376069\n",
      "76           76   0.8539473684210527\n",
      "81           81   0.8506172839506173\n",
      "67           67   0.8499607227022781\n",
      "36           36   0.8492063492063492\n",
      "68           68   0.8447712418300654\n",
      "69           69   0.8388746803069054\n",
      "77           77   0.8369408369408369\n",
      "40           40   0.8305555555555556\n",
      "78           78   0.8237179487179487\n",
      "72           72   0.8154761904761905\n",
      "79           79   0.8065099457504521\n",
      "80           80   0.7833333333333333\n"
     ]
    }
   ],
   "source": [
    "# Read in accuracies for each look ahead value from Accuracies_for_Look_Ahead_Variations.csv file and sort by accuracy to determine best look ahead value\n",
    "import csv\n",
    "pd.set_option('display.max_columns', None)  # or 1000\n",
    "pd.set_option('display.max_rows', None)  # or 1000\n",
    "mylist = pd.DataFrame(columns=['Look Ahead', 'Accuracy'])\n",
    "with open('Accuracies_for_Look_Ahead_Variations.csv', 'r') as csvfile:\n",
    "    for i,row in enumerate(csv.reader(csvfile, delimiter='\\n')):\n",
    "        mylist.loc[i,'Look Ahead'] = row[0].split(':')[0]\n",
    "        mylist.loc[i,'Accuracy'] = row[0].split(':')[1]\n",
    "print(mylist.sort_values('Accuracy', 0,ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all accuracies are above 0.8. The highest accuracy is 0.947 and looks ahead 85 days, a value equal to almost the entire test set size. Rather than iterate through each test instance and leave one out, it is best to predict the entire test set at once, rather than leaving a group out at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Predictions:  86\n",
      "Window Size:  223\n",
      "train labels:       AvgHR_bin\n",
      "86         1.0\n",
      "87         1.0\n",
      "88         1.0\n",
      "89         0.0\n",
      "90         1.0\n",
      "91         1.0\n",
      "92         1.0\n",
      "93         1.0\n",
      "94         1.0\n",
      "95         1.0\n",
      "96         1.0\n",
      "97         0.0\n",
      "98         1.0\n",
      "99         1.0\n",
      "100        1.0\n",
      "101        1.0\n",
      "102        1.0\n",
      "103        1.0\n",
      "104        0.0\n",
      "105        1.0\n",
      "106        0.0\n",
      "107        0.0\n",
      "108        1.0\n",
      "109        1.0\n",
      "110        1.0\n",
      "111        1.0\n",
      "112        1.0\n",
      "113        1.0\n",
      "114        1.0\n",
      "115        1.0\n",
      "116        1.0\n",
      "117        1.0\n",
      "118        1.0\n",
      "119        1.0\n",
      "120        1.0\n",
      "121        1.0\n",
      "122        1.0\n",
      "123        0.0\n",
      "124        0.0\n",
      "125        0.0\n",
      "126        1.0\n",
      "127        0.0\n",
      "128        1.0\n",
      "129        1.0\n",
      "130        1.0\n",
      "131        0.0\n",
      "132        1.0\n",
      "133        0.0\n",
      "134        1.0\n",
      "135        1.0\n",
      "136        0.0\n",
      "137        1.0\n",
      "138        1.0\n",
      "139        1.0\n",
      "140        1.0\n",
      "141        1.0\n",
      "142        0.0\n",
      "143        1.0\n",
      "144        1.0\n",
      "145        0.0\n",
      "146        1.0\n",
      "147        1.0\n",
      "148        1.0\n",
      "149        1.0\n",
      "150        1.0\n",
      "151        0.0\n",
      "152        0.0\n",
      "153        1.0\n",
      "154        0.0\n",
      "155        1.0\n",
      "156        1.0\n",
      "157        1.0\n",
      "158        0.0\n",
      "159        0.0\n",
      "160        1.0\n",
      "161        1.0\n",
      "162        1.0\n",
      "163        0.0\n",
      "164        1.0\n",
      "165        1.0\n",
      "166        0.0\n",
      "167        1.0\n",
      "168        1.0\n",
      "169        0.0\n",
      "170        0.0\n",
      "171        1.0\n",
      "172        1.0\n",
      "173        1.0\n",
      "174        1.0\n",
      "175        1.0\n",
      "176        1.0\n",
      "177        0.0\n",
      "178        0.0\n",
      "179        0.0\n",
      "180        1.0\n",
      "181        1.0\n",
      "182        0.0\n",
      "183        1.0\n",
      "184        1.0\n",
      "185        0.0\n",
      "186        1.0\n",
      "187        1.0\n",
      "188        1.0\n",
      "189        0.0\n",
      "190        1.0\n",
      "191        1.0\n",
      "192        0.0\n",
      "193        1.0\n",
      "194        1.0\n",
      "195        1.0\n",
      "196        1.0\n",
      "197        0.0\n",
      "198        1.0\n",
      "199        0.0\n",
      "200        0.0\n",
      "201        1.0\n",
      "202        0.0\n",
      "203        1.0\n",
      "204        1.0\n",
      "205        0.0\n",
      "206        1.0\n",
      "207        1.0\n",
      "208        1.0\n",
      "209        0.0\n",
      "210        0.0\n",
      "211        1.0\n",
      "212        1.0\n",
      "213        1.0\n",
      "214        1.0\n",
      "215        0.0\n",
      "216        1.0\n",
      "217        1.0\n",
      "218        1.0\n",
      "219        1.0\n",
      "220        1.0\n",
      "221        1.0\n",
      "222        1.0\n",
      "223        1.0\n",
      "224        1.0\n",
      "225        1.0\n",
      "226        0.0\n",
      "227        1.0\n",
      "228        1.0\n",
      "229        1.0\n",
      "230        1.0\n",
      "231        0.0\n",
      "232        0.0\n",
      "233        0.0\n",
      "234        1.0\n",
      "235        1.0\n",
      "236        1.0\n",
      "237        1.0\n",
      "238        1.0\n",
      "239        1.0\n",
      "240        1.0\n",
      "241        1.0\n",
      "242        0.0\n",
      "243        1.0\n",
      "244        0.0\n",
      "245        0.0\n",
      "246        1.0\n",
      "247        1.0\n",
      "248        0.0\n",
      "249        0.0\n",
      "250        0.0\n",
      "251        1.0\n",
      "252        1.0\n",
      "253        1.0\n",
      "254        0.0\n",
      "255        0.0\n",
      "256        0.0\n",
      "257        1.0\n",
      "258        1.0\n",
      "259        0.0\n",
      "260        1.0\n",
      "261        1.0\n",
      "262        0.0\n",
      "263        1.0\n",
      "264        0.0\n",
      "265        1.0\n",
      "266        0.0\n",
      "267        0.0\n",
      "268        0.0\n",
      "269        0.0\n",
      "270        1.0\n",
      "271        0.0\n",
      "272        0.0\n",
      "273        0.0\n",
      "274        0.0\n",
      "275        0.0\n",
      "276        1.0\n",
      "277        0.0\n",
      "278        1.0\n",
      "279        0.0\n",
      "280        0.0\n",
      "281        0.0\n",
      "282        0.0\n",
      "283        1.0\n",
      "284        1.0\n",
      "285        1.0\n",
      "286        0.0\n",
      "287        1.0\n",
      "288        0.0\n",
      "289        0.0\n",
      "290        0.0\n",
      "291        1.0\n",
      "292        1.0\n",
      "293        0.0\n",
      "294        0.0\n",
      "295        1.0\n",
      "296        0.0\n",
      "297        0.0\n",
      "298        1.0\n",
      "299        0.0\n",
      "300        0.0\n",
      "301        0.0\n",
      "302        0.0\n",
      "303        0.0\n",
      "304        0.0\n",
      "305        0.0\n",
      "306        0.0\n",
      "307        0.0\n",
      "308        0.0\n",
      "test labels:      AvgHR_bin\n",
      "0         1.0\n",
      "1         1.0\n",
      "2         1.0\n",
      "3         1.0\n",
      "4         1.0\n",
      "5         1.0\n",
      "6         1.0\n",
      "7         0.0\n",
      "8         0.0\n",
      "9         0.0\n",
      "10        1.0\n",
      "11        1.0\n",
      "12        1.0\n",
      "13        0.0\n",
      "14        1.0\n",
      "15        0.0\n",
      "16        0.0\n",
      "17        1.0\n",
      "18        0.0\n",
      "19        1.0\n",
      "20        1.0\n",
      "21        0.0\n",
      "22        0.0\n",
      "23        0.0\n",
      "24        0.0\n",
      "25        0.0\n",
      "26        0.0\n",
      "27        0.0\n",
      "28        0.0\n",
      "29        0.0\n",
      "30        0.0\n",
      "31        0.0\n",
      "32        0.0\n",
      "33        0.0\n",
      "34        0.0\n",
      "35        0.0\n",
      "36        0.0\n",
      "37        0.0\n",
      "38        0.0\n",
      "39        0.0\n",
      "40        0.0\n",
      "41        0.0\n",
      "42        0.0\n",
      "43        0.0\n",
      "44        0.0\n",
      "45        0.0\n",
      "46        0.0\n",
      "47        0.0\n",
      "48        0.0\n",
      "49        0.0\n",
      "50        0.0\n",
      "51        0.0\n",
      "52        1.0\n",
      "53        1.0\n",
      "54        0.0\n",
      "55        0.0\n",
      "56        0.0\n",
      "57        0.0\n",
      "58        0.0\n",
      "59        0.0\n",
      "60        0.0\n",
      "61        0.0\n",
      "62        0.0\n",
      "63        0.0\n",
      "64        0.0\n",
      "65        1.0\n",
      "66        0.0\n",
      "67        0.0\n",
      "68        1.0\n",
      "69        0.0\n",
      "70        0.0\n",
      "71        1.0\n",
      "72        1.0\n",
      "73        1.0\n",
      "74        0.0\n",
      "75        0.0\n",
      "76        1.0\n",
      "77        0.0\n",
      "78        1.0\n",
      "79        0.0\n",
      "80        1.0\n",
      "81        0.0\n",
      "82        0.0\n",
      "83        0.0\n",
      "84        1.0\n",
      "85        1.0\n",
      "Iteration:  0\n",
      "X train shape:  223\n",
      "X test shape:  86\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 85, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 84, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 83, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 82, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 81, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 80, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 79, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 78, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 77, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 76, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 75, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 74, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 73, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 72, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 71, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 70, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 69, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 68, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 67, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 66, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 65, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 64, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 63, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 62, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 61, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 60, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 59, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 58, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 57, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 56, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 55, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 54, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 53, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 52, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 51, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 50, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 49, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 48, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 47, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 46, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 45, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 44, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 43, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 42, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 41, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 40, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 39, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 38, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 37, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 36, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 35, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 34, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 33, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual:  AvgHR_bin    0.0\n",
      "Name: 32, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 31, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 30, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 29, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 28, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 27, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 26, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 25, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 24, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 23, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 22, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 21, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 20, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 19, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 18, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 17, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 16, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 15, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 14, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 13, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 12, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 11, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 10, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 9, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 8, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    0.0\n",
      "Name: 7, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 6, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 5, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 4, dtype: float64 \n",
      " pred:  [0.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 3, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 2, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 1, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "actual:  AvgHR_bin    1.0\n",
      "Name: 0, dtype: float64 \n",
      " pred:  [1.] \n",
      "\n",
      "X test inst:      Distance (km)  Avg Pace (/km)   HRSS  Elevation Gain (m)\n",
      "85           56.3           121.0  142.0               668.0\n",
      "84           40.2           113.0   91.0               379.0\n",
      "83           55.8           126.0  125.0               496.0\n",
      "82           80.3           137.0  164.0              1019.0\n",
      "81           56.1           134.0  123.0               665.0\n",
      "80           30.0            91.0  111.0                13.0\n",
      "79           40.0           118.0   87.0               371.0\n",
      "78           60.6           116.0  137.0               546.0\n",
      "77           50.8           116.0  107.0               510.0\n",
      "76           32.5           109.0   86.0               285.0\n",
      "75           80.3           133.0  171.0              1077.0\n",
      "74           40.1           118.0   69.0               388.0\n",
      "73           73.9           148.0  292.0              1986.0\n",
      "72           20.1           177.0   73.0               488.0\n",
      "71           29.1           112.0  102.0               412.0\n",
      "70           50.5           163.0  135.0              1306.0\n",
      "69           33.1           170.0   98.0               852.0\n",
      "68           90.2            87.0  213.0               461.0\n",
      "67           30.1           158.0    6.0               678.0\n",
      "66           51.5           161.0  148.0              1258.0\n",
      "65           33.1           155.0  123.0               878.0\n",
      "64          107.6           160.0  335.0              2925.0\n",
      "63           33.2           174.0   93.0               866.0\n",
      "62           41.8           188.0  117.0              1094.0\n",
      "61           34.3           160.0  111.0               894.0\n",
      "60          151.2           165.0  344.0              3142.0\n",
      "59           50.8           150.0  137.0               783.0\n",
      "58           43.8           140.0   90.0               387.0\n",
      "57           33.3           158.0  104.0               882.0\n",
      "56           55.2           146.0  157.0              1126.0\n",
      "55           50.4           162.0  137.0              1293.0\n",
      "54           40.1           156.0  103.0               859.0\n",
      "53           73.2           146.0  227.0              2031.0\n",
      "52          144.1           124.0  467.0              3120.0\n",
      "51           50.4           165.0  126.0              1239.0\n",
      "50           73.6           177.0  181.0              1922.0\n",
      "49           64.3           180.0  164.0              1053.0\n",
      "48           60.4           157.0  134.0               679.0\n",
      "47           40.6           207.0   96.0               833.0\n",
      "46           52.9           176.0  135.0              1313.0\n",
      "45          105.5           169.0  309.0              3067.0\n",
      "44           53.0           164.0  137.0              1400.0\n",
      "43           53.0           155.0  138.0              1387.0\n",
      "42          146.9           168.0  445.0              3617.0\n",
      "41           93.8           153.0  205.0              1446.0\n",
      "40           50.4           165.0  108.0              1327.0\n",
      "39          101.6           146.0  213.0              1790.0\n",
      "38           76.0           172.0  197.0              2003.0\n",
      "37           50.4           180.0  100.0              1310.0\n",
      "36           30.4           117.0   52.0               247.0\n",
      "35           93.4           173.0  300.0              2624.0\n",
      "34           76.3           173.0  189.0              2067.0\n",
      "33           76.3           168.0  172.0              2074.0\n",
      "32           25.6           208.0  109.0               685.0\n",
      "31          170.6           149.0  471.0              3388.0\n",
      "30           26.0           307.0  128.0               715.0\n",
      "29           27.2           221.0   82.0               636.0\n",
      "28          141.0           167.0  354.0              3366.0\n",
      "27           15.1           477.0   71.0               492.0\n",
      "26           52.9           193.0  137.0              1150.0\n",
      "25           64.2           195.0  195.0              2194.0\n",
      "24           40.3           266.0  166.0              1326.0\n",
      "23          139.8           151.0  390.0              2768.3\n",
      "22           55.2           149.0  116.0               884.0\n",
      "21           50.6           187.0  133.0               981.0\n",
      "20           52.8           116.0  138.0               605.0\n",
      "19           55.4           108.0  123.0               541.0\n",
      "18           63.0           119.0  128.0               744.0\n",
      "17           47.5           110.0  132.0               406.0\n",
      "16           32.5           119.0   66.0               282.0\n",
      "15           45.9           148.0   79.0               447.0\n",
      "14           30.3           127.0   92.0               550.0\n",
      "13           13.2           301.0   74.0               184.0\n",
      "12           52.0           116.0  152.0               560.0\n",
      "11          100.9           130.0  337.0              1395.0\n",
      "10           52.6           122.0  172.0               535.0\n",
      "9            47.2           116.0  106.0               447.0\n",
      "8            55.4           114.0  116.0               502.0\n",
      "7            51.4           128.0   90.0               576.0\n",
      "6            81.0           129.0  225.0              1096.0\n",
      "5            61.4           122.0  146.0               692.0\n",
      "4            41.1           120.0   96.0               454.0\n",
      "3            45.6           114.0  119.0               447.0\n",
      "2            35.2           116.0   87.0               314.0\n",
      "1            80.1           125.0  217.0               890.0\n",
      "0            62.5           113.0  144.0               589.0\n",
      "predictions:  [1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1.\n",
      " 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1.]\n",
      "actuals:  [1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1.\n",
      " 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "accuracy for window size 223 with look ahead value 86: 0.813953488372093\n",
      "accuracies:  [0.81395349]\n",
      "avg accuracy:  0.813953488372093\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import math\n",
    "import statistics as stats\n",
    "# Run LR with optimal window size of 15 (# predictions = 294)\n",
    "\n",
    "# Create the model with 100 trees\n",
    "RF = RandomForestClassifier(n_estimators=100, \n",
    "                               bootstrap = True,\n",
    "                               max_features = 'sqrt')\n",
    "window_size = 86 # num of preds, window_size = 309-window_size param\n",
    "print('# Predictions: ', window_size)\n",
    "print('Window Size: ', 309-window_size)\n",
    "# train set\n",
    "X_train = X.iloc[window_size:]\n",
    "y_train = y.iloc[window_size:]\n",
    "print('train labels: ', y_train)\n",
    "# test set\n",
    "X_test = X.iloc[:window_size]\n",
    "y_test = y.iloc[:window_size]\n",
    "print('test labels: ', y_test)\n",
    "\n",
    "look_ahead = 86\n",
    "\n",
    "\n",
    "accuracies = np.zeros(math.ceil(y_test.shape[0]/look_ahead))\n",
    "# print('X_test: \\n', X_test)\n",
    "\n",
    "for i in range(0,math.ceil(y_test.shape[0]/look_ahead)):\n",
    "    preds = np.zeros(min(look_ahead,y_test.shape[0]))\n",
    "    rf_probs = np.zeros(min(look_ahead,y_test.shape[0]))\n",
    "    actuals_look_ahead = np.zeros(min(look_ahead,y_test.shape[0]))\n",
    "    print('Iteration: ', i)\n",
    "    print('X train shape: ', X_train.shape[0])\n",
    "    print('X test shape: ', y_test.shape[0])\n",
    "    RF.fit(X_train, y_train.values.ravel())\n",
    "    # Predict test set\n",
    "    for j in range(1,min(look_ahead+1,y_test.shape[0]+1)):\n",
    "        y_pred = RF.predict(np.array(X_test.iloc[-j]).reshape(1,-1))\n",
    "        print('actual: ',y_test.iloc[-j], '\\n pred: ',y_pred, '\\n')\n",
    "        preds[j-1] = y_pred\n",
    "        actuals_look_ahead[j-1] = y_test.iloc[-j]\n",
    "        rf_probs[j-1] = RF.predict_proba(np.array(X_test.iloc[-j]).reshape(1,-1))[:, 1]\n",
    "    # print('Accuracy of logistic regression classifier on test set {}: {:.2f}'.format(i, logreg.score(X_test, y_test)))\n",
    "    #print('test instance: ', X_test.iloc[-1])\n",
    "    # X_train = pd.concat([X_test.iloc[0], X_train]).reset_index(drop = True)\n",
    "    # print('new X train: ', X_train.head())\n",
    "    \n",
    "    #print(\"X test -1: \", X_test.iloc[-1])\n",
    "    X_test_inst = pd.DataFrame(columns=[\"Distance (km)\",\"Avg Pace (/km)\", \"HRSS\", \"Elevation Gain (m)\"])\n",
    "    y_test_inst = pd.DataFrame(columns=[\"AvgHR_bin\"])\n",
    "    for k in range(1,min(look_ahead+1,y_test.shape[0]+1)):\n",
    "        X_test_inst = X_test_inst.append(X_test.iloc[-k])\n",
    "        y_test_inst = y_test_inst.append(y_test.iloc[-k])        \n",
    "    print(\"X test inst: \", X_test_inst)\n",
    "\n",
    "    X_train = X_train.drop(X_train.index[-1]).reset_index(drop=True)\n",
    "    X_train = pd.concat([X_test_inst,X_train]).reset_index(drop = True)\n",
    "    #print(\"X train: \\n\", X_train)\n",
    "\n",
    "    y_train = y_train.drop(y_train.index[-1])\n",
    "    y_train = pd.concat([y_test_inst,y_train]).reset_index(drop=True)\n",
    "    #print(\"y train \\n\", y_train)\n",
    "    \n",
    "    X_test = X_test.drop(X_test_inst.index.values)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    #print(\"X test: \\n\", X_test)\n",
    "\n",
    "    y_test = y_test.drop(y_test_inst.index.values)\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "    \n",
    "    preds_act_df = pd.DataFrame(preds)\n",
    "    act_df = pd.DataFrame(actuals_look_ahead)\n",
    "    print(\"predictions: \", preds)\n",
    "    print(\"actuals: \", actuals_look_ahead)\n",
    "    #preds_act_df = preds_act_df.join(act_df)\n",
    "    #print('actuals and preds: \\n', preds_act_df)\n",
    "    accuracy = metrics.accuracy_score(preds,actuals_look_ahead)\n",
    "    accuracies[i] = accuracy\n",
    "    print('accuracy for window size {} with look ahead value {}: {}'.format(309-window_size, look_ahead, accuracy))\n",
    "print('accuracies: ', accuracies)\n",
    "print('avg accuracy: ', stats.mean(accuracies))\n",
    "print('probabilities: ', rf_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest performs better than LR when looking ahead 1 value for a window size of 223 and using the 'simple moving average' method of dropping the oldest record and adding the newest test record for the next prediction (0.884 accuracy). When using the 'cumulative average method', or not dropping oldest record, RF scores 0.86 accuracy for a window size of 223. For a window size of 85, RF scored an accuracy of 0.906, while a window size of 86 yielded an accuracy of 0.814."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
